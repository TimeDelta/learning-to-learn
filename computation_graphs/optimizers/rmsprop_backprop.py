# Generated by ChatGPT
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from typing import Dict, List, Tuple

class RMSPropBackprop(nn.Module):
    square_avg: Dict[str, torch.Tensor] = torch.jit.Attribute({}, Dict[str, torch.Tensor])

    def __init__(self, step_size: float = 0.001, alpha: float = 0.99, eps: float = 1e-8):
        super(RMSPropBackprop, self).__init__()
        self.step_size = step_size
        self.alpha = alpha
        self.eps = eps
        self.square_avg: Dict[str, torch.Tensor] = {}

    def forward(self, loss: torch.Tensor, prev_loss: torch.Tensor,
                named_parameters: List[Tuple[str, Parameter]]) -> Dict[str, torch.Tensor]:
        params = [param for _, param in named_parameters]
        grads = torch.autograd.grad([loss], params, create_graph=False)
        new_params: Dict[str, torch.Tensor] = {}
        for (name, param), grad in zip(named_parameters, grads):
            if grad is None:
                grad = torch.zeros_like(param)
            if name not in self.square_avg:
                self.square_avg[name] = torch.zeros_like(param)
            v = self.square_avg[name]
            # Update moving average of squared gradients
            v = self.alpha * v + (1 - self.alpha) * (grad * grad)
            # RMSProp update
            update = self.step_size * grad / (torch.sqrt(v) + self.eps)
            new_params[name] = param - update
            self.square_avg[name] = v
        return new_params

if __name__ == "__main__":
    optimizer = torch.jit.script(RMSPropBackprop())
    torch.jit.save(optimizer, __file__.replace('.py', '.pt'))
    print(optimizer.graph)
