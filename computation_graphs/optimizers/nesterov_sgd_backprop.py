# Generated by ChatGPT
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from typing import Dict, List, Tuple

class NesterovSGDBackprop(nn.Module):
    velocity: Dict[str, torch.Tensor] = torch.jit.Attribute({}, Dict[str, torch.Tensor])

    def __init__(self, step_size: float = 0.1, momentum: float = 0.9):
        super(NesterovSGD, self).__init__()
        self.step_size = step_size
        self.momentum = momentum
        self.velocity: Dict[str, torch.Tensor] = {}

    def forward(self, loss: torch.Tensor, prev_loss: torch.Tensor,
                named_parameters: List[Tuple[str, Parameter]]) -> Dict[str, torch.Tensor]:
        params = [param for _, param in named_parameters]
        grads = torch.autograd.grad([loss], params, create_graph=False, allow_unused=True)
        new_params: Dict[str, torch.Tensor] = {}
        for (name, param), grad in zip(named_parameters, grads):
            if grad is None:
                grad = torch.zeros_like(param)
            if name not in self.velocity:
                self.velocity[name] = torch.zeros_like(param)
            v_prev = self.velocity[name]
            v_new = self.momentum * v_prev + grad
            # Nesterov update uses lookahead gradient
            update = grad + self.momentum * v_new
            new_params[name] = param - self.step_size * update
            self.velocity[name] = v_new
        return new_params

if __name__ == "__main__":
    optimizer = torch.jit.script(NesterovSGDBackprop())
    torch.jit.save(optimizer, __file__.replace('.py', '.pt'))
    print(optimizer.graph)
