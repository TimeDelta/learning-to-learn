# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import add_self_loops, softmax


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """
    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr='add')  # sum aggregation
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        # a^T [h_i || h_j]  => 2*out_channels parameters
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index):
        # Linear transform
        x = self.lin(x)  # [N, out_channels]
        # Add self-loops so each node attends to itself
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        # Message passing
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        # Un-normalized attention scores from concatenated node pairs
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        # Normalize per target node
        alpha = softmax(alpha, index, ptr, size_i)  # [E]
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node embeddings and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """
    def __init__(self, num_node_types, node_emb_dim, hidden_dims, latent_dim):
        super().__init__()
        self.node_embedding = nn.Embedding(num_node_types, node_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = node_emb_dim
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    def forward(self, x_type, edge_index, batch):
        # x_type: [num_nodes] long tensor of type indices
        x = self.node_embedding(x_type)  # → [N, node_emb_dim]
        for conv in self.convs:
            x = conv(x, edge_index)
        x = global_mean_pool(x, batch)  # → [batch_size, hidden_dim]
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar


class GraphDecoder(nn.Module):
    """
    Reconstruct adjacency and node features from latent code.
    """
    def __init__(self, latent_dim, max_nodes, node_feat_dim, hidden_dim=128):
        super().__init__()
        self.max_nodes = max_nodes
        self.node_feat_dim = node_feat_dim
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_adj = nn.Linear(hidden_dim, max_nodes * max_nodes)
        self.fc_feat = nn.Linear(hidden_dim, max_nodes * node_feat_dim)

    def forward(self, z):
        batch_size = z.size(0)
        h = F.relu(self.fc1(z))
        adj_logits = self.fc_adj(h).view(batch_size, self.max_nodes, self.max_nodes)
        feat_logits = self.fc_feat(h).view(batch_size, self.max_nodes, self.node_feat_dim)
        return adj_logits, feat_logits


class FitnessPredictor(nn.Module):
    """
    Auxiliary network: predicts scalar fitness from latent z.
    """
    def __init__(self, latent_dim, hidden_dim=64):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)

    def forward(self, z):
        h = F.relu(self.fc1(z))
        fitness_pred = self.fc2(h)
        return fitness_pred.squeeze(-1)


class DAGVAE(nn.Module):
    """
    DAG-VAE with ARD prior and latent-mask for dynamic pruning.
    """
    def __init__(self, encoder: GraphEncoder, decoder: GraphDecoder, predictor: FitnessPredictor):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.predictor = predictor
        # ARD: learnable log-precision per latent dimension
        latent_dim = self.encoder.lin_mu.out_features
        self.log_alpha = nn.Parameter(torch.zeros(latent_dim))
        # mask for active latent dims (1=active, 0=pruned)
        self.register_buffer('latent_mask', torch.ones(latent_dim))

    def encode(self, x, edge_index, batch):
        return self.encoder(x, edge_index, batch)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        # apply mask: pruned dims set to zero
        z = z * self.latent_mask
        return z

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x, edge_index, batch):
        mu, logvar = self.encode(x, edge_index, batch)
        z = self.reparameterize(mu, logvar)
        adj_logits, feat_logits = self.decode(z)
        fitness_pred = self.predictor(z)
        return adj_logits, feat_logits, fitness_pred, mu, logvar, z


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL and dynamic latent pruning.
    """
    def __init__(self, model: DAGVAE, optimizer, device=torch.device('cpu')):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.device = device
        self.dataset = []
        self.loss_history = []

    def add_data(self, graphs, fitnesses):
        for graph, fit in zip(graphs, fitnesses):
            data = graph.clone()
            data.y = torch.tensor([fit], dtype=torch.float)
            self.dataset.append(data)

    def compute_ard_precisions(self):
        # precision = exp(log_alpha)
        return torch.exp(self.model.log_alpha).detach().cpu().numpy()

    def prune_low_precision_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        precisions = self.compute_ard_precisions()
        active_mask = self.model.latent_mask.cpu().numpy().astype(bool)
        active_indices = np.where(active_mask)[0]
        active_precs = precisions[active_indices]
        # dims to prune: highest precision
        sorted_idx = np.argsort(active_precs)
        dims_to_prune = active_indices[sorted_idx[-num_prune:]]
        # update mask
        mask = self.model.latent_mask.clone()
        device = mask.device
        mask[dims_to_prune.tolist()] = 0.0
        self.model.latent_mask.copy_(mask)
        print(f"Pruned latent dims: {dims_to_prune}")

    def train(self, epochs=1, batch_size=16, kl_weight=1.0, fitness_weight=1.0,
              prune_after=None, prune_amount=1, verbose=True):
        """
        Train with ARD-KL.
        :param prune_after: epoch number after which to run pruning.
        """
        self.model.train()
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        for epoch in range(1, epochs + 1):
            total_loss = 0.0
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                adj_logits, feat_logits, fitness_pred, mu, logvar, z = self.model(batch.x, batch.edge_index, batch.batch)
                num_graphs = batch.num_graphs
                ptr = batch.ptr  # tensor of size (num_graphs+1)

                # Reconstruction losses
                loss_adj = 0.0
                loss_feat = 0.0
                for i in range(num_graphs):
                    start_idx = ptr[i].item()
                    end_idx = ptr[i+1].item()
                    Ni = end_idx - start_idx
                    if Ni == 0:
                        continue
                    # Build adjacency target
                    edge_mask = ((batch.batch[batch.edge_index[0]] == i) &
                                 (batch.batch[batch.edge_index[1]] == i))
                    edges_i = batch.edge_index[:, edge_mask]
                    src_local = edges_i[0] - start_idx
                    dst_local = edges_i[1] - start_idx
                    adj_target = torch.zeros((Ni, Ni), device=self.device)
                    adj_target[src_local, dst_local] = 1.0
                    # Upper triangle mask (i < j)
                    mask = torch.triu(torch.ones((Ni, Ni), device=self.device), diagonal=1) == 1
                    if mask.any():
                        pred_flat = adj_logits[i, :Ni, :Ni][mask]
                        target_flat = adj_target[mask]
                        loss_adj += F.binary_cross_entropy_with_logits(pred_flat, target_flat)
                    # Node feature loss (one-hot -> class labels)
                    target_features = batch.x[start_idx:end_idx]  # (Ni, node_feat_dim)
                    # Node feature loss (class labels from integer features)
                    target_classes = batch.x[start_idx:end_idx]  # (Ni,)
                    pred_feats = feat_logits[i, :Ni, :]          # (Ni, node_feat_dim)
                    loss_feat += F.cross_entropy(pred_feats, target_classes.to(self.device))

                loss_adj = loss_adj / num_graphs
                loss_feat = loss_feat / num_graphs

                # ARD-KL divergence per sample and per dim
                precision = torch.exp(self.model.log_alpha)
                var = torch.exp(logvar)
                # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                kl_per_dim = 0.5 * (
                    - self.model.log_alpha
                    - logvar
                    + precision * (var + mu.pow(2))
                    - 1
                )
                kl_loss = kl_per_dim.sum(dim=1).mean()
                # Fitness loss
                fitness_target = batch.y.view(-1).to(self.device)
                loss_fitness = F.mse_loss(fitness_pred, fitness_target)
                # Total loss
                loss = loss_adj + loss_feat + kl_weight * kl_loss + fitness_weight * loss_fitness
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
            avg_loss = total_loss / len(loader)
            self.loss_history.append(avg_loss)
            if verbose:
                print(f"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}")
            # Optional pruning
            if prune_after and epoch == prune_after:
                self.prune_low_precision_dims(num_prune=prune_amount)
        return self.loss_history

if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    num_node_types = 3
    max_nodes = 10
    latent_dim = 16

    encoder = GraphEncoder(num_node_types, node_emb_dim=10, hidden_dims=[32, 32], latent_dim=latent_dim)
    decoder = GraphDecoder(latent_dim=latent_dim, max_nodes=max_nodes, node_feat_dim=num_node_types, hidden_dim=128)
    predictor = FitnessPredictor(latent_dim=latent_dim, hidden_dim=64)
    model = DAGVAE(encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random
    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i+1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
        return Data(x=types, edge_index=edge_index)

    # Create synthetic dataset
    graphs = []
    fitnesses = []
    for _ in range(50):
        n = random.randint(3, max_nodes)
        graph = generate_random_dag(n, num_node_types, edge_prob=0.4)
        fit = graph.edge_index.size(1) + 0.1 * random.random()
        graphs.append(graph)
        fitnesses.append(fit)

    trainer = OnlineTrainer(model, optimizer, device=device)
    trainer.add_data(graphs, fitnesses)
    trainer.train(epochs=20, batch_size=8, kl_weight=0.1, fitness_weight=1.0)

    # Continue training with new data
    new_graphs = []
    new_fits = []
    for _ in range(10):
        n = random.randint(3, max_nodes)
        graph = generate_random_dag(n, num_node_types, edge_prob=0.5)
        fit = graph.edge_index.size(1) + 0.1 * random.random()
        new_graphs.append(graph)
        new_fits.append(fit)
    trainer.add_data(new_graphs, new_fits)
    trainer.train(epochs=10, batch_size=8, kl_weight=0.1, fitness_weight=1.0)
