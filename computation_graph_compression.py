# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import add_self_loops, softmax


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """
    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr='add')
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node embeddings and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """
    def __init__(self, num_node_types, node_emb_dim, hidden_dims, latent_dim):
        super().__init__()
        self.node_embedding = nn.Embedding(num_node_types, node_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = node_emb_dim
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    def forward(self, x_type, edge_index, batch):
        # x_type: [num_nodes] long tensor of type indices
        x = self.node_embedding(x_type)  # → [N, node_emb_dim]
        for conv in self.convs:
            x = conv(x, edge_index)
        x = global_mean_pool(x, batch)  # → [batch_size, hidden_dim]
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """
        device = self.lin_mu.weight.device
        in_feats = self.lin_mu.in_features
        new_dim  = kept_idx.numel()

        # mu
        old_mu = self.lin_mu
        new_mu = nn.Linear(in_feats, new_dim, bias=True).to(device)
        new_mu.weight.data.copy_(old_mu.weight.data[kept_idx, :])
        new_mu.bias.data.copy_(old_mu.bias.data[kept_idx])
        self.lin_mu = new_mu

        # logvar
        old_lv = self.lin_logvar
        new_lv = nn.Linear(in_feats, new_dim, bias=True).to(device)
        new_lv.weight.data.copy_(old_lv.weight.data[kept_idx, :])
        new_lv.bias.data.copy_(old_lv.bias.data[kept_idx])
        self.lin_logvar = new_lv


class GraphDecoder(nn.Module):
    """
    Reconstruct adjacency and node features from latent code.
    """
    def __init__(self, latent_dim, max_nodes, node_feat_dim, hidden_dim=128):
        super().__init__()
        self.max_nodes = max_nodes
        self.node_feat_dim = node_feat_dim
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_adj = nn.Linear(hidden_dim, max_nodes * max_nodes)
        self.fc_feat = nn.Linear(hidden_dim, max_nodes * node_feat_dim)

    def forward(self, z):
        batch_size = z.size(0)
        h = F.relu(self.fc1(z))
        adj_logits = self.fc_adj(h).view(batch_size, self.max_nodes, self.max_nodes)
        feat_logits = self.fc_feat(h).view(batch_size, self.max_nodes, self.node_feat_dim)
        return adj_logits, feat_logits

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for decoder.fc1.
        fc_adj and fc_feat stay the same.
        """
        device = self.fc1.weight.device
        out_feats = self.fc1.out_features
        new_dim   = kept_idx.numel()

        old_fc1 = self.fc1
        new_fc1 = nn.Linear(new_dim, out_feats, bias=True).to(device)
        # copy only the kept columns
        new_fc1.weight.data.copy_(old_fc1.weight.data[:, kept_idx])
        new_fc1.bias.data.copy_(old_fc1.bias.data)
        self.fc1 = new_fc1


class FitnessPredictor(nn.Module):
    """Auxiliary predictor: maps latent z to fitness."""
    def __init__(self, latent_dim, hidden_dim=64, fitness_dim=2):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)

    def forward(self, z):
        h = F.relu(self.fc1(z))
        return self.fc2(h).squeeze(-1)

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for predictor.fc1.
        fc2 stays the same.
        """
        device = self.fc1.weight.device
        out_feats = self.fc1.out_features
        new_dim   = kept_idx.numel()

        old_p1 = self.fc1
        new_p1 = nn.Linear(new_dim, out_feats, bias=True).to(device)
        new_p1.weight.data.copy_(old_p1.weight.data[:, kept_idx])
        new_p1.bias.data.copy_(old_p1.bias.data)
        self.fc1 = new_p1


class DAGVAE(nn.Module):
    """
    DAG-VAE with ARD prior and latent-mask for dynamic pruning.
    """
    def __init__(self, encoder: GraphEncoder, decoder: GraphDecoder, predictor: FitnessPredictor):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.predictor = predictor
        # ARD: learnable log-precision per latent dimension
        latent_dim = self.encoder.lin_mu.out_features
        self.log_alpha = nn.Parameter(torch.zeros(latent_dim))
        # mask for active latent dims (1=active, 0=pruned)
        self.register_buffer('latent_mask', torch.ones(latent_dim))

    def encode(self, x, edge_index, batch):
        return self.encoder(x, edge_index, batch)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z * self.latent_mask

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x, edge_index, batch):
        mu, logvar = self.encode(x, edge_index, batch)
        z = self.reparameterize(mu, logvar)
        adj_logits, feat_logits = self.decode(z)
        fitness_pred = self.predictor(z)
        return adj_logits, feat_logits, fitness_pred, mu, logvar, z

    def resize_bottleneck(self, device):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        mask = self.latent_mask.to(device)
        kept_idx = mask.nonzero(as_tuple=True)[0]
        old_dim = mask.numel()
        new_dim = kept_idx.numel()
        if new_dim == old_dim:
            print("No latent dims to permanently prune.")
            return

        print(f"Permanently resizing bottleneck: {old_dim} → {new_dim} dims")

        # delegate to each module
        self.encoder.prune_latent_dims(kept_idx)
        self.decoder.prune_latent_dims(kept_idx)
        self.predictor.prune_latent_dims(kept_idx)
        self.log_alpha = nn.Parameter(self.log_alpha.data[kept_idx].clone().to(device))


        # update metadata
        self.latent_dim  = new_dim
        self.latent_mask = torch.ones(new_dim, dtype=torch.bool, device=device)

        print("Bottleneck resized")


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning.
    """
    def __init__(self, model: DAGVAE, optimizer, device=torch.device('cpu')):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.device = device
        self.dataset = []
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0

    def add_data(self, graphs, fitnesses):
        for graph, fit in zip(graphs, fitnesses):
            data = graph.clone()
            data.y = torch.tensor([fit], dtype=torch.float)
            self.dataset.append(data)

    def compute_ard_precisions(self):
        # precision = exp(log_alpha)
        return torch.exp(self.model.log_alpha).detach().cpu().numpy()

    def prune_low_precision_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        precisions = self.compute_ard_precisions()
        active_mask = self.model.latent_mask.cpu().numpy().astype(bool)
        active_indices = np.where(active_mask)[0]
        active_indices = active_indices[np.argsort(precisions[active_indices])]
        dims_to_prune = active_indices[-num_prune:]
        mask = self.model.latent_mask.clone()
        mask[dims_to_prune.tolist()] = 0.0
        self.model.latent_mask.copy_(mask)
        print(f"Pruned latent dims: {dims_to_prune}")

    def train(self, epochs=1, batch_size=16, kl_weight=1.0, fitness_weight=1.5,
              warmup_epochs=None, loss_threshold=0.9, min_prune_break=5,
              prune_amount=1, min_active_dims=1, stop_epsilon=1E-3, verbose=True):
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        epoch = 1
        num_loss_terms = 4
        prev_loss_terms = np.array([0.0] * num_loss_terms)
        avg_loss_terms = np.array([0.0] * num_loss_terms)
        def stop():
            max_loss_term_change = np.max(abs(avg_loss_terms - prev_loss_terms))
            return (epochs is not None and epoch == epochs + 1) or (epoch > 1 and max_loss_term_change < stop_epsilon)
        while not stop():
            prev_loss_terms = avg_loss_terms
            total_loss_terms = np.array([0.0] * num_loss_terms)
            self.model.train()
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                adj_logits, feat_logits, fitness_pred, mu, logvar, z = self.model(batch.x, batch.edge_index, batch.batch)
                num_graphs = batch.num_graphs
                ptr = batch.ptr  # tensor of size (num_graphs+1)

                # Reconstruction losses
                loss_adj = 0.0
                loss_feat = 0.0
                for i in range(num_graphs):
                    start_idx = ptr[i].item()
                    end_idx = ptr[i+1].item()
                    Ni = end_idx - start_idx
                    if Ni == 0:
                        continue
                    # Build adjacency target
                    edge_mask = ((batch.batch[batch.edge_index[0]] == i) &
                                 (batch.batch[batch.edge_index[1]] == i))
                    edges_i = batch.edge_index[:, edge_mask]
                    src_local = edges_i[0] - start_idx
                    dst_local = edges_i[1] - start_idx
                    adj_target = torch.zeros((Ni, Ni), device=self.device)
                    adj_target[src_local, dst_local] = 1.0
                    # Upper triangle mask (i < j)
                    mask = torch.triu(torch.ones((Ni, Ni), device=self.device), diagonal=1) == 1
                    if mask.any():
                        pred_flat = adj_logits[i, :Ni, :Ni][mask]
                        target_flat = adj_target[mask]
                        loss_adj += F.binary_cross_entropy_with_logits(pred_flat, target_flat)
                    # Node feature loss (one-hot -> class labels)
                    target_features = batch.x[start_idx:end_idx]  # (Ni, node_feat_dim)
                    # Node feature loss (class labels from integer features)
                    target_classes = batch.x[start_idx:end_idx]  # (Ni,)
                    pred_feats = feat_logits[i, :Ni, :]          # (Ni, node_feat_dim)
                    loss_feat += F.cross_entropy(pred_feats, target_classes.to(self.device))

                loss_adj = loss_adj / num_graphs
                loss_feat = loss_feat / num_graphs

                # ARD-KL divergence per sample and per dim
                precision = torch.exp(self.model.log_alpha)
                var = torch.exp(logvar)
                # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                kl_per_dim = 0.5 * (
                    - self.model.log_alpha
                    - logvar
                    + precision * (var + mu.pow(2))
                    - 1
                )
                kl_loss = kl_per_dim.sum(dim=1).mean()
                loss_fitness = F.mse_loss(fitness_pred, batch.y.to(self.device))
                loss_terms = [loss_adj, loss_feat, kl_weight * kl_loss, fitness_weight * loss_fitness]
                loss = sum(loss_terms)
                loss.backward()
                self.optimizer.step()
                total_loss_terms += np.array([t.detach().numpy() for t in loss_terms])
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms)
            if verbose:
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")
                else:
                    print(f"Epoch {epoch}/{epochs}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")

            # set baseline after warmup
            if epoch == warmup_epochs:
                self.initial_loss = avg_loss_terms.sum()

            # pruning condition
            if (self.initial_loss is not None
                and avg_loss_terms.sum() <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.latent_mask.sum().item())
                if active_count > min_active_dims:
                    self.prune_low_precision_dims(num_prune=prune_amount)
                    self.last_prune_epoch = epoch
                    self.initial_loss = avg_loss_terms.sum()  # reset baseline

            epoch += 1
        return self.loss_history

    def resize_bottleneck(self):
        self.model.resize_bottleneck(self.device)
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")


if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    num_node_types = 3
    max_nodes = 10
    latent_dim = 16
    fitness_dim = 2

    encoder = GraphEncoder(num_node_types, node_emb_dim=10, hidden_dims=[32, 32], latent_dim=latent_dim)
    decoder = GraphDecoder(latent_dim=latent_dim, max_nodes=max_nodes, node_feat_dim=num_node_types, hidden_dim=128)
    predictor = FitnessPredictor(latent_dim=latent_dim, hidden_dim=64, fitness_dim=fitness_dim)
    model = DAGVAE(encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random
    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i+1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
        return Data(x=types, edge_index=edge_index)

    # Create synthetic dataset
    print('Generating random graphs')
    def generate_data(num_fitnesses):
        graphs, fitnesses = [], []
        for _ in range(num_fitnesses):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            fit = []
            for f in range(fitness_dim):
                fit.append(graph.edge_index.size(1) + 0.1 * random.random())
            graphs.append(graph)
            fitnesses.append(fit)
        return graphs, fitnesses
    graphs, fitnesses = generate_data(50)

    trainer = OnlineTrainer(model, optimizer, device=device)
    trainer.add_data(graphs, fitnesses)
    print('Training')
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10)
    trainer.resize_bottleneck()

    # Continue training with new data
    graphs, fitnesses = generate_data(10)
    trainer.add_data(graphs, fitnesses)
    print('Continuing training')
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
