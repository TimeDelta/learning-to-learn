# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.utils import add_self_loops, softmax, degree, to_dense_adj, to_dense_batch

from typing import Dict, Any
from warnings import warn

from tasks import TASK_FEATURE_DIMS, TASK_TYPE_TO_INDEX


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """
    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr='add')
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class TasksEncoder(nn.Module):
    def __init__(self, hidden_dim:int, latent_dim:int, type_embedding_dim:int):
        super().__init__()
        # Separate module per task type because each task has different number of features
        # and features don't semantically align
        self.latent_dim = latent_dim
        self.embedder = nn.Embedding(len(TASK_FEATURE_DIMS), type_embedding_dim)
        self.encoders = nn.ModuleDict({
            str(TASK_TYPE_TO_INDEX[type]): nn.Sequential(
                nn.Linear(num_features + type_embedding_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            ) for type, num_features in TASK_FEATURE_DIMS.items()
        })
        # Shared mu/logvar heads
        self.lin_mu     = nn.Linear(hidden_dim, latent_dim)
        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, task_types, feature_vecs):
        """
        task_types: list[str] of length B
        feature_vecs: list[Tensor] of length B, each shape (feat_dim,)
        """
        mu_list, logvar_list = [], []
        for task_type, feature_vec in zip(task_types, feature_vecs):
            task_type_embedding = self.embedder(task_type)
            feature_vec = torch.as_tensor(feature_vec, dtype=torch.float, device=device)
            h = self.encoders[str(task_type.item())](torch.cat((task_type_embedding, feature_vec), dim=0))
            mu_list.append(self.lin_mu(h))
            logvar_list.append(self.lin_logvar(h))
        # stack into B×latent_dim tensors
        return torch.stack(mu_list, dim=0), torch.stack(logvar_list, dim=0)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if self.latent_mask is None else z * self.latent_mask

    def prune_latent_dims(self, kept_idx:torch.LongTensor):
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node embeddings and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """
    def __init__(self, num_node_types, node_emb_dim, hidden_dims, latent_dim):
        super().__init__()
        self.latent_dim = latent_dim
        self.node_embedding = nn.Embedding(num_node_types, node_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = node_emb_dim
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    def forward(self, x_type, edge_index, batch):
        # x_type: [num_nodes] long tensor of type indices
        x = self.node_embedding(x_type)  # → [N, node_emb_dim]
        for conv in self.convs:
            x = conv(x, edge_index)
        x = global_mean_pool(x, batch)  # → [batch_size, hidden_dim]
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class GraphDeconvNet(MessagePassing):
    """
    A Graph Deconvolutional Network (GDN) layer that acts as the
    transpose/inverse of a GCNConv.  It takes an input signal X of
    size [N, in_channels] on a graph with edge_index, and produces
    an output signal of size [N, out_channels], without any fixed
    max_nodes or feature‐size assumptions.
    """
    def __init__(self, in_channels:int, out_channels:int, aggr:str='add'):
        super().__init__(aggr=aggr)
        # weight for the "transpose" convolution
        self.lin = nn.Linear(in_channels, out_channels, bias=True)
        # optional bias after aggregation
        self.bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, edge_index: torch.LongTensor, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            edge_index: LongTensor of shape [2, E] with COO edges.
            x:          FloatTensor of shape [N, in_channels] node signals.
        Returns:
            FloatTensor of shape [N, out_channels].
        """
        x = self.lin(x) # (acts like W^T in a transposed convolution)
        # if there are no edges, skip the propagate/unpack step
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return F.relu(x + self.bias) # just apply bias + activation
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        out = self.propagate(edge_index, x=x, norm=deg.pow(0.5)[col] * deg.pow(0.5)[row]) # each node collects from neighbors
        return out + self.bias

    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:
        # x_j: neighbor features [E, out_channels]
        # norm: normalization per edge [E]
        return x_j * norm.view(-1, 1)

    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:
        # optional nonlinearity
        return F.relu(aggr_out)


class ARGraphDecoder(nn.Module):
    """
    Autoregressive generation + GDN refinement.
    1) Autoregressively generate node embeddings & edges (GraphRNN style).
    2) Refine embeddings via Graph Deconvolutional Nets (GDNs).
    """
    def __init__(self, latent_dim:int, hidden_dim:int=128, gdn_layers:int=2):
        super().__init__()
        # — map latent → initial Node‐RNN state
        self.hidden_node_state_init = nn.Linear(latent_dim, hidden_dim)

        # — Node‐RNN (no input, only hidden evolves)
        self.node_rnn = nn.GRUCell(input_size=0, hidden_size=hidden_dim)
        self.fc_stop  = nn.Linear(hidden_dim, 1)
        self.fc_node  = nn.Linear(hidden_dim, hidden_dim)
        self.edge_rnn = nn.GRUCell(input_size=1, hidden_size=hidden_dim)
        self.fc_edge  = nn.Linear(hidden_dim, 1)

        # — GraphDeconvNet stack (learned spectral decoders)
        self.gdns = nn.ModuleList([GraphDeconvNet(hidden_dim, hidden_dim) for _ in range(gdn_layers)])

    def forward(self, latent):
        """
        (num_graphs, latent_dim)
        returns: list of graphs, each {'node_features': Tensor[N, hidden_dim],
                                       'edge_index': LongTensor[2, E]}
        """
        device = latent.device
        all_graphs = []

        for l in range(latent.size(0)):
            hidden_node = F.relu(self.hidden_node_state_init(latent[l])).unsqueeze(0) # (hidden_dim,)
            node_features = []
            edges = []
            t = 0
            while True:
                # inside forward, for each graph or each time step:
                hidden_node = self.node_rnn(torch.zeros(hidden_node.shape[0], 0, device=device), hidden_node)

                p_stop = torch.sigmoid(self.fc_stop(hidden_node))
                if torch.bernoulli(1 - p_stop).item() == 0:
                    break
                new_node = self.fc_node(hidden_node).squeeze(0)
                node_features.append(new_node)

                # edge generation to previous nodes
                hidden_edge = hidden_node
                edge_in = torch.zeros(1, 1, device=device)
                for i in range(t):
                    hidden_edge = self.edge_rnn(edge_in, hidden_edge)
                    p_edge = torch.sigmoid(self.fc_edge(hidden_edge)).view(-1)
                    if torch.bernoulli(p_edge).item() == 1:
                        edges.append([i, t])
                    edge_in = p_edge.unsqueeze(0)
                t += 1

            if node_features:
                X = torch.stack(node_features, dim=0)
                E = torch.tensor(edges, dtype=torch.long).t().contiguous()
            else:
                X = torch.zeros((0, self.fc_node.out_features), device=device)
                E = torch.zeros((2, 0), dtype=torch.long, device=device)

            # refine via graph deconvolution
            for gdn in self.gdns:
                X = gdn(E, X)

            all_graphs.append({'node_features': X, 'edge_index': E})

        return all_graphs

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Permanently remove unused latent dimensions from the hidden_node_state_init layer.
        kept_idx: 1D LongTensor of indices to keep from the original latent vector.
        """
        device = self.hidden_node_state_init.weight.device
        new_hidden_node_state_init = nn.Linear(kept_idx.numel(), self.hidden_node_state_init.out_features, bias=True).to(device)
        new_hidden_node_state_init.weight.data.copy_(old.weight.data[:, kept_idx])

        new_hidden_node_state_init.bias.data.copy_(old.bias.data)
        self.hidden_node_state_init = new_hidden_node_state_init
        self.latent_dim = kept_idx.numel()

class FitnessPredictor(nn.Module):
    """
    Auxiliary predictor: maps concatentated graph and task latents (z_g, z_t) to fitness.
    """
    def __init__(self, latent_dim, hidden_dim=64, fitness_dim=2):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)

    def forward(self, z_graph, z_task):
        return self.fc2(F.relu(self.fc1(torch.cat([z_graph, z_task], dim=-1))))

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for predictor.fc1.
        fc2 stays the same.
        """
        device = self.fc1.weight.device

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.fc1 = prune_lin(self.fc1)


class DAGTaskFitnessRegularizedVAE(nn.Module):
    """
    DAG-VAE with ARD prior and latent-mask for dynamic pruning.
    """
    def __init__(self,
        graph_encoder: GraphEncoder,
        tasks_encoder: TasksEncoder,
        decoder: ARGraphDecoder,
        fitness_predictor: FitnessPredictor
    ):
        super().__init__()
        self.graph_encoder = graph_encoder
        self.decoder = decoder
        self.fitness_predictor = fitness_predictor
        self.tasks_encoder = tasks_encoder
        # ARD: learnable log-precision per latent dimension
        self.log_alpha_g = nn.Parameter(torch.zeros(self.graph_encoder.latent_dim))
        self.log_alpha_t = nn.Parameter(torch.zeros(self.tasks_encoder.latent_dim))
        # masks for active latent dims (1=active, 0=pruned)
        self.register_buffer('graph_latent_mask', torch.ones(self.graph_encoder.latent_dim))
        self.register_buffer('tasks_latent_mask', torch.ones(self.tasks_encoder.latent_dim))

    def encode(self, x, edge_index, batch, task_type, task_features):
        mu_g, lv_g = self.graph_encoder(x, edge_index, batch)
        mu_t, lv_t = self.tasks_encoder(task_type, task_features)
        return mu_g, lv_g, mu_t, lv_t

    def reparameterize(self, mu, logvar, latent_mask):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if latent_mask is None else z * latent_mask

    def decode(self, z):
        graphs = self.decoder(z)
        return [{'edge_index': g['edge_index'], 'node_features': g['node_features']} for g in graphs]

    def forward(self, x, edge_index, batch, task_type, task_features):
        mu_g, lv_g, mu_t, lv_t = self.encode(x, edge_index, batch, task_type, task_features)
        graph_latent = self.reparameterize(mu_g, lv_g, self.graph_latent_mask)
        task_latent = self.reparameterize(mu_t, lv_t, self.tasks_latent_mask)
        decoded_graphs = self.decode(graph_latent)
        fitness_pred = self.fitness_predictor(graph_latent, task_latent)
        return decoded_graphs, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t

    def prune_latent_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        def prune(latent_mask, log_alpha, desc):
            # precision = exp(log_alpha)
            latent_mask = latent_mask.cpu().numpy().astype(bool)
            precisions = torch.exp(log_alpha).detach().cpu().numpy();
            active_indices = np.where(latent_mask)[0]
            active_indices = active_indices[np.argsort(precisions[active_indices])]
            dims_to_prune = active_indices[-num_prune:]
            dims_to_prune = [d for d in dims_to_prune.tolist() if precisions[d] > 1]
            latent_mask[dims_to_prune] = 0.0
            if len(dims_to_prune) > 0:
                print(f"Pruned {desc} latent dims: {dims_to_prune}")
            return torch.tensor(latent_mask)
        self.graph_latent_mask.copy_(prune(self.graph_latent_mask, self.log_alpha_g, 'graph'))
        self.tasks_latent_mask.copy_(prune(self.tasks_latent_mask, self.log_alpha_t, 'tasks'))

    def resize_bottleneck(self):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        graph_kept_idx = self.graph_latent_mask.nonzero(as_tuple=True)[0]
        old_graph_dim = self.graph_latent_mask.numel()
        new_graph_dim = graph_kept_idx.numel()
        if new_graph_dim == old_graph_dim:
            print("No graph latent dims to permanently prune.")
            return
        task_kept_idx = self.tasks_latent_mask.nonzero(as_tuple=True)[0]
        old_task_dim = self.tasks_latent_mask.numel()
        new_task_dim = task_kept_idx.numel()
        if new_task_dim == old_task_dim:
            print("No task latent dims to permanently prune.")
            return

        print(f"Permanently resizing graph bottleneck: {old_graph_dim} → {new_graph_dim}; task bottleneck: {old_task_dim} → {new_task_dim}")

        # Prune graph-related modules
        self.graph_encoder.prune_latent_dims(graph_kept_idx)
        self.decoder.prune_latent_dims(graph_kept_idx)
        self.fitness_predictor.prune_latent_dims(graph_kept_idx)
        self.tasks_encoder.prune_latent_dims(task_kept_idx)
        # Update ARD precision parameters for graph and task
        self.log_alpha_g = nn.Parameter(self.log_alpha_g.data[graph_kept_idx].clone())
        self.log_alpha_t = nn.Parameter(self.log_alpha_t.data[task_kept_idx].clone())
        # Update latent masks
        self.register_buffer('graph_latent_mask', torch.ones(graph_kept_idx.numel()))
        self.register_buffer('tasks_latent_mask', torch.ones(task_kept_idx.numel()))

        # update metadata
        self.latent_dim  = new_dim
        self.latent_mask = torch.ones(new_dim, dtype=torch.bool, device=device)

        print("Bottlenecks resized")


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning,
    supporting variable node-feature dimensions via list-of-graphs decoding.
    """
    def __init__(self, model: DAGTaskFitnessRegularizedVAE, optimizer, device=torch.device('cpu')):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.device = device
        self.dataset = []
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0

    def add_data(self, graphs, fitnesses, task_types, task_feats):
        """
        Each Data in self.dataset must have:
         - .x : Tensor[Ni, Fi]      (raw node features)
         - .edge_index : [2, Ei]
         - .y : Tensor[1, fitness_dim]
         - .task_type : Tensor[1]
         - .task_features : list of length Fi_task
        """
        for graph, fitness, task_type, task_features in zip(graphs, fitnesses, task_types, task_feats):
            data = graph.clone()
            data.y = torch.as_tensor(fitness, dtype=torch.float).unsqueeze(0)
            data.task_type = torch.tensor([TASK_TYPE_TO_INDEX[task_type]], dtype=torch.long)
            if isinstance(task_features, torch.Tensor):
                data.task_features = task_features.detach().cpu().tolist()
            else:
                data.task_features = list(task_features)
            self.dataset.append(data)

    def train(self, epochs=1, batch_size=16, kl_weight=1.0, fitness_weight=1.5,
              warmup_epochs=None, loss_threshold=0.9, min_prune_break=5,
              prune_amount=1, min_active_dims=5, stop_epsilon=1E-3, verbose=True):
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        epoch = 1
        num_loss_terms = 5
        prev_loss_terms = torch.zeros(num_loss_terms)
        avg_loss_terms = torch.zeros(num_loss_terms)

        def stop():
            max_loss_term_change = (avg_loss_terms - prev_loss_terms).abs().max().item()
            return (epochs is not None and epoch == epochs + 1) or (epoch > 1 and max_loss_term_change < stop_epsilon)

        while not stop():
            prev_loss_terms = avg_loss_terms.clone()
            total_loss_terms = torch.zeros(num_loss_terms)
            self.model.train()
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()

                # --- 1) forward pass ---
                # assume model.forward now returns:
                #   decoded_graphs: List[{'edge_index', 'node_features'}] (len=B)
                #   fitness_pred: Tensor[B, fitness_dim]
                #   mu_g, lv_g, mu_t, lv_t: Tensors[B, latent_dim]
                decoded_graphs, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t = self.model(
                    batch.x, batch.edge_index, batch.batch, batch.task_type, batch.task_features
                )
                # --- trivial task-latent check ---
                mean_var = task_latent.var(dim=0).mean().item()
                if mean_var < 1E-3:
                    warn(f"Task latent collapsed (mean variance={mean_var:.2e}): Consider adding task reconstruction to cost")

                dense_adj = to_dense_adj(batch.edge_index, batch.batch, max_num_nodes=None)
                # slice out GT node-features per graph
                gt_node_features = []
                for i in range(batch.num_graphs):
                    mask_i = (batch.batch == i)
                    gt_node_features.append(batch.x[mask_i])

                # --- 2) reconstruction losses ---
                loss_adj, loss_feat = 0.0, 0.0
                for i, dg in enumerate(decoded_graphs):
                    edge_predictions = dg['edge_index']
                    pred_features = dg['node_features']
                    target_features = gt_node_features[i]
                    Ni_pred = pred_features.size(0)
                    Ni_gt   = target_features.size(0)
                    if Ni_pred == 0 or Ni_gt == 0:
                        continue

                    # compare only the overlap
                    Ni = min(Ni_pred, Ni_gt)
                    # build binary adjacency preds/targets on 0..Ni-1
                    adj_pred = torch.zeros((Ni,Ni), device=self.device)
                    for s,d in edge_predictions.t().tolist():
                        if s < Ni and d < Ni:
                            adj_pred[s,d] = 1.0
                    adj_true = dense_adj[i, :Ni, :Ni]

                    mask = torch.triu(torch.ones_like(adj_pred), diagonal=1).bool()
                    if mask.any():
                        pred_flat = adj_pred[mask]
                        true_flat = adj_true[mask]
                        loss_adj += F.binary_cross_entropy_with_logits(pred_flat, true_flat)

                    # node feature loss
                    if pred_features.ndim < 2 and target_features.ndim < 2:
                        continue # correctly predicted no features present
                    if pred_features.ndim < 2:
                        loss_feat += pred_features[:Ni,].sum()
                        continue
                    if target_features.ndim < 2:
                        loss_feat += target_features[:Ni,].sum()
                        continue
                    feat_overlap = min(pred_features.size(1), target_features.size(1))
                    loss_feat += F.mse_loss(pred_features[:Ni, :feat_overlap], target_features[:Ni, :feat_overlap].to(self.device))
                    if pred_features.size(1) < target_features.size(1):
                        loss_feat += target_features[:Ni, feat_overlap:].sum()
                    elif pred_features.size(1) > target_features.size(1):
                        loss_feat += pred_features[:Ni, feat_overlap:].sum()

                loss_adj  /= batch.num_graphs
                loss_feat /= batch.num_graphs

                # --- 3) ARD-KL losses ---
                def calc_kl_div_per_dim(log_alpha, logvar, mu):
                    # ARD-KL divergence per sample and per dim
                    precision = torch.exp(log_alpha)
                    var = torch.exp(logvar)
                    # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                    return 0.5 * (
                        - log_alpha
                        - logvar
                        + precision * (var + mu.pow(2))
                        - 1
                    )
                graph_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_g, lv_g, mu_g).sum(dim=1).mean()
                task_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_t, lv_t, mu_t).sum(dim=1).mean()

                # --- 4) fitness loss ---
                loss_fitness = F.mse_loss(fitness_pred, batch.y.to(self.device))

                # --- 5) total & backward ---
                loss_terms = [
                    loss_adj,
                    loss_feat,
                    kl_weight*graph_kl_loss,
                    kl_weight*task_kl_loss,
                    fitness_weight*loss_fitness,
                ]
                sum(loss_terms).backward()
                self.optimizer.step()
                total_loss_terms += torch.tensor([t for t in loss_terms])
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms.cpu().numpy())

            if verbose:
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")
                else:
                    print(f"Epoch {epoch}/{epochs}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")

            # warm-up baseline
            if epoch == warmup_epochs:
                self.initial_loss = avg_loss_terms.sum().item()

            # pruning condition
            if (self.initial_loss is not None
                and avg_loss_terms.sum().item() <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.graph_latent_mask.sum().item())
                if active_count > min_active_dims:
                    self.model.prune_latent_dims(num_prune=prune_amount)
                    self.last_prune_epoch = epoch
                    self.initial_loss = avg_loss_terms.sum()  # reset baseline

            epoch += 1
        return self.loss_history

    def resize_bottleneck(self):
        """Rebuild all modules to permanently prune inactive dims."""
        self.model.resize_bottleneck(self.device)
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")


if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    num_node_types = 3
    max_nodes = 10
    graph_latent_dim = 32
    fitness_dim = 2
    task_latent_dim = graph_latent_dim - 2

    graph_encoder = GraphEncoder(num_node_types, node_emb_dim=10, hidden_dims=[32, 32], latent_dim=graph_latent_dim)
    task_encoder = TasksEncoder(hidden_dim=16, latent_dim=task_latent_dim, type_embedding_dim=8)
    decoder = ARGraphDecoder(latent_dim=graph_latent_dim, hidden_dim=128)
    predictor = FitnessPredictor(latent_dim=graph_latent_dim+task_latent_dim, hidden_dim=64, fitness_dim=fitness_dim)
    model = DAGTaskFitnessRegularizedVAE(graph_encoder, task_encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random
    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i+1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
        return Data(x=types, edge_index=edge_index)

    # Create synthetic dataset
    print('Generating random training data')
    def generate_data(num_samples):
        graphs, fitnesses, task_types, task_features = [], [], [], []
        for _ in range(num_samples):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            graphs.append(graph)
            fitnesses.append([graph.edge_index.size(1) + 0.1 * random.random() for _ in range(fitness_dim)])
            task_type = random.choice(list(TASK_FEATURE_DIMS.keys()))
            task_types.append(task_type)
            task_features.append(torch.randn((TASK_FEATURE_DIMS[task_type],)))
        return graphs, fitnesses, task_types, task_features

    trainer = OnlineTrainer(model, optimizer, device=device)
    trainer.add_data(*generate_data(50))
    print('Training')
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10)
    trainer.resize_bottleneck()

    # Continue training with new data
    trainer.add_data(*generate_data(10))
    print('Continuing training')
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
