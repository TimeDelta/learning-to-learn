# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.utils import add_self_loops, softmax, degree, to_dense_adj, to_dense_batch

import math
from typing import Dict, Any, List
from warnings import warn

from tasks import TASK_FEATURE_DIMS, TASK_TYPE_TO_INDEX
from utility import generate_random_string


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """
    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr='add')
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class TasksEncoder(nn.Module):
    def __init__(self, hidden_dim:int, latent_dim:int, type_embedding_dim:int):
        super().__init__()
        # Separate module per task type because each task has different number of features
        # and features don't semantically align
        self.latent_dim = latent_dim
        self.embedder = nn.Embedding(len(TASK_FEATURE_DIMS), type_embedding_dim)
        self.encoders = nn.ModuleDict({
            str(TASK_TYPE_TO_INDEX[type]): nn.Sequential(
                nn.Linear(num_features + type_embedding_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            ) for type, num_features in TASK_FEATURE_DIMS.items()
        })
        # Shared mu/logvar heads
        self.lin_mu     = nn.Linear(hidden_dim, latent_dim)
        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, task_types, feature_vecs):
        """
        task_types: list[str] of length B
        feature_vecs: list[Tensor] of length B, each shape (feat_dim,)
        """
        mu_list, logvar_list = [], []
        for task_type, feature_vec in zip(task_types, feature_vecs):
            task_type_embedding = self.embedder(task_type)
            feature_vec = torch.as_tensor(feature_vec, dtype=torch.float)
            h = self.encoders[str(task_type.item())](torch.cat((task_type_embedding, feature_vec), dim=0))
            mu_list.append(self.lin_mu(h))
            logvar_list.append(self.lin_logvar(h))
        # stack into B×latent_dim tensors
        return torch.stack(mu_list, dim=0), torch.stack(logvar_list, dim=0)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if self.latent_mask is None else z * self.latent_mask

    def prune_latent_dims(self, kept_idx:torch.LongTensor):
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class SharedAttributeVocab(nn.Module):
    """
    Holds one name-to-index mapping plus a single Embedding that
    can grow on‐the‐fly as new names are added.
    """
    def __init__(self, initial_names:List[str], embedding_dim:int):
        super().__init__()
        self.name_to_index = {name: i for i, name in enumerate(initial_names)}
        self.name_to_index['<UNK>'] = len(self.name_to_index)
        self.name_to_index['<EOS>'] = len(self.name_to_index)
        self.index_to_name = {i: name for name, i in self.name_to_index.items()}
        self.embedding = nn.Embedding(len(self.name_to_index), embedding_dim)

    def add_names(self, new_names:List[str]):
        names_to_add = [name for name in new_names if name not in self.name_to_index]

        starting_index = len(self.name_to_index)
        num_new_names = len(names_to_add)

        # 3) Assign a new index to each new name
        for offset, name in enumerate(names_to_add):
            new_idx = starting_index + offset
            self.name_to_index[name] = new_idx
            self.index_to_name[new_idx] = name

        # expand the embedding matrix with He‐style initialization for the new rows
        old_weight = self.embedding.weight.data
        fan_in = old_weight.size(1)
        new_rows = torch.randn(len(names_to_add), fan_in) * math.sqrt(2/fan_in)
        new_weight = torch.cat([old_weight, new_rows], dim=0)
        self.embedding = nn.Embedding.from_pretrained(new_weight, freeze=False)


class NodeAttributeDeepSetEncoder(nn.Module):
    """
    Permutation‐invariant encoder for a dictionary of arbitrary node attributes.
    See "Deep Sets" by Zaheer et al. at https://arxiv.org/abs/1703.06114
    """
    def __init__(self,
        shared_attr_vocab: SharedAttributeVocab,
        encoder_hdim:int,
        aggregator_hdim:int,
        out_dim:int
    ):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.max_value_dim = shared_attr_vocab.embedding.embedding_dim
        self.attr_encoder = nn.Sequential( # phi
            nn.Linear(shared_attr_vocab.embedding.embedding_dim + self.max_value_dim, encoder_hdim),
            nn.ReLU(),
            nn.Linear(encoder_hdim, encoder_hdim),
            nn.ReLU()
        )
        self.aggregator = nn.Sequential( # rho
            nn.Linear(encoder_hdim, aggregator_hdim),
            nn.ReLU(),
            nn.Linear(aggregator_hdim, out_dim),
        )
        self.out_dim = out_dim

    def get_value_tensor(self, value:Any):
        if isinstance(value, (int, float)):
            value = torch.tensor([value], dtype=torch.float)
        elif isinstance(value, str):
            index = self.shared_attr_vocab.name_to_index.get(value, self.shared_attr_vocab.name_to_index['<UNK>'])
            index = torch.tensor(index, dtype=torch.long)
            value = self.shared_attr_vocab.embedding(index)
        else:
            raise TypeError(f"Unsupported attribute value type: {type(value)}")
        value = value.view(-1)
        if value.numel() < self.max_value_dim:
            pad_amt = self.max_value_dim - value.numel()
            return F.pad(value, (0, pad_amt), "constant", 0.0)
        else:
            return value[:self.max_value_dim]

    def forward(self, attr_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        if not attr_dict:
            return torch.zeros(self.aggregator[-1].out_features)

        phis = []
        for name, value in attr_dict.items():
            name_index = torch.tensor(self.shared_attr_vocab.name_to_index[name], dtype=torch.long)
            value = self.get_value_tensor(value)
            phis.append(self.attr_encoder(torch.cat([self.shared_attr_vocab.embedding(name_index), value], dim=0)))
        return self.aggregator(torch.stack(phis, dim=0).sum(dim=0))


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node type embeddings, deep set encoder for node attributes and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """
    def __init__(self, num_node_types:int, attr_encoder:NodeAttributeDeepSetEncoder, latent_dim:int, hidden_dims:List[int]):
        super().__init__()
        self.latent_dim = latent_dim
        self.attr_encoder = attr_encoder
        attr_emb_dim = attr_encoder.out_dim
        self.node_type_embedding = nn.Embedding(num_node_types, attr_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = attr_emb_dim * 2
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    def forward(self, node_types, edge_index, node_attributes, batch):
        type_embedding = self.node_type_embedding(node_types)
        attr_embedding = torch.stack([self.attr_encoder(attrs) for graph in node_attributes for attrs in graph], dim=0)
        x = torch.cat([type_embedding, attr_embedding], dim=-1)
        for conv in self.convs:
            x = conv(x, edge_index)
        x = global_mean_pool(x, batch)
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class GraphDeconvNet(MessagePassing):
    """
    A Graph Deconvolutional Network (GDN) layer that acts as the
    transpose/inverse of a GCNConv.  It takes an input signal X of
    size [N, in_channels] on a graph with edge_index, and produces
    an output signal of size [N, out_channels], without any fixed
    max_nodes or feature‐size assumptions.
    """
    def __init__(self, in_channels:int, out_channels:int, aggr:str='add'):
        super().__init__(aggr=aggr)
        # weight for the "transpose" convolution
        self.lin = nn.Linear(in_channels, out_channels, bias=True)
        # optional bias after aggregation
        self.bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, edge_index: torch.LongTensor, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            edge_index: LongTensor of shape [2, E] with COO edges.
            x:          FloatTensor of shape [N, in_channels] node signals.
        Returns:
            FloatTensor of shape [N, out_channels].
        """
        x = self.lin(x) # (acts like W^T in a transposed convolution)
        # if there are no edges, skip the propagate/unpack step
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return F.relu(x + self.bias) # just apply bias + activation
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        out = self.propagate(edge_index, x=x, norm=deg.pow(0.5)[col] * deg.pow(0.5)[row]) # each node collects from neighbors
        return out + self.bias

    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:
        # x_j: neighbor features [E, out_channels]
        # norm: normalization per edge [E]
        return x_j * norm.view(-1, 1)

    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:
        # optional nonlinearity
        return F.relu(aggr_out)


class GraphDecoder(nn.Module):
    """
    Recurrent generation + GDN refinement.
    1) Recurrently generate node embeddings & edges (GraphRNN style).
    2) Refine embeddings via Graph Deconvolutional Nets (GDNs).
    """
    def __init__(self, num_node_types:int, latent_dim:int, shared_attr_vocab:SharedAttributeVocab, hidden_dim:int=128, gdn_layers:int=2):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.latent_dim = latent_dim
        # — map latent → initial Node‐RNN state
        self.node_hidden_state_init = nn.Linear(latent_dim, hidden_dim)

        # — Node‐RNN (no input, only hidden evolves)
        self.node_rnn  = nn.GRUCell(input_size=0, hidden_size=hidden_dim)
        self.stop_head = nn.Linear(hidden_dim, 1)
        self.node_head = nn.Linear(hidden_dim, hidden_dim)
        self.type_head = nn.Linear(hidden_dim, num_node_types)
        self.edge_rnn  = nn.GRUCell(input_size=1, hidden_size=hidden_dim)
        self.edge_head = nn.Linear(hidden_dim, 1)

        self.attr_type_rnn  = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_type_head = nn.Linear(hidden_dim, 1)
        self.attr_name_rnn  = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_name_head = nn.Linear(hidden_dim, shared_attr_vocab.embedding.embedding_dim)
        self.attr_dims_head = nn.Linear(hidden_dim, 1) # determine num dimensions per attr
        self.attr_val_rnn   = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_val_head  = nn.Linear(hidden_dim, 1) # extract value per attr dimension

        # — GraphDeconvNet stack (learned spectral decoders)
        self.gdns = nn.ModuleList([GraphDeconvNet(hidden_dim, hidden_dim) for _ in range(gdn_layers)])
        self.max_nodes = 1000
        self.max_attributes_per_node = 50

    def forward(self, latent):
        """
        (num_graphs, latent_dim)
        returns: list of graphs, each {'node_types': LongTensor[1, N],
                                       'node_attributes': List[{name: attribute_value} x N],
                                       'edge_index': LongTensor[2, E]}
        """
        device = latent.device
        all_graphs = []

        for l in range(latent.size(0)):
            hidden_node = F.relu(self.node_hidden_state_init(latent[l])).unsqueeze(0) # (hidden_dim,)
            node_embeddings = []
            edges = []
            node_types = []
            t = 0
            while True:
                if t > self.max_nodes:
                    warn('max nodes reached')
                    break
                hidden_node = self.node_rnn(torch.zeros(hidden_node.shape[0], 0, device=device), hidden_node)

                p_stop = torch.sigmoid(self.stop_head(hidden_node))
                if torch.bernoulli(1 - p_stop).item() == 0:
                    break
                new_node = self.node_head(hidden_node).squeeze(0)
                node_embeddings.append(new_node)
                node_types.append(self.type_head(new_node).argmax(dim=-1).cpu().tolist())

                # edge generation to previous nodes
                hidden_edge = hidden_node
                edge_in = torch.zeros(1, 1, device=device)
                for i in range(t):
                    hidden_edge = self.edge_rnn(edge_in, hidden_edge)
                    p_edge = torch.sigmoid(self.edge_head(hidden_edge)).view(-1)
                    if torch.bernoulli(p_edge).item() == 1:
                        edges.append([i, t])
                    edge_in = p_edge.unsqueeze(0)
                t += 1

            if node_embeddings:
                node_embeddings = torch.stack(node_embeddings, dim=0)
                edges = torch.tensor(edges, dtype=torch.long).t().contiguous()
            else:
                node_embeddings = torch.zeros((0, self.node_head.out_features), device=device)
                edges = torch.zeros((2, 0), dtype=torch.long, device=device)

            # refine via graph deconvolution
            for gdn in self.gdns:
                node_embeddings = gdn(edges, node_embeddings)

            node_attributes = []
            for embedding in node_embeddings:
                attrs = {}
                name_hidden = embedding.unsqueeze(0)
                val_hidden  = None
                t = 0
                while True:
                    if t > self.max_attributes_per_node:
                        warn('max attributes per node reached')
                        break
                    name_input = torch.zeros(1, 1, device=device)
                    name_out, name_hidden = self.attr_name_rnn(name_input, name_hidden)
                    similarity_logits = torch.matmul(
                        self.shared_attr_vocab.embedding.weight, # [vocab_size, embedding_dim]
                        self.attr_name_head(name_out).squeeze(0) # query vector [embedding_dim]
                    )
                    name_index = int(similarity_logits.argmax().item())
                    if name_index == self.shared_attr_vocab.name_to_index['<EOS>']:
                        break
                    elif name_index == self.shared_attr_vocab.name_to_index['<UNK>']:
                        name_index = len(self.shared_attr_vocab.name_to_index)
                        name = generate_random_string(8)
                        self.shared_attr_vocab.name_to_index[name] = name_index
                        self.shared_attr_vocab.index_to_name[name_index] = name
                    else:
                        name = self.shared_attr_vocab.index_to_name[name_index]

                    attr_dims = max(1, int(math.ceil(F.softplus(self.attr_dims_head(embedding)).item())))
                    values = []
                    value_hidden = embedding.unsqueeze(0)
                    value_input = torch.zeros(1, 1, device=device)
                    for _ in range(attr_dims):
                        value_out, value_hidden = self.attr_val_rnn(value_input, value_hidden)
                        v = self.attr_val_head(value_out).view(-1)
                        values.append(v)
                        value_input = v.unsqueeze(0).unsqueeze(-1)
                    if name in attrs:
                        warn(name + ' is already defined for currently decoding node')
                    attrs[name] = torch.stack(values)
                    t += 1
                node_attributes.append(attrs)
            all_graphs.append({'node_types': torch.as_tensor(node_types), 'node_attributes': node_attributes, 'edge_index': edges})
        return all_graphs

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Permanently remove unused latent dimensions from the node_hidden_state_init layer.
        kept_idx: 1D LongTensor of indices to keep from the original latent vector.
        """
        device = self.node_hidden_state_init.weight.device
        new_node_hidden_state_init = nn.Linear(kept_idx.numel(), self.node_hidden_state_init.out_features, bias=True).to(device)
        new_node_hidden_state_init.weight.data.copy_(old.weight.data[:, kept_idx])

        new_node_hidden_state_init.bias.data.copy_(old.bias.data)
        self.node_hidden_state_init = new_node_hidden_state_init
        self.latent_dim = kept_idx.numel()

class FitnessPredictor(nn.Module):
    """
    Auxiliary predictor: maps concatentated graph and task latents (z_g, z_t) to fitness.
    """
    def __init__(self, latent_dim, hidden_dim=64, fitness_dim=2):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)

    def forward(self, z_graph, z_task):
        return self.fc2(F.relu(self.fc1(torch.cat([z_graph, z_task], dim=-1))))

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for predictor.fc1.
        fc2 stays the same.
        """
        device = self.fc1.weight.device

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.fc1 = prune_lin(self.fc1)


class DAGTaskFitnessRegularizedVAE(nn.Module):
    """
    DAG-VAE with ARD prior that is regularized by fitness prediction from latent and has
    latent-mask for dynamic pruning.
    """
    def __init__(self,
        graph_encoder: GraphEncoder,
        tasks_encoder: TasksEncoder,
        decoder: GraphDecoder,
        fitness_predictor: FitnessPredictor
    ):
        super().__init__()
        self.graph_encoder = graph_encoder
        self.decoder = decoder
        self.fitness_predictor = fitness_predictor
        self.tasks_encoder = tasks_encoder
        # ARD: learnable log-precision per latent dimension
        self.log_alpha_g = nn.Parameter(torch.zeros(self.graph_encoder.latent_dim))
        self.log_alpha_t = nn.Parameter(torch.zeros(self.tasks_encoder.latent_dim))
        # masks for active latent dims (1=active, 0=pruned)
        self.register_buffer('graph_latent_mask', torch.ones(self.graph_encoder.latent_dim))
        self.register_buffer('tasks_latent_mask', torch.ones(self.tasks_encoder.latent_dim))

    def shared_attr_vocab(self):
        return self.graph_encoder.shared_attr_vocab

    def attr_encoder(self):
        return self.graph_encoder.attr_encoder

    def encode(self, node_types, edge_index, node_attributes, batch, task_type, task_features):
        mu_g, lv_g = self.graph_encoder(node_types, edge_index, node_attributes, batch)
        mu_t, lv_t = self.tasks_encoder(task_type, task_features)
        return mu_g, lv_g, mu_t, lv_t

    def reparameterize(self, mu, logvar, latent_mask):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if latent_mask is None else z * latent_mask

    def decode(self, z):
        return self.decoder(z)

    def forward(self, node_types, edge_index, node_attributes, batch, task_type, task_features):
        mu_g, lv_g, mu_t, lv_t = self.encode(node_types, edge_index, node_attributes, batch, task_type, task_features)
        graph_latent = self.reparameterize(mu_g, lv_g, self.graph_latent_mask)
        task_latent = self.reparameterize(mu_t, lv_t, self.tasks_latent_mask)
        decoded_graphs = self.decode(graph_latent)
        fitness_pred = self.fitness_predictor(graph_latent, task_latent)
        return decoded_graphs, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t

    def prune_latent_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        def prune(latent_mask, log_alpha, desc):
            # precision = exp(log_alpha)
            latent_mask = latent_mask.cpu().numpy().astype(bool)
            precisions = torch.exp(log_alpha).detach().cpu().numpy();
            active_indices = np.where(latent_mask)[0]
            active_indices = active_indices[np.argsort(precisions[active_indices])]
            dims_to_prune = active_indices[-num_prune:]
            dims_to_prune = [d for d in dims_to_prune.tolist() if precisions[d] > 1]
            latent_mask[dims_to_prune] = 0.0
            if len(dims_to_prune) > 0:
                print(f"Pruned {desc} latent dims: {dims_to_prune}")
            return torch.tensor(latent_mask)
        self.graph_latent_mask.copy_(prune(self.graph_latent_mask, self.log_alpha_g, 'graph'))
        self.tasks_latent_mask.copy_(prune(self.tasks_latent_mask, self.log_alpha_t, 'tasks'))

    def resize_bottleneck(self):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        graph_kept_idx = self.graph_latent_mask.nonzero(as_tuple=True)[0]
        old_graph_dim = self.graph_latent_mask.numel()
        new_graph_dim = graph_kept_idx.numel()
        if new_graph_dim == old_graph_dim:
            print("No graph latent dims to permanently prune.")
            return
        task_kept_idx = self.tasks_latent_mask.nonzero(as_tuple=True)[0]
        old_task_dim = self.tasks_latent_mask.numel()
        new_task_dim = task_kept_idx.numel()
        if new_task_dim == old_task_dim:
            print("No task latent dims to permanently prune.")
            return

        print(f"Permanently resizing graph bottleneck: {old_graph_dim} → {new_graph_dim}; task bottleneck: {old_task_dim} → {new_task_dim}")

        # Prune graph-related modules
        self.graph_encoder.prune_latent_dims(graph_kept_idx)
        self.decoder.prune_latent_dims(graph_kept_idx)
        self.fitness_predictor.prune_latent_dims(graph_kept_idx)
        self.tasks_encoder.prune_latent_dims(task_kept_idx)
        # Update ARD precision parameters for graph and task
        self.log_alpha_g = nn.Parameter(self.log_alpha_g.data[graph_kept_idx].clone())
        self.log_alpha_t = nn.Parameter(self.log_alpha_t.data[task_kept_idx].clone())
        # Update latent masks
        self.register_buffer('graph_latent_mask', torch.ones(graph_kept_idx.numel()))
        self.register_buffer('tasks_latent_mask', torch.ones(task_kept_idx.numel()))

        print("Bottlenecks resized")


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning,
    supporting variable node-feature dimensions via list-of-graphs decoding.
    """
    def __init__(self, model: DAGTaskFitnessRegularizedVAE, optimizer):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = model.to(self.device)
        self.optimizer = optimizer
        self.dataset = []
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0

    def add_data(self, graphs, fitnesses, task_type:str, task_features):
        for graph, fitness in zip(graphs, fitnesses):
            data = graph.clone()
            data.y = torch.as_tensor(fitness, dtype=torch.float).unsqueeze(0)
            data.task_type = torch.tensor([TASK_TYPE_TO_INDEX[task_type]], dtype=torch.long)
            if isinstance(task_features, torch.Tensor):
                data.task_features = task_features.detach().cpu().tolist()
            else:
                data.task_features = list(task_features)
            self.dataset.append(data)

    def train(self, epochs=1, batch_size=16, kl_weight=1.0, fitness_weight=1.5,
              warmup_epochs=None, loss_threshold=0.9, min_prune_break=5,
              prune_amount=1, min_active_dims=5, stop_epsilon=1E-3, verbose=True):
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        epoch = 1
        num_loss_terms = 5
        prev_loss_terms = torch.zeros(num_loss_terms)
        avg_loss_terms = torch.zeros(num_loss_terms)

        def stop():
            max_loss_term_change = (avg_loss_terms - prev_loss_terms).abs().max().item()
            return (epochs is not None and epoch == epochs + 1) or (epoch > 1 and max_loss_term_change < stop_epsilon)

        while not stop():
            prev_loss_terms = avg_loss_terms.clone()
            total_loss_terms = torch.zeros(num_loss_terms)
            self.model.train()
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()

                # --- 1) forward pass ---
                # assume model.forward now returns:
                #   decoded_graphs: List[{'node_types', 'edge_index', 'node_attributes'}] (len=B)
                #   fitness_pred: Tensor[B, fitness_dim]
                #   mu_g, lv_g, mu_t, lv_t: Tensors[B, latent_dim]
                decoded_graphs, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t = self.model(
                    batch.node_types, batch.edge_index, batch.node_attributes, batch.batch, batch.task_type, batch.task_features
                )
                mean_var = task_latent.var(dim=0).mean().item()
                if mean_var < 1E-3:
                    warn(f"Task latent collapsed (mean variance={mean_var:.2e}): Consider adding task reconstruction to cost")

                # --- 2) reconstruction losses ---
                dense_adj = to_dense_adj(batch.edge_index, batch.batch, max_num_nodes=None)

                loss_adj, loss_feat = torch.tensor(0.0), torch.tensor(0.0)
                for i, dg in enumerate(decoded_graphs):
                    pred_types = dg['node_types']
                    pred_edges = dg['edge_index']
                    pred_attrs = dg['node_attributes']
                    target_types = batch.node_types[i]
                    target_attrs = batch.node_attributes[i]
                    num_nodes = min(len(pred_attrs), len(target_attrs)) # compare only the overlap
                    target_edges = dense_adj[i, :num_nodes, :num_nodes]

                    # build binary adjacency preds/targets on 0..num_nodes-1
                    adj_pred = torch.zeros((num_nodes,num_nodes), device=self.device)
                    for s,d in pred_edges.t().tolist():
                        if s < num_nodes and d < num_nodes:
                            adj_pred[s,d] = 1.0
                    adj_true = dense_adj[i, :num_nodes, :num_nodes]

                    mask = torch.triu(torch.ones_like(adj_pred), diagonal=1).bool()
                    if adj_pred[mask].size(0) > 0 and adj_true[mask].size(0) > 0:
                        loss_adj += F.binary_cross_entropy_with_logits(adj_pred[mask], adj_true[mask])
                    elif adj_pred[mask].size(0) > 0:
                        loss_adj += adj_pred[mask].sum()
                    else:
                        loss_adj += adj_true[mask].sum()

                    # node feature loss
                    def convert_string(value):
                        if isinstance(value, str):
                            value = self.model.attr_encoder().get_value_tensor(value)
                        return value
                    def to_tensor(value):
                        value = convert_string(value)
                        if not isinstance(value, torch.Tensor):
                            value = torch.as_tensor([value], dtype=torch.float)
                        return value
                    def attribute_value_loss(value):
                        value = convert_string(value)
                        if isinstance(value, int):
                            value = float(value)
                        if isinstance(value, torch.Tensor):
                            value = value.abs().sum()
                        return value
                    for node_idx in range(num_nodes):
                        all_attr_names = set(pred_attrs[node_idx].keys()) | \
                                         set(target_attrs[node_idx].keys())
                        for attr_name in all_attr_names:
                            pred_value = pred_attrs[node_idx].get(attr_name)
                            target_value = target_attrs[node_idx].get(attr_name)
                            if (pred_value is not None) and (target_value is not None):
                                loss_feat += F.mse_loss(to_tensor(pred_value), to_tensor(target_value))
                            elif target_value is not None:
                                loss_feat += attribute_value_loss(target_value)
                            else:
                                loss_feat += attribute_value_loss(pred_value)

                loss_adj  /= batch.num_graphs
                loss_feat /= batch.num_graphs

                # --- 3) ARD-KL losses ---
                def calc_kl_div_per_dim(log_alpha, logvar, mu):
                    # ARD-KL divergence per sample and per dim
                    precision = torch.exp(log_alpha)
                    var = torch.exp(logvar)
                    # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                    return 0.5 * (
                        - log_alpha
                        - logvar
                        + precision * (var + mu.pow(2))
                        - 1
                    )
                graph_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_g, lv_g, mu_g).sum(dim=1).mean()
                task_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_t, lv_t, mu_t).sum(dim=1).mean()

                # --- 4) fitness loss ---
                loss_fitness = F.mse_loss(fitness_pred, batch.y.to(self.device))

                # --- 5) total & backward ---
                loss_terms = [
                    loss_adj,
                    loss_feat,
                    kl_weight*graph_kl_loss,
                    kl_weight*task_kl_loss,
                    fitness_weight*loss_fitness,
                ]
                sum(loss_terms).backward()
                self.optimizer.step()
                total_loss_terms += torch.tensor(loss_terms)
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms.cpu().numpy())

            if verbose:
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")
                else:
                    print(f"Epoch {epoch}/{epochs}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")

            # warm-up baseline
            if epoch == warmup_epochs:
                self.initial_loss = avg_loss_terms.sum().item()

            # pruning condition
            if (self.initial_loss is not None
                and avg_loss_terms.sum().item() <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.graph_latent_mask.sum().item())
                if active_count > min_active_dims:
                    self.model.prune_latent_dims(num_prune=prune_amount)
                    self.last_prune_epoch = epoch
                    self.initial_loss = avg_loss_terms.sum()  # reset baseline

            epoch += 1
        return self.loss_history

    def resize_bottleneck(self):
        """Rebuild all modules to permanently prune inactive dims."""
        self.model.resize_bottleneck(self.device)
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")


if __name__ == "__main__":
    num_node_types = 3
    max_nodes = 10
    graph_latent_dim = 32
    fitness_dim = 2
    task_latent_dim = graph_latent_dim - 2
    attr_name_vocab_size = 50
    attr_name_vocab = [generate_random_string(5) for _ in range(attr_name_vocab_size)]
    shared_attr_vocab = SharedAttributeVocab(attr_name_vocab, 5)
    attr_encoder = NodeAttributeDeepSetEncoder(
        shared_attr_vocab,
        encoder_hdim=10,
        aggregator_hdim=20,
        out_dim=50)

    graph_encoder = GraphEncoder(num_node_types, attr_encoder, graph_latent_dim, hidden_dims=[32, 32])
    task_encoder = TasksEncoder(hidden_dim=16, latent_dim=task_latent_dim, type_embedding_dim=8)
    decoder = GraphDecoder(num_node_types, graph_latent_dim, shared_attr_vocab)
    predictor = FitnessPredictor(latent_dim=graph_latent_dim+task_latent_dim, hidden_dim=64, fitness_dim=fitness_dim)
    model = DAGTaskFitnessRegularizedVAE(graph_encoder, task_encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random
    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i+1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
        dyn_attrs = []
        for _ in range(num_nodes):
            attributes = []
            for _ in range(random.randint(0, 5)):
                if random.random() <= .5:
                    attributes.append(random.randint(0, 10))
                if random.random() <= .5:
                    attributes.append(random.random() * 5.0)
                if random.random() <= .5:
                    attributes.append(random.choice(attr_name_vocab))
            dyn_attrs.append({random.choice(attr_name_vocab): v for v in attributes})
        return Data(node_types=types, edge_index=edge_index, node_attributes=dyn_attrs)

    # Create synthetic dataset
    print('Generating random training data')
    def generate_data(num_samples):
        graphs, fitnesses = [], []
        task_type = random.choice(list(TASK_FEATURE_DIMS.keys()))
        task_features = torch.randn((TASK_FEATURE_DIMS[task_type],))
        for _ in range(num_samples):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            graphs.append(graph)
            fitnesses.append([graph.edge_index.size(1) + 0.1 * random.random() for _ in range(fitness_dim)])
        return graphs, fitnesses, task_type, torch.randn((TASK_FEATURE_DIMS[task_type],))

    trainer = OnlineTrainer(model, optimizer)
    trainer.add_data(*generate_data(50))
    print('Training')
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10)
    trainer.resize_bottleneck()

    # Continue training with new data
    trainer.add_data(*generate_data(10))
    print('Continuing training')
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
