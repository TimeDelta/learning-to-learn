# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.utils import add_self_loops, softmax

from typing import Dict, Any
from warnings import warn


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """
    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr='add')
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        nn.init.xavier_uniform_(self.lin.weight)
        nn.init.xavier_uniform_(self.att)

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class TasksEncoder(nn.Module):
    def __init__(self, task_feature_dims:Dict[Any,int], hidden_dim:int, latent_dim:int, type_embedding_dim:int):
        super().__init__()
        # Separate module per task type because each task has different number of features
        # and features don't semantically align
        self.latent_dim = latent_dim
        self.embedder = nn.Embedding(len(task_feature_dims), type_embedding_dim)
        self.encoders = nn.ModuleDict({
            str(type): nn.Sequential(
                nn.Linear(num_features + type_embedding_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU()
            ) for type, num_features in task_feature_dims.items()
        })
        # Shared mu/logvar heads
        self.lin_mu     = nn.Linear(hidden_dim, latent_dim)
        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)
        self.task_type_indices = {k: i for i, k in enumerate(task_feature_dims.keys())}

    def forward(self, task_types, feature_vecs):
        """
        task_types: list[str]   of length B
        feature_vecs: list[Tensor] of length B, each shape (feat_dim,)
        """
        mu_list, logvar_list = [], []
        for task_type, feature_vec in zip(task_types, feature_vecs):
            task_type_embedding = self.embedder(torch.tensor(self.task_type_indices[task_type]))
            feature_vec = torch.as_tensor(feature_vec, dtype=torch.float, device=device)
            h = self.encoders[task_type](torch.cat((task_type_embedding, feature_vec), dim=0))
            mu_list.append(self.lin_mu(h))
            logvar_list.append(self.lin_logvar(h))
        # stack into B×latent_dim tensors
        return torch.stack(mu_list, dim=0), torch.stack(logvar_list, dim=0)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if self.latent_mask is None else z * self.latent_mask

    def prune_latent_dims(self, kept_idx:torch.LongTensor):
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node embeddings and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """
    def __init__(self, num_node_types, node_emb_dim, hidden_dims, latent_dim):
        super().__init__()
        self.latent_dim = latent_dim
        self.node_embedding = nn.Embedding(num_node_types, node_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = node_emb_dim
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    def forward(self, x_type, edge_index, batch):
        # x_type: [num_nodes] long tensor of type indices
        x = self.node_embedding(x_type)  # → [N, node_emb_dim]
        for conv in self.convs:
            x = conv(x, edge_index)
        x = global_mean_pool(x, batch)  # → [batch_size, hidden_dim]
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.lin_mu     = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class GraphDecoder(nn.Module):
    """
    Reconstruct adjacency and node features from latent code.
    """
    def __init__(self, latent_dim, max_nodes, node_feat_dim, hidden_dim=128):
        super().__init__()
        self.max_nodes = max_nodes
        self.node_feat_dim = node_feat_dim
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc_adj = nn.Linear(hidden_dim, max_nodes * max_nodes)
        self.fc_feat = nn.Linear(hidden_dim, max_nodes * node_feat_dim)

    def forward(self, z):
        batch_size = z.size(0)
        h = F.relu(self.fc1(z))
        adj_logits = self.fc_adj(h).view(batch_size, self.max_nodes, self.max_nodes)
        node_features = self.fc_feat(h).view(batch_size, self.max_nodes, self.node_feat_dim)
        return adj_logits, node_features

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for decoder.fc1.
        fc_adj and fc_feat stay the same.
        """
        device = self.fc1.weight.device
        out_feats = self.fc1.out_features
        new_dim   = kept_idx.numel()

        old_fc1 = self.fc1
        new_fc1 = nn.Linear(new_dim, out_feats, bias=True).to(device)
        # copy only the kept columns
        new_fc1.weight.data.copy_(old_fc1.weight.data[:, kept_idx])
        new_fc1.bias.data.copy_(old_fc1.bias.data)
        self.fc1 = new_fc1


class FitnessPredictor(nn.Module):
    """
    Auxiliary predictor: maps concatentated graph and task latents (z_g, z_t) to fitness.
    """
    def __init__(self, latent_dim, hidden_dim=64, fitness_dim=2):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)

    def forward(self, z_graph, z_task):
        return self.fc2(F.relu(self.fc1(torch.cat([z_graph, z_task], dim=-1))))

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for predictor.fc1.
        fc2 stays the same.
        """
        device = self.fc1.weight.device

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer
        self.fc1 = prune_lin(self.fc1)


class DAGTaskFitnessRegularizedVAE(nn.Module):
    """
    DAG-VAE with ARD prior and latent-mask for dynamic pruning.
    """
    def __init__(self,
        graph_encoder: GraphEncoder,
        tasks_encoder: TasksEncoder,
        decoder: GraphDecoder,
        fitness_predictor: FitnessPredictor
    ):
        super().__init__()
        self.graph_encoder = graph_encoder
        self.decoder = decoder
        self.fitness_predictor = fitness_predictor
        self.tasks_encoder = tasks_encoder
        # ARD: learnable log-precision per latent dimension
        self.log_alpha_g = nn.Parameter(torch.zeros(self.graph_encoder.latent_dim))
        self.log_alpha_t = nn.Parameter(torch.zeros(self.tasks_encoder.latent_dim))
        # masks for active latent dims (1=active, 0=pruned)
        self.register_buffer('graph_latent_mask', torch.ones(self.graph_encoder.latent_dim))
        self.register_buffer('tasks_latent_mask', torch.ones(self.tasks_encoder.latent_dim))

    def encode(self, x, edge_index, batch, task_type, task_features):
        mu_g, lv_g = self.graph_encoder(x, edge_index, batch)
        mu_t, lv_t = self.tasks_encoder(task_type, task_features)
        return mu_g, lv_g, mu_t, lv_t

    def reparameterize(self, mu, logvar, latent_mask):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if latent_mask is None else z * latent_mask

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x, edge_index, batch, task_type, task_features):
        mu_g, lv_g, mu_t, lv_t = self.encode(x, edge_index, batch, task_type, task_features)
        graph_latent = self.reparameterize(mu_g, lv_g, self.graph_latent_mask)
        task_latent = self.reparameterize(mu_t, lv_t, self.tasks_latent_mask)
        adj_logits, node_features = self.decode(graph_latent)
        fitness_pred = self.fitness_predictor(graph_latent, task_latent)
        return adj_logits, node_features, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t

    def prune_latent_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        def prune(latent_mask, log_alpha, desc):
            # precision = exp(log_alpha)
            latent_mask = latent_mask.cpu().numpy().astype(bool)
            precisions = torch.exp(log_alpha).detach().cpu().numpy();
            active_indices = np.where(latent_mask)[0]
            active_indices = active_indices[np.argsort(precisions[active_indices])]
            dims_to_prune = active_indices[-num_prune:]
            dims_to_prune = [d for d in dims_to_prune.tolist() if precisions[d] > 1]
            latent_mask[dims_to_prune] = 0.0
            if len(dims_to_prune) > 0:
                print(f"Pruned {desc} latent dims: {dims_to_prune}")
            return torch.tensor(latent_mask)
        self.graph_latent_mask.copy_(prune(self.graph_latent_mask, self.log_alpha_g, 'graph'))
        self.tasks_latent_mask.copy_(prune(self.tasks_latent_mask, self.log_alpha_t, 'tasks'))

    def resize_bottleneck(self):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        graph_kept_idx = self.graph_latent_mask.nonzero(as_tuple=True)[0]
        old_graph_dim = self.graph_latent_mask.numel()
        new_graph_dim = graph_kept_idx.numel()
        if new_graph_dim == old_graph_dim:
            print("No graph latent dims to permanently prune.")
            return
        task_kept_idx = self.tasks_latent_mask.nonzero(as_tuple=True)[0]
        old_task_dim = self.tasks_latent_mask.numel()
        new_task_dim = task_kept_idx.numel()
        if new_task_dim == old_task_dim:
            print("No task latent dims to permanently prune.")
            return

        print(f"Permanently resizing graph bottleneck: {old_graph_dim} → {new_graph_dim}; task bottleneck: {old_task_dim} → {new_task_dim}")

        # Prune graph-related modules
        self.graph_encoder.prune_latent_dims(graph_kept_idx)
        self.decoder.prune_latent_dims(graph_kept_idx)
        self.fitness_predictor.prune_latent_dims(graph_kept_idx)
        self.tasks_encoder.prune_latent_dims(task_kept_idx)
        # Update ARD precision parameters for graph and task
        self.log_alpha_g = nn.Parameter(self.log_alpha_g.data[graph_kept_idx].clone())
        self.log_alpha_t = nn.Parameter(self.log_alpha_t.data[task_kept_idx].clone())
        # Update latent masks
        self.register_buffer('graph_latent_mask', torch.ones(graph_kept_idx.numel()))
        self.register_buffer('tasks_latent_mask', torch.ones(task_kept_idx.numel()))

        # update metadata
        self.latent_dim  = new_dim
        self.latent_mask = torch.ones(new_dim, dtype=torch.bool, device=device)

        print("Bottlenecks resized")


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning.
    """
    def __init__(self, model: DAGTaskFitnessRegularizedVAE, optimizer, device=torch.device('cpu')):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.device = device
        self.dataset = []
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0

    def add_data(self, graphs, fitnesses, task_types, task_feats):
        for graph, fitness, task_type, task_features in zip(graphs, fitnesses, task_types, task_feats):
            data = graph.clone()
            data.y = torch.tensor(fitness, dtype=torch.float) # each fitness already has multiple dimensions
            data.task_type = task_type
            data = graph.clone()
            if isinstance(task_features, torch.Tensor):
                data.task_features = task_features.detach().cpu().tolist()
            else:
                data.task_features = list(task_features)
            self.dataset.append(data)

    def train(self, epochs=1, batch_size=16, kl_weight=1.0, fitness_weight=1.5,
              warmup_epochs=None, loss_threshold=0.9, min_prune_break=5,
              prune_amount=1, min_active_dims=1, stop_epsilon=1E-3, verbose=True):
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        epoch = 1
        num_loss_terms = 5
        prev_loss_terms = np.array([0.0] * num_loss_terms)
        avg_loss_terms = np.array([0.0] * num_loss_terms)
        def stop():
            max_loss_term_change = np.max(abs(avg_loss_terms - prev_loss_terms))
            return (epochs is not None and epoch == epochs + 1) or (epoch > 1 and max_loss_term_change < stop_epsilon)
        while not stop():
            prev_loss_terms = avg_loss_terms
            total_loss_terms = np.array([0.0] * num_loss_terms)
            self.model.train()
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                adj_logits, node_features, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t = self.model(
                    batch.x, batch.edge_index, batch.batch, batch.task_type, batch.task_features
                )
                # --- trivial task-latent check ---
                mean_var = task_latent.var(dim=0).mean().item()
                if mean_var < 1E-3:
                    warn(f"Task latent collapsed (mean variance={mean_var:.2e}): Consider adding task reconstruction to cost")
                num_graphs = batch.num_graphs
                ptr = batch.ptr  # tensor of size (num_graphs+1)

                # Reconstruction losses
                loss_adj = 0.0
                loss_feat = 0.0
                for i in range(num_graphs):
                    start_idx = ptr[i].item()
                    end_idx = ptr[i+1].item()
                    Ni = end_idx - start_idx
                    if Ni == 0:
                        continue
                    # Build adjacency target
                    edge_mask = ((batch.batch[batch.edge_index[0]] == i) &
                                 (batch.batch[batch.edge_index[1]] == i))
                    edges_i = batch.edge_index[:, edge_mask]
                    src_local = edges_i[0] - start_idx
                    dst_local = edges_i[1] - start_idx
                    adj_target = torch.zeros((Ni, Ni), device=self.device)
                    adj_target[src_local, dst_local] = 1.0
                    # Upper triangle mask (i < j)
                    mask = torch.triu(torch.ones((Ni, Ni), device=self.device), diagonal=1) == 1
                    if mask.any():
                        pred_flat = adj_logits[i, :Ni, :Ni][mask]
                        target_flat = adj_target[mask]
                        loss_adj += F.binary_cross_entropy_with_logits(pred_flat, target_flat)
                    # Node feature loss (one-hot -> class labels)
                    target_features = batch.x[start_idx:end_idx] # (Ni, node_feat_dim)
                    # Node feature loss (class labels from integer features)
                    target_classes = batch.x[start_idx:end_idx]  # (Ni,)
                    pred_feats = node_features[i, :Ni, :]        # (Ni, node_feat_dim)
                    loss_feat += F.cross_entropy(pred_feats, target_classes.to(self.device))

                loss_adj = loss_adj / num_graphs
                loss_feat = loss_feat / num_graphs

                def calc_kl_div_per_dim(log_alpha, logvar, mu):
                    # ARD-KL divergence per sample and per dim
                    precision = torch.exp(log_alpha)
                    var = torch.exp(logvar)
                    # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                    return 0.5 * (
                        - log_alpha
                        - logvar
                        + precision * (var + mu.pow(2))
                        - 1
                    )
                graph_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_g, lv_g, mu_g).sum(dim=1).mean()
                task_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_t, lv_t, mu_t).sum(dim=1).mean()
                loss_fitness = F.mse_loss(fitness_pred, batch.y.to(self.device).view_as(fitness_pred))
                loss_terms = [loss_adj, loss_feat, kl_weight*graph_kl_loss, kl_weight*task_kl_loss, fitness_weight * loss_fitness]
                sum(loss_terms).backward()
                self.optimizer.step()
                total_loss_terms += np.array([t.detach().numpy() for t in loss_terms])
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms)
            if verbose:
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")
                else:
                    print(f"Epoch {epoch}/{epochs}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")

            # set baseline after warmup
            if epoch == warmup_epochs:
                self.initial_loss = avg_loss_terms.sum()

            # pruning condition
            if (self.initial_loss is not None
                and avg_loss_terms.sum() <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.graph_latent_mask.sum().item())
                if active_count > min_active_dims:
                    self.model.prune_latent_dims(num_prune=prune_amount)
                    self.last_prune_epoch = epoch
                    self.initial_loss = avg_loss_terms.sum()  # reset baseline

            epoch += 1
        return self.loss_history

    def resize_bottleneck(self):
        self.model.resize_bottleneck(self.device)
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")


if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    num_node_types = 3
    max_nodes = 10
    graph_latent_dim = 32
    fitness_dim = 2
    task_latent_dim = graph_latent_dim - 2
    task_feature_dims = {'a':3,'b':5,'c':7}

    graph_encoder = GraphEncoder(num_node_types, node_emb_dim=10, hidden_dims=[32, 32], latent_dim=graph_latent_dim)
    task_encoder = TasksEncoder(task_feature_dims=task_feature_dims, hidden_dim=16, latent_dim=task_latent_dim, type_embedding_dim=8)
    decoder = GraphDecoder(latent_dim=graph_latent_dim, max_nodes=max_nodes, node_feat_dim=num_node_types, hidden_dim=128)
    predictor = FitnessPredictor(latent_dim=graph_latent_dim+task_latent_dim, hidden_dim=64, fitness_dim=fitness_dim)
    model = DAGTaskFitnessRegularizedVAE(graph_encoder, task_encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random
    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i+1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
        return Data(x=types, edge_index=edge_index)

    # Create synthetic dataset
    print('Generating random training data')
    def generate_data(num_samples):
        graphs, fitnesses, task_types, task_features = [], [], [], []
        for _ in range(num_samples):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            graphs.append(graph)
            fitnesses.append([graph.edge_index.size(1) + 0.1 * random.random() for _ in range(fitness_dim)])
            task_type = random.choice(['a', 'b', 'c'])
            task_types.append(task_type)
            task_features.append(torch.randn((task_feature_dims[task_type],)))
        return graphs, fitnesses, task_types, task_features

    trainer = OnlineTrainer(model, optimizer, device=device)
    trainer.add_data(*generate_data(50))
    print('Training')
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10)
    trainer.resize_bottleneck()

    # Continue training with new data
    trainer.add_data(*generate_data(10))
    print('Continuing training')
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
