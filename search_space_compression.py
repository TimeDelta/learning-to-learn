# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import logging
import math
import os
import time
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple
from warnings import warn

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.utils import (
    add_self_loops,
    degree,
    softmax,
    to_dense_adj,
    to_dense_batch,
)

from tasks import TASK_FEATURE_DIMS, TASK_INDEX_TO_DIM, TASK_TYPE_TO_INDEX
from utility import generate_random_string


def _env_flag(name: str) -> bool:
    value = os.environ.get(name, "")
    return value.strip().lower() in {"1", "true", "yes", "on"}


DEBUG_DECODER = _env_flag("L2L_DEBUG_DECODER")
DEBUG_TRAINER = _env_flag("L2L_DEBUG_TRAINER")

logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("[%(levelname)s] %(name)s: %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


def flatten_task_features(task_features, expected_len: Optional[int] = None) -> np.ndarray:
    """Flatten heterogeneous task feature collections into a single 1D array with optional padding."""
    if isinstance(task_features, torch.Tensor):
        return task_features.detach().cpu().view(-1).numpy()
    if isinstance(task_features, np.ndarray):
        return task_features.reshape(-1)
    flat_parts: List[np.ndarray] = []
    for feat in task_features:
        if isinstance(feat, np.ndarray):
            flat_parts.append(feat.reshape(-1))
        elif isinstance(feat, torch.Tensor):
            flat_parts.append(feat.detach().cpu().view(-1).numpy())
        else:
            flat_parts.append(np.atleast_1d(np.asarray(feat)))
    if not flat_parts:
        flat = np.array([], dtype=float)
    else:
        flat = np.concatenate(flat_parts, axis=0)
    if expected_len is not None:
        if flat.size < expected_len:
            flat = np.pad(flat, (0, expected_len - flat.size))
        elif flat.size > expected_len:
            flat = flat[:expected_len]
    return flat


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """

    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr="add")
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        nn.init.kaiming_uniform_(self.lin.weight, negative_slope, mode="fan_in", nonlinearity="leaky_relu")
        nn.init.kaiming_uniform_(self.att, negative_slope, mode="fan_in", nonlinearity="leaky_relu")

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class AsyncDAGLayer(nn.Module):
    """Simple asynchronous message passing layer following a topological order."""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.lin(x)
        num_nodes = x.size(0)
        device = x.device
        indeg = torch.zeros(num_nodes, dtype=torch.long, device=device)
        indeg.scatter_add_(0, edge_index[1], torch.ones(edge_index.size(1), dtype=torch.long, device=device))
        preds = [[] for _ in range(num_nodes)]
        for s, d in edge_index.t().tolist():
            preds[d].append(s)
        order = [n for n in range(num_nodes) if indeg[n] == 0]
        i = 0
        while i < len(order):
            n = order[i]
            mask = edge_index[0] == n
            for dest in edge_index[1][mask].tolist():
                indeg[dest] -= 1
                if indeg[dest] == 0:
                    order.append(dest)
            i += 1
        h = torch.zeros_like(x)
        for n in order:
            if preds[n]:
                agg = h[preds[n]].mean(dim=0)
                h[n] = F.relu(x[n] + agg)
            else:
                h[n] = F.relu(x[n])
        return h


class TasksEncoder(nn.Module):
    def __init__(self, hidden_dim: int, latent_dim: int, type_embedding_dim: int):
        super().__init__()
        # Separate module per task type because each task has different number of features
        # and features don't semantically align
        self.latent_dim = latent_dim
        self.embedder = nn.Embedding(len(TASK_FEATURE_DIMS), type_embedding_dim)
        self.encoders = nn.ModuleDict(
            {
                str(TASK_TYPE_TO_INDEX[type]): nn.Sequential(
                    nn.Linear(num_features + type_embedding_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                )
                for type, num_features in TASK_FEATURE_DIMS.items()
            }
        )
        # Shared mu/logvar heads
        self.lin_mu = nn.Linear(hidden_dim, latent_dim)
        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, task_types, feature_vecs):
        mu_list, logvar_list = [], []
        for task_type, feature_vec in zip(task_types, feature_vecs):
            task_type_embedding = self.embedder(task_type)
            feature_vec = torch.as_tensor(feature_vec, dtype=torch.float).flatten()
            h = self.encoders[str(task_type.item())](torch.cat((task_type_embedding, feature_vec), dim=0))
            mu_list.append(self.lin_mu(h))
            logvar_list.append(self.lin_logvar(h))
        return torch.stack(mu_list, dim=0), torch.stack(logvar_list, dim=0)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if self.latent_mask is None else z * self.latent_mask

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer

        self.lin_mu = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class SharedAttributeVocab(nn.Module):
    """
    Holds one name-to-index mapping plus a single Embedding that
    can grow on‐the‐fly as new names are added.
    """

    def __init__(self, initial_names: List[str], embedding_dim: int):
        super().__init__()
        self.name_to_index = {name: i for i, name in enumerate(initial_names)}
        self.index_to_name = {i: name for name, i in self.name_to_index.items()}
        for token in ("<UNK>", "<EOS>", "<SOS>"):
            if token not in self.name_to_index:
                idx = len(self.name_to_index)
                self.name_to_index[token] = idx
                self.index_to_name[idx] = token
        self.embedding = nn.Embedding(len(self.name_to_index), embedding_dim)

    @property
    def unk_index(self) -> int:
        return self.name_to_index["<UNK>"]

    @property
    def eos_index(self) -> int:
        return self.name_to_index["<EOS>"]

    @property
    def sos_index(self) -> int:
        return self.name_to_index["<SOS>"]

    def add_names(self, new_names: List[str]):
        names_to_add = [name for name in new_names if name not in self.name_to_index]

        if not names_to_add:
            return

        starting_index = len(self.name_to_index)

        for offset, name in enumerate(names_to_add):
            new_idx = starting_index + offset
            self.name_to_index[name] = new_idx
            self.index_to_name[new_idx] = name

        # expand the embedding matrix with He‐style initialization for the new rows
        old_weight = self.embedding.weight.data
        fan_in = old_weight.size(1)
        device = old_weight.device
        new_rows = torch.randn(len(names_to_add), fan_in, device=device) * math.sqrt(2 / fan_in)
        new_weight = torch.cat([old_weight, new_rows], dim=0)
        self.embedding = nn.Embedding.from_pretrained(new_weight, freeze=False)

    def ensure_index(self, name: str) -> int:
        if name not in self.name_to_index:
            self.add_names([name])
        idx = self.name_to_index[name]
        if idx >= self.embedding.num_embeddings:
            # expand just enough rows to accommodate idx
            missing = idx - self.embedding.num_embeddings + 1
            old_weight = self.embedding.weight.data
            fan_in = old_weight.size(1)
            device = old_weight.device
            new_rows = torch.randn(missing, fan_in, device=device) * math.sqrt(2 / fan_in)
            new_weight = torch.cat([old_weight, new_rows], dim=0)
            self.embedding = nn.Embedding.from_pretrained(new_weight, freeze=False)
        return idx


def attribute_key_to_name(attr_key: Any) -> str:
    if hasattr(attr_key, "name"):
        return attr_key.name  # neat attribute objects
    if isinstance(attr_key, str):
        return attr_key
    return str(attr_key)


def group_node_attributes(node_attrs: Sequence[Any], batch_vec: Optional[torch.Tensor]):
    if not node_attrs:
        return []
    first = node_attrs[0]
    if isinstance(first, dict):
        if batch_vec is None or (isinstance(batch_vec, torch.Tensor) and batch_vec.numel() == 0):
            return [node_attrs]
        per_graph = []
        cursor = 0
        max_graph = int(batch_vec.max().item()) + 1 if batch_vec is not None else 0
        for g in range(max_graph):
            count = int((batch_vec == g).sum().item())
            per_graph.append(node_attrs[cursor : cursor + count])
            cursor += count
        if cursor < len(node_attrs):
            per_graph.append(node_attrs[cursor:])
        return per_graph
    return node_attrs


def build_teacher_attr_targets(
    node_attrs: Sequence[Any], batch_vec: Optional[torch.Tensor], shared_vocab: SharedAttributeVocab
) -> List[List[List[int]]]:
    per_graph_attrs = group_node_attributes(node_attrs, batch_vec)
    sequences: List[List[List[int]]] = []
    for graph_attrs in per_graph_attrs:
        node_sequences: List[List[int]] = []
        for attrs in graph_attrs:
            attr_dict = attrs or {}
            attr_names = sorted(attribute_key_to_name(name) for name in attr_dict.keys())
            tokens = [shared_vocab.ensure_index(name) for name in attr_names]
            tokens.append(shared_vocab.eos_index)
            node_sequences.append(tokens)
        sequences.append(node_sequences)
    return sequences


class NodeAttributeDeepSetEncoder(nn.Module):
    """
    Permutation‐invariant encoder for a dictionary of arbitrary node attributes.
    See "Deep Sets" by Zaheer et al. at https://arxiv.org/abs/1703.06114
    """

    def __init__(self, shared_attr_vocab: SharedAttributeVocab, encoder_hdim: int, aggregator_hdim: int, out_dim: int):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.max_value_dim = shared_attr_vocab.embedding.embedding_dim
        self.attr_encoder = nn.Sequential(  # phi
            nn.Linear(shared_attr_vocab.embedding.embedding_dim + self.max_value_dim, encoder_hdim),
            nn.ReLU(),
            nn.Linear(encoder_hdim, encoder_hdim),
            nn.ReLU(),
        )
        self.aggregator = nn.Sequential(  # rho
            nn.Linear(encoder_hdim, aggregator_hdim),
            nn.ReLU(),
            nn.Linear(aggregator_hdim, out_dim),
        )
        self.out_dim = out_dim

    def get_value_tensor(self, value: Any):
        if isinstance(value, (int, float)):
            value = torch.tensor([value], dtype=torch.float)
        elif isinstance(value, str):
            index = self.shared_attr_vocab.name_to_index.get(value, self.shared_attr_vocab.name_to_index["<UNK>"])
            index = torch.tensor(index, dtype=torch.long)
            value = self.shared_attr_vocab.embedding(index)
        else:
            raise TypeError(f"Unsupported attribute value type: {type(value)}")
        value = value.view(-1)
        if value.numel() < self.max_value_dim:
            pad_amt = self.max_value_dim - value.numel()
            return F.pad(value, (0, pad_amt), "constant", 0.0)
        else:
            return value[: self.max_value_dim]

    def forward(self, attr_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        if not attr_dict or len(attr_dict) == 0:
            return torch.zeros(self.aggregator[-1].out_features)

        phis = []
        for attr, value in sorted(
            attr_dict.items(), key=lambda item: attribute_key_to_name(item[0])
        ):  # consistent ordering
            attr_name = attribute_key_to_name(attr)
            name_idx = self.shared_attr_vocab.ensure_index(attr_name)
            name_index = torch.tensor(name_idx, dtype=torch.long, device=self.shared_attr_vocab.embedding.weight.device)
            value = self.get_value_tensor(value)
            phis.append(self.attr_encoder(torch.cat([self.shared_attr_vocab.embedding(name_index), value], dim=0)))
        return self.aggregator(torch.stack(phis, dim=0).sum(dim=0))


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node type embeddings, deep set encoder for node attributes and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """

    def __init__(
        self, num_node_types: int, attr_encoder: NodeAttributeDeepSetEncoder, latent_dim: int, hidden_dims: List[int]
    ):
        super().__init__()
        self.latent_dim = latent_dim
        self.attr_encoder = attr_encoder
        attr_emb_dim = attr_encoder.out_dim
        self.node_type_embedding = nn.Embedding(num_node_types, attr_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = attr_emb_dim * 2
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    @property
    def max_value_dim(self):
        return self.attr_encoder.max_value_dim

    @property
    def shared_attr_vocab(self):
        return self.attr_encoder.shared_attr_vocab

    def forward(self, node_types, edge_index, node_attributes, batch, num_graphs: Optional[int] = None):
        type_embedding = self.node_type_embedding(node_types)
        num_nodes = len(node_types)
        attr_embedding = torch.zeros((num_nodes, self.attr_encoder.out_dim), device=type_embedding.device)

        per_graph_attrs = group_node_attributes(node_attributes, batch)

        cursor = 0
        for graph_attrs in per_graph_attrs:
            for attrs in graph_attrs:
                if cursor >= num_nodes:
                    warn(
                        f"Attribute encoder produced more vectors than nodes ({cursor + 1} > {num_nodes}); extras discarded"
                    )
                    break
                attr_embedding[cursor] = self.attr_encoder(attrs).to(type_embedding.device)
                cursor += 1
            if cursor >= num_nodes:
                break
        if cursor < num_nodes:
            if cursor == 0 and num_nodes > 0:
                warn(f"No attribute vectors generated; leaving {num_nodes} zero rows")
            elif num_nodes > 0:
                warn(
                    f"Attribute encoder produced {cursor} vectors but expected {num_nodes}; remaining filled with zeros"
                )
        x = torch.cat([type_embedding, attr_embedding], dim=-1)
        for conv in self.convs:
            x = conv(x, edge_index)

        if num_nodes == 0:
            pooled = x.new_zeros((0, x.size(-1)))
        else:
            pooled = global_mean_pool(x, batch)

        if num_graphs is not None:
            expected = int(num_graphs)
            current = pooled.size(0)
            if current < expected:
                pad = pooled.new_zeros((expected - current, pooled.size(1)))
                pooled = torch.cat([pooled, pad], dim=0)
            elif current > expected:
                pooled = pooled[:expected]
        x = pooled
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer

        self.lin_mu = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class AsyncGraphEncoder(GraphEncoder):
    """Graph encoder using AsyncDAGLayer instead of attention."""

    def __init__(
        self, num_node_types: int, attr_encoder: NodeAttributeDeepSetEncoder, latent_dim: int, hidden_dims: List[int]
    ):
        super().__init__(num_node_types, attr_encoder, latent_dim, hidden_dims=[])
        self.convs = nn.ModuleList()
        prev_dim = attr_encoder.out_dim * 2
        for h in hidden_dims:
            self.convs.append(AsyncDAGLayer(prev_dim, h))
            prev_dim = h
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)


class GraphDeconvNet(MessagePassing):

    """
    A Graph Deconvolutional Network (GDN) layer that acts as the
    transpose/inverse of a GCNConv.  It takes an input signal X of
    size [N, in_channels] on a graph with edge_index, and produces
    an output signal of size [N, out_channels], without any fixed
    max_nodes or feature‐size assumptions.
    """

    def __init__(self, in_channels: int, out_channels: int, aggr: str = "add"):
        super().__init__(aggr=aggr)
        # weight for the "transpose" convolution
        self.lin = nn.Linear(in_channels, out_channels, bias=True)
        # optional bias after aggregation
        self.bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, edge_index: torch.LongTensor, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            edge_index: LongTensor of shape [2, E] with COO edges.
            x:          FloatTensor of shape [N, in_channels] node signals.
        Returns:
            FloatTensor of shape [N, out_channels].
        """
        x = self.lin(x)  # (acts like W^T in a transposed convolution)
        # if there are no edges, skip the propagate/unpack step
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return F.relu(x + self.bias)  # just apply bias + activation
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        out = self.propagate(
            edge_index, x=x, norm=deg.pow(0.5)[col] * deg.pow(0.5)[row]
        )  # each node collects from neighbors
        return out + self.bias

    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:
        # x_j: neighbor features [E, out_channels]
        # norm: normalization per edge [E]
        return x_j * norm.view(-1, 1)

    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:
        # optional nonlinearity
        return F.relu(aggr_out)


class GraphDecoder(nn.Module):
    """
    Recurrent generation + GDN refinement.
    1) Recurrently generate node embeddings & edges (GraphRNN style).
    2) Refine embeddings via Graph Deconvolutional Nets (GDNs).
    """

    def __init__(
        self,
        num_node_types: int,
        latent_dim: int,
        shared_attr_vocab: SharedAttributeVocab,
        hidden_dim: int = 128,
        gdn_layers: int = 2,
    ):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.latent_dim = latent_dim
        # — map latent → initial Node‐RNN state
        self.node_hidden_state_init = nn.Linear(latent_dim, hidden_dim)

        # — Node‐RNN (no input, only hidden evolves)
        self.node_rnn = nn.GRUCell(input_size=0, hidden_size=hidden_dim)
        self.stop_head = nn.Linear(hidden_dim, 1)
        self.node_head = nn.Linear(hidden_dim, hidden_dim)
        self.type_head = nn.Linear(hidden_dim, num_node_types)
        self.edge_rnn = nn.GRUCell(input_size=1, hidden_size=hidden_dim)
        self.edge_head = nn.Linear(hidden_dim, 1)

        self.attr_type_rnn = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_type_head = nn.Linear(hidden_dim, 1)
        self.attr_name_rnn = nn.GRU(input_size=self.shared_attr_vocab.embedding.embedding_dim, hidden_size=hidden_dim)
        self.attr_name_head = nn.Linear(hidden_dim, shared_attr_vocab.embedding.embedding_dim)
        self.attr_dims_head = nn.Linear(hidden_dim, 1)  # determine num dimensions per attr
        self.attr_val_rnn = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_val_head = nn.Linear(hidden_dim, 1)  # extract value per attr dimension
        self.attr_sos_index = self.shared_attr_vocab.sos_index
        self.attr_eos_index = self.shared_attr_vocab.eos_index
        self.attr_unk_index = self.shared_attr_vocab.unk_index

        # — GraphDeconvNet stack (learned spectral decoders)
        self.gdns = nn.ModuleList([GraphDeconvNet(hidden_dim, hidden_dim) for _ in range(gdn_layers)])
        self.max_nodes = 1000
        self.max_attributes_per_node = 50
        # Encourage attribute decoder to terminate by progressively biasing the EOS logit.
        self.attr_eos_bias_base = 0.0
        self.attr_eos_bias_slope = 0.1

    def forward(self, latent, teacher_attr_targets: Optional[List[List[List[int]]]] = None):
        """
        (num_graphs, latent_dim)
        returns: list of graphs, each {'node_types': LongTensor[1, N],
                                       'node_attributes': List[{name: attribute_value} x N],
                                       'edge_index': LongTensor[2, E]}
        """
        _forward_prof = torch.autograd.profiler.record_function("GraphDecoder.forward")
        _forward_prof.__enter__()
        try:
            device = latent.device
            all_graphs = []
            attr_teacher_loss = None
            attr_teacher_tokens = 0
            if teacher_attr_targets is not None:
                attr_teacher_loss = latent.new_tensor(0.0)
                # ensure decoder caps accommodate teacher supervision
                max_teacher_nodes = 0
                max_teacher_attrs = 0
                for graph_targets in teacher_attr_targets:
                    max_teacher_nodes = max(max_teacher_nodes, len(graph_targets))
                    for node_seq in graph_targets:
                        if node_seq:
                            # sequences include EOS token; actual attribute count is len - 1
                            max_teacher_attrs = max(max_teacher_attrs, max(len(node_seq) - 1, 0))
                if max_teacher_nodes > self.max_nodes:
                    self.max_nodes = max_teacher_nodes
                if max_teacher_attrs > self.max_attributes_per_node:
                    self.max_attributes_per_node = max_teacher_attrs

            def make_name_input(token_idx: int) -> torch.Tensor:
                token_tensor = torch.tensor([token_idx], dtype=torch.long, device=device)
                return self.shared_attr_vocab.embedding(token_tensor).view(1, 1, -1)

            with torch.autograd.profiler.record_function("GraphDecoder.graph_loop"):
                for l in range(latent.size(0)):
                    decode_stats = None
                    if DEBUG_DECODER:
                        decode_stats = {
                            "graph_index": l,
                            "node_start": time.perf_counter(),
                            "node_loop_exit": None,
                            "node_stop_samples": [],
                            "attr_events": [],
                        }
                    graph_teacher_targets = None
                    target_node_count = None
                    if teacher_attr_targets is not None and l < len(teacher_attr_targets):
                        graph_teacher_targets = teacher_attr_targets[l]
                        if graph_teacher_targets is not None:
                            target_node_count = len(graph_teacher_targets)
                try:
                    latent_norm = float(latent[l].detach().norm().item())
                except Exception:
                    latent_norm = float("nan")
                logger.info("Decoder graph %d start | latent_norm=%.4f", l, latent_norm)
                hidden_node = F.relu(self.node_hidden_state_init(latent[l])).unsqueeze(0)  # (hidden_dim,)
                node_embeddings = []
                edges = []
                node_types = []
                t = 0
                node_loop_timer = time.perf_counter()
                node_loop_iters = 0
                with torch.autograd.profiler.record_function("GraphDecoder.node_loop"):
                    while True:
                        node_loop_iters += 1
                        if t > self.max_nodes:
                            warn("max nodes reached")
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "max_nodes_pre"
                            break
                        if target_node_count is not None and t >= target_node_count:
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "teacher_nodes"
                            break
                        hidden_node = self.node_rnn(torch.zeros(hidden_node.shape[0], 0, device=device), hidden_node)

                        # clamp for precision errors
                        p_stop = 1 - torch.sigmoid(self.stop_head(hidden_node))
                        p_stop = torch.nan_to_num(p_stop, nan=1.0).clamp(0.0, 1.0)
                        if DEBUG_DECODER and decode_stats is not None:
                            decode_stats["node_stop_samples"].append(float(p_stop.item()))
                        if torch.bernoulli(p_stop).item() == 0:
                            if t == 0:
                                # ensure at least one node
                                p_stop = torch.ones_like(p_stop)
                            else:
                                break
                        if t > self.max_nodes:
                            warn("max nodes reached")
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "max_nodes_post"
                            break
                        new_node = self.node_head(hidden_node).squeeze(0)
                        node_embeddings.append(new_node)
                        node_types.append(self.type_head(new_node).argmax(dim=-1).cpu().tolist())

                        # edge generation to previous nodes
                        hidden_edge = hidden_node
                        edge_in = torch.zeros(1, 1, device=device)
                        for i in range(t):
                            hidden_edge = self.edge_rnn(edge_in, hidden_edge)
                            p_edge = torch.sigmoid(self.edge_head(hidden_edge)).view(-1)
                            p_edge = torch.nan_to_num(p_edge, nan=0.0).clamp(0.0, 1.0)
                            if torch.bernoulli(p_edge).item() == 1:
                                edges.append([i, t])
                            edge_in = p_edge.unsqueeze(0)
                        t += 1
                        if target_node_count is not None and t >= target_node_count:
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "teacher_nodes"
                            break
                        if DEBUG_DECODER and t % 50 == 0:
                            elapsed = time.perf_counter() - node_loop_timer
                            logger.info(
                                "Decoder graph %d node generation progress: %d nodes (max=%d) elapsed=%.2fs",
                                decode_stats["graph_index"],
                                t,
                                self.max_nodes,
                                elapsed,
                            )
                        if DEBUG_DECODER and node_loop_iters % 200 == 0:
                            logger.info(
                                "Decoder graph %d node loop iterations=%d (nodes=%d)",
                                decode_stats["graph_index"],
                                node_loop_iters,
                                t,
                            )

            if node_embeddings:
                node_embeddings = torch.stack(node_embeddings, dim=0)
                edges = torch.tensor(edges, dtype=torch.long).t().contiguous()
            else:
                node_embeddings = torch.zeros((0, self.node_head.out_features), device=device)
                edges = torch.zeros((2, 0), dtype=torch.long, device=device)

            if DEBUG_DECODER:
                decode_stats["node_time"] = time.perf_counter() - decode_stats["node_start"]
                exit_reason = decode_stats["node_loop_exit"] or "stop"
                stop_samples = decode_stats["node_stop_samples"]
                avg_p_stop = float(sum(stop_samples) / len(stop_samples)) if stop_samples else float("nan")
                last_p_stop = stop_samples[-1] if stop_samples else float("nan")
                logger.info(
                    "Decoder graph %d node loop exit=%s nodes=%d max=%d iters=%d avg_p_stop=%.4f last_p_stop=%.4f",
                    decode_stats["graph_index"],
                    exit_reason,
                    node_embeddings.size(0),
                    self.max_nodes,
                    node_loop_iters,
                    avg_p_stop,
                    last_p_stop,
                )
                decode_stats["attr_start"] = time.perf_counter()
                decode_stats["node_count"] = int(node_embeddings.size(0))
                decode_stats["edge_count"] = int(edges.size(1)) if edges.numel() > 0 else 0
                decode_stats["attr_nodes"] = 0
                decode_stats["attr_iters"] = 0
                decode_stats["attr_value_cells"] = 0

            # refine via graph deconvolution
            for gdn in self.gdns:
                node_embeddings = gdn(edges, node_embeddings)

            node_attributes = []
            for node_idx, embedding in enumerate(node_embeddings):
                if DEBUG_DECODER:
                    decode_stats["attr_nodes"] += 1
                attrs = {}
                name_hidden = embedding.unsqueeze(0).unsqueeze(0)
                val_hidden = None
                t = 0
                attr_loop_iters = 0
                used_name_indices: Set[int] = set()
                node_teacher_targets: Optional[List[int]] = None
                attr_exit_reason = "eos"
                last_attr_logits: Optional[Tuple[float, float, int]] = None
                if graph_teacher_targets is not None and node_idx < len(graph_teacher_targets):
                    node_teacher_targets = graph_teacher_targets[node_idx] or None

                def project_name_logits(name_out: torch.Tensor) -> torch.Tensor:
                    return self.attr_name_head(name_out).reshape(-1)

                def add_attribute(name_index: int, name: str):
                    nonlocal t
                    raw_dim = self.attr_dims_head(embedding).squeeze()
                    raw_dim = torch.nan_to_num(raw_dim, nan=0.0, posinf=20.0, neginf=-20.0)
                    dim_val = F.softplus(raw_dim)
                    dim_val = torch.nan_to_num(dim_val, nan=1.0, posinf=float(self.max_attributes_per_node), neginf=1.0)
                    attr_dims = int(torch.clamp(dim_val, min=1.0, max=float(self.max_attributes_per_node)).item())
                    attr_dims = max(1, min(attr_dims, self.max_attributes_per_node))
                    values = []
                    value_hidden = embedding.unsqueeze(0).unsqueeze(1)
                    value_input = torch.zeros(1, 1, 1, device=device)
                    for _ in range(attr_dims):
                        value_out, value_hidden = self.attr_val_rnn(value_input, value_hidden)
                        v = self.attr_val_head(value_out).view(-1)
                        values.append(v)
                        value_input = v.unsqueeze(0).unsqueeze(-1)
                    if DEBUG_DECODER:
                        decode_stats["attr_value_cells"] += attr_dims
                        if attr_dims >= self.max_attributes_per_node:
                            logger.info(
                                "Decoder graph %d node %d attribute %s clamped to max dims (%d)",
                                decode_stats["graph_index"],
                                node_idx,
                                name,
                                self.max_attributes_per_node,
                            )
                    if name in attrs:
                        warn(name + " is already defined for currently decoding node")
                        return False
                    attrs[name] = torch.stack(values)
                    t += 1
                    used_name_indices.add(name_index)
                    if DEBUG_DECODER and (t % 25 == 0):
                        logger.info(
                            "Decoder graph %d node %d attr_count=%d",
                            decode_stats["graph_index"],
                            node_idx,
                            t,
                        )
                    return True

                with torch.autograd.profiler.record_function("GraphDecoder.attr_loop"):
                    if node_teacher_targets:
                        prev_token_idx = self.attr_sos_index
                        for target_idx in node_teacher_targets:
                            attr_loop_iters += 1
                            if DEBUG_DECODER and attr_loop_iters % 50 == 0:
                                logger.info(
                                    "Decoder graph %d node %d attr_name iterations=%d",
                                    decode_stats["graph_index"],
                                    node_idx,
                                    attr_loop_iters,
                                )
                            if t > self.max_attributes_per_node:
                                warn("max attributes per node reached")
                                attr_exit_reason = "max_attr_teacher"
                                if DEBUG_DECODER:
                                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                                    teacher_len = len(node_teacher_targets) if node_teacher_targets else 0
                                    logger.info(
                                        "Decoder graph %d node %d attr loop hit teacher cap | attrs=%d of %d | last_eos=%.3f last_max=%.3f last_idx=%d",
                                        decode_stats["graph_index"],
                                        node_idx,
                                        t,
                                        teacher_len,
                                        eos_logit,
                                        top_logit,
                                        top_idx,
                                    )
                                break
                            name_input = make_name_input(prev_token_idx)
                            name_out, name_hidden = self.attr_name_rnn(name_input, name_hidden)
                            if DEBUG_DECODER:
                                decode_stats["attr_iters"] += 1
                            similarity_logits = torch.matmul(
                                self.shared_attr_vocab.embedding.weight,
                                project_name_logits(name_out),
                            )
                            if DEBUG_DECODER:
                                top_logit, top_idx = torch.max(similarity_logits, dim=0)
                                last_attr_logits = (
                                    float(similarity_logits[self.attr_eos_index].item()),
                                    float(top_logit.item()),
                                    int(top_idx.item()),
                                )
                            eos_bias = self.attr_eos_bias_base + self.attr_eos_bias_slope * t
                            similarity_logits[self.attr_eos_index] = similarity_logits[self.attr_eos_index] + eos_bias
                            target_tensor = torch.tensor([target_idx], dtype=torch.long, device=device)
                            if attr_teacher_loss is not None:
                                step_loss = F.cross_entropy(
                                    similarity_logits.unsqueeze(0), target_tensor, reduction="mean"
                                )
                                attr_teacher_loss = attr_teacher_loss + step_loss
                                attr_teacher_tokens += 1
                            prev_token_idx = target_idx
                            if target_idx == self.attr_eos_index:
                                break
                            name = self.shared_attr_vocab.index_to_name.get(target_idx)
                            if name is None:
                                name = generate_random_string(8)
                                target_idx = self.shared_attr_vocab.ensure_index(name)
                            add_attribute(target_idx, name)
                    else:
                        prev_token_idx = self.attr_sos_index
                        while True:
                            attr_loop_iters += 1
                            if DEBUG_DECODER and attr_loop_iters % 50 == 0:
                                logger.info(
                                    "Decoder graph %d node %d attr_name iterations=%d",
                                    decode_stats["graph_index"],
                                    node_idx,
                                    attr_loop_iters,
                                )
                            if t > self.max_attributes_per_node:
                                warn("max attributes per node reached")
                                attr_exit_reason = "max_attr_sampling"
                                if DEBUG_DECODER:
                                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                                    logger.info(
                                        "Decoder graph %d node %d attr loop hit sampling cap | attrs=%d | last_eos=%.3f last_max=%.3f last_idx=%d",
                                        decode_stats["graph_index"],
                                        node_idx,
                                        t,
                                        eos_logit,
                                        top_logit,
                                        top_idx,
                                    )
                                break
                            name_input = make_name_input(prev_token_idx)
                            name_out, name_hidden = self.attr_name_rnn(name_input, name_hidden)
                            if DEBUG_DECODER:
                                decode_stats["attr_iters"] += 1
                            similarity_logits = torch.matmul(
                                self.shared_attr_vocab.embedding.weight,
                                project_name_logits(name_out),
                            )
                            if DEBUG_DECODER:
                                top_logit, top_idx = torch.max(similarity_logits, dim=0)
                                last_attr_logits = (
                                    float(similarity_logits[self.attr_eos_index].item()),
                                    float(top_logit.item()),
                                    int(top_idx.item()),
                                )
                            if used_name_indices:
                                used_idx_tensor = torch.tensor(
                                    list(used_name_indices), device=similarity_logits.device, dtype=torch.long
                                )
                                similarity_logits.index_fill_(0, used_idx_tensor, float("-inf"))
                            eos_bias = self.attr_eos_bias_base + self.attr_eos_bias_slope * t
                            similarity_logits[self.attr_eos_index] = similarity_logits[self.attr_eos_index] + eos_bias
                            name_index = int(similarity_logits.argmax().item())
                            prev_token_idx = name_index
                            if name_index == self.attr_eos_index:
                                break
                            elif name_index == self.attr_unk_index:
                                name = generate_random_string(8)
                                while name in attrs:
                                    name = generate_random_string(8)
                                name_index = self.shared_attr_vocab.ensure_index(name)
                            else:
                                name = self.shared_attr_vocab.index_to_name.get(name_index)
                                if name is None:
                                    name = generate_random_string(8)
                                    name_index = self.shared_attr_vocab.ensure_index(name)
                                else:
                                    name_index = self.shared_attr_vocab.ensure_index(name)
                                if name in attrs:
                                    continue

                            add_attribute(name_index, name)
                node_attributes.append(attrs)
                if DEBUG_DECODER:
                    teacher_len = len(node_teacher_targets) if node_teacher_targets else 0
                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                    decode_stats["attr_events"].append(
                        {
                            "node_idx": node_idx,
                            "exit_reason": attr_exit_reason,
                            "attr_count": t,
                            "teacher_len": teacher_len,
                            "last_eos_logit": eos_logit,
                            "last_top_logit": top_logit,
                            "last_top_idx": top_idx,
                        }
                    )
                    if attr_exit_reason.startswith("max_attr"):
                        logger.info(
                            "Decoder graph %d node %d summary | exit=%s attrs=%d teacher_len=%d last_eos=%.3f last_max=%.3f last_idx=%d",
                            decode_stats["graph_index"],
                            node_idx,
                            attr_exit_reason,
                            t,
                            teacher_len,
                            eos_logit,
                            top_logit,
                            top_idx,
                        )
                all_graphs.append(
                    {"node_types": torch.as_tensor(node_types), "node_attributes": node_attributes, "edge_index": edges}
                )
                if DEBUG_DECODER:
                    attr_time = time.perf_counter() - decode_stats["attr_start"]
                    total_time = decode_stats["node_time"] + attr_time
                    logger.info(
                        "Decoder graph %d: nodes=%d edges=%d attr_nodes=%d attr_names=%d attr_values=%d | node_time=%.3fs attr_time=%.3fs total=%.3fs",
                        decode_stats["graph_index"],
                        decode_stats["node_count"],
                        decode_stats["edge_count"],
                        decode_stats["attr_nodes"],
                        decode_stats["attr_iters"],
                        decode_stats["attr_value_cells"],
                        decode_stats["node_time"],
                        attr_time,
                        total_time,
                    )
                    attr_cap_hits = 0
                    teacher_cap_hits = 0
                    for event in decode_stats["attr_events"]:
                        if event["exit_reason"].startswith("max_attr"):
                            attr_cap_hits += 1
                            if event["exit_reason"].endswith("teacher"):
                                teacher_cap_hits += 1
                    if attr_cap_hits:
                        logger.info(
                            "Decoder graph %d attr cap hits=%d (teacher=%d, sampling=%d)",
                            decode_stats["graph_index"],
                            attr_cap_hits,
                            teacher_cap_hits,
                            attr_cap_hits - teacher_cap_hits,
                        )
            if attr_teacher_loss is not None:
                return all_graphs, {"loss": attr_teacher_loss, "tokens": attr_teacher_tokens}
            return all_graphs
        finally:
            _forward_prof.__exit__(None, None, None)

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Permanently remove unused latent dimensions from the node_hidden_state_init layer.
        kept_idx: 1D LongTensor of indices to keep from the original latent vector.
        """
        device = self.node_hidden_state_init.weight.device
        new_node_hidden_state_init = nn.Linear(
            kept_idx.numel(), self.node_hidden_state_init.out_features, bias=True
        ).to(device)
        new_node_hidden_state_init.weight.data.copy_(old.weight.data[:, kept_idx])

        new_node_hidden_state_init.bias.data.copy_(old.bias.data)
        self.node_hidden_state_init = new_node_hidden_state_init
        self.latent_dim = kept_idx.numel()


class FitnessPredictor(nn.Module):
    """
    Auxiliary predictor: maps concatentated graph and task latents (z_g, z_t) to fitness.
    """

    def __init__(self, latent_dim, hidden_dim=64, fitness_dim=4):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)
        self.output_dim = fitness_dim
        self.latent_dim = latent_dim

    def forward(self, z_graph, z_task):
        return self.fc2(F.relu(self.fc1(torch.cat([z_graph, z_task], dim=-1))))

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for predictor.fc1.
        fc2 stays the same.
        """
        device = self.fc1.weight.device

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer

        self.fc1 = prune_lin(self.fc1)


class TaskConditionedFitnessPredictor(nn.Module):
    """Maintains one fitness head per task type with task-specific output dimensionality."""

    def __init__(self, latent_dim, hidden_dim=64):
        super().__init__()
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.heads = nn.ModuleDict()
        self.head_dims: Dict[str, int] = {}

    def _key(self, task_type_id: int) -> str:
        return str(int(task_type_id))

    def ensure_head(self, task_type_id: int, output_dim: int) -> FitnessPredictor:
        key = self._key(task_type_id)
        head = self.heads[key] if key in self.heads else None
        if head is None:
            head = FitnessPredictor(self.latent_dim, self.hidden_dim, output_dim)
            self.heads[key] = head
            self.head_dims[key] = output_dim
        else:
            if self.head_dims[key] != output_dim:
                raise ValueError(
                    f"Existing fitness head for task {task_type_id} expects dim {self.head_dims[key]},"
                    f" got {output_dim}"
                )
        return head

    def forward(self, z_graph, z_task, task_types: torch.LongTensor, fitness_dims: torch.LongTensor) -> torch.Tensor:
        if fitness_dims is None:
            raise ValueError("fitness_dims tensor required for task-conditioned predictor")
        if task_types.dim() == 0:
            task_types = task_types.unsqueeze(0)
        if fitness_dims.dim() == 0:
            fitness_dims = fitness_dims.unsqueeze(0)
        batch_size = z_graph.size(0)
        max_dim = int(fitness_dims.max().item()) if batch_size > 0 else 0
        if max_dim == 0:
            return z_graph.new_zeros((batch_size, 0))
        preds = z_graph.new_zeros((batch_size, max_dim))
        unique_tasks = torch.unique(task_types).tolist()
        for task_id in unique_tasks:
            idx = (task_types == task_id).nonzero(as_tuple=False).view(-1)
            if idx.numel() == 0:
                continue
            output_dim = int(fitness_dims[idx[0]].item())
            head = self.ensure_head(task_id, output_dim)
            head_pred = head(z_graph[idx], z_task[idx])
            preds[idx, :output_dim] = head_pred
        return preds

    def predict_task(self, task_type_id: int, output_dim: int, z_graph, z_task) -> torch.Tensor:
        head = self.ensure_head(task_type_id, output_dim)
        return head(z_graph, z_task)

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        for head in self.heads.values():
            head.prune_latent_dims(kept_idx)

    def head_specs(self) -> List[Tuple[int, int]]:
        return [(int(key), dim) for key, dim in self.head_dims.items()]


class SelfCompressingFitnessRegularizedDAGVAE(nn.Module):
    """
    DAG-VAE with ARD prior that is regularized by fitness prediction from latent and has
    latent-mask for dynamic pruning.
    """

    def __init__(
        self,
        graph_encoder: GraphEncoder,
        tasks_encoder: TasksEncoder,
        decoder: GraphDecoder,
        fitness_predictor: TaskConditionedFitnessPredictor,
    ):
        super().__init__()
        self.graph_encoder = graph_encoder
        self.decoder = decoder
        self.fitness_predictor = fitness_predictor
        self.tasks_encoder = tasks_encoder
        # ARD: learnable log-precision per latent dimension
        self.log_alpha_g = nn.Parameter(torch.zeros(self.graph_encoder.latent_dim))
        self.log_alpha_t = nn.Parameter(torch.zeros(self.tasks_encoder.latent_dim))
        # masks for active latent dims (1=active, 0=pruned)
        self.register_buffer("graph_latent_mask", torch.ones(self.graph_encoder.latent_dim))
        self.register_buffer("tasks_latent_mask", torch.ones(self.tasks_encoder.latent_dim))

    @property
    def shared_attr_vocab(self):
        return self.graph_encoder.shared_attr_vocab

    @property
    def attr_encoder(self):
        return self.graph_encoder.attr_encoder

    @property
    def max_value_dim(self):
        return self.graph_encoder.max_value_dim

    def encode(self, node_types, edge_index, node_attributes, batch, task_type, task_features, num_graphs=None):
        mu_g, lv_g = self.graph_encoder(node_types, edge_index, node_attributes, batch, num_graphs=num_graphs)
        mu_t, lv_t = self.tasks_encoder(task_type, task_features)
        return mu_g, lv_g, mu_t, lv_t

    def reparameterize(self, mu, logvar, latent_mask):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if latent_mask is None else z * latent_mask

    def decode(self, z, teacher_attr_targets: Optional[List[List[List[int]]]] = None):
        return self.decoder(z, teacher_attr_targets=teacher_attr_targets)

    def forward(
        self,
        node_types,
        edge_index,
        node_attributes,
        batch,
        task_type,
        task_features,
        teacher_attr_targets: Optional[List[List[List[int]]]] = None,
        fitness_dims: Optional[torch.LongTensor] = None,
        num_graphs: Optional[int] = None,
    ):
        mu_g, lv_g, mu_t, lv_t = self.encode(
            node_types,
            edge_index,
            node_attributes,
            batch,
            task_type,
            task_features,
            num_graphs=num_graphs,
        )
        graph_latent = self.reparameterize(mu_g, lv_g, self.graph_latent_mask)
        task_latent = self.reparameterize(mu_t, lv_t, self.tasks_latent_mask)
        if graph_latent.size(0) != task_latent.size(0):
            raise RuntimeError(
                "Graph and task latent batches must match, got " f"{graph_latent.size(0)} vs {task_latent.size(0)}"
            )
        decoded = self.decode(graph_latent, teacher_attr_targets=teacher_attr_targets)
        if isinstance(decoded, tuple):
            decoded_graphs, decoder_aux = decoded
        else:
            decoded_graphs, decoder_aux = decoded, None
        if fitness_dims is None:
            raise RuntimeError("fitness_dims must be provided for task-conditioned fitness prediction")
        fitness_pred = self.fitness_predictor(graph_latent, task_latent, task_type, fitness_dims)
        return decoded_graphs, fitness_pred, graph_latent, task_latent, mu_g, lv_g, mu_t, lv_t, decoder_aux

    def prune_latent_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        def prune(latent_mask, log_alpha, desc):
            # precision = exp(log_alpha)
            latent_mask = latent_mask.cpu().numpy().astype(bool)
            precisions = torch.exp(log_alpha).detach().cpu().numpy()
            active_indices = np.where(latent_mask)[0]
            active_indices = active_indices[np.argsort(precisions[active_indices])]
            dims_to_prune = active_indices[-num_prune:]
            dims_to_prune = [d for d in dims_to_prune.tolist() if precisions[d] > 1]
            latent_mask[dims_to_prune] = 0.0
            if len(dims_to_prune) > 0:
                print(f"Pruned {desc} latent dims: {dims_to_prune}")
            return torch.tensor(latent_mask)

        self.graph_latent_mask.copy_(prune(self.graph_latent_mask, self.log_alpha_g, "graph"))
        self.tasks_latent_mask.copy_(prune(self.tasks_latent_mask, self.log_alpha_t, "tasks"))

    def resize_bottleneck(self):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        graph_kept_idx = self.graph_latent_mask.nonzero(as_tuple=True)[0]
        old_graph_dim = self.graph_latent_mask.numel()
        new_graph_dim = graph_kept_idx.numel()
        if new_graph_dim == old_graph_dim:
            print("No graph latent dims to permanently prune.")
            return
        task_kept_idx = self.tasks_latent_mask.nonzero(as_tuple=True)[0]
        old_task_dim = self.tasks_latent_mask.numel()
        new_task_dim = task_kept_idx.numel()
        if new_task_dim == old_task_dim:
            print("No task latent dims to permanently prune.")
            return

        print(
            f"Permanently resizing graph bottleneck: {old_graph_dim} → {new_graph_dim}; task bottleneck: {old_task_dim} → {new_task_dim}"
        )

        # Prune graph-related modules
        self.graph_encoder.prune_latent_dims(graph_kept_idx)
        self.decoder.prune_latent_dims(graph_kept_idx)
        self.fitness_predictor.prune_latent_dims(graph_kept_idx)
        self.tasks_encoder.prune_latent_dims(task_kept_idx)
        # Update ARD precision parameters for graph and task
        self.log_alpha_g = nn.Parameter(self.log_alpha_g.data[graph_kept_idx].clone())
        self.log_alpha_t = nn.Parameter(self.log_alpha_t.data[task_kept_idx].clone())
        # Update latent masks
        self.register_buffer("graph_latent_mask", torch.ones(graph_kept_idx.numel()))
        self.register_buffer("tasks_latent_mask", torch.ones(task_kept_idx.numel()))

        print("Bottlenecks resized")

    def available_task_heads(self) -> List[Tuple[int, int]]:
        return self.fitness_predictor.head_specs()

    def predict_task_head(
        self, task_type_id: int, output_dim: int, z_graph: torch.Tensor, z_task: torch.Tensor
    ) -> torch.Tensor:
        return self.fitness_predictor.predict_task(task_type_id, output_dim, z_graph, z_task)


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning,
    supporting variable node-feature dimensions via list-of-graphs decoding.
    """

    def __init__(self, model: SelfCompressingFitnessRegularizedDAGVAE, optimizer, fitness_key_ordering=[]):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model.to(self.device)
        self.optimizer = optimizer
        self.dataset = []
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0
        self.max_fitness_dim = 0
        self.task_feature_bank: Dict[int, np.ndarray] = {}
        self.task_metric_dims: Dict[int, int] = {}

    def add_data(self, graphs, fitnesses, task_type: str, task_features):
        for graph, fitness_dict in zip(graphs, fitnesses):
            data = graph.clone()
            # sort fitness values by key
            fitness = [f[1] for f in sorted(fitness_dict.items(), key=lambda item: item[0].name)]
            task_type_id = TASK_TYPE_TO_INDEX[task_type]
            expected_len = TASK_FEATURE_DIMS[task_type]
            flat_features = flatten_task_features(task_features, expected_len)
            self._remember_task_signature(task_type_id, flat_features, len(fitness))
            flat_tensor = torch.as_tensor(flat_features, dtype=torch.float).unsqueeze(0)

            fitness_tensor = torch.as_tensor(fitness, dtype=torch.float)
            self._ensure_dataset_target_width(fitness_tensor.numel())
            if fitness_tensor.numel() < self.max_fitness_dim:
                pad = self.max_fitness_dim - fitness_tensor.numel()
                fitness_tensor = torch.cat([fitness_tensor, torch.zeros(pad)], dim=-1)
            mask = torch.zeros(self.max_fitness_dim, dtype=torch.float)
            mask[: len(fitness)] = 1.0
            data.y = fitness_tensor
            data.fitness_mask = mask
            data.fitness_dim = torch.tensor(len(fitness), dtype=torch.long)
            data.task_type = torch.tensor(task_type_id, dtype=torch.long)
            data.task_features = flat_tensor
            self.dataset.append(data)

    def train(
        self,
        epochs=1,
        batch_size=16,
        kl_weight=1.0,
        fitness_weight=1.5,
        warmup_epochs=None,
        loss_threshold=0.9,
        min_prune_break=5,
        prune_amount=1,
        min_active_dims=5,
        stop_epsilon=1e-3,
        verbose=True,
    ):
        self._normalize_task_feature_shapes()
        loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)
        dataset_size = len(self.dataset)
        total_batches = len(loader)
        train_start = time.perf_counter()
        if DEBUG_TRAINER:
            logger.info(
                "OnlineTrainer.train start | samples=%d batch_size=%d epochs=%s",
                dataset_size,
                batch_size,
                epochs,
            )
        epoch = 1
        num_loss_terms = 5
        prev_loss_terms = torch.zeros(num_loss_terms)
        avg_loss_terms = torch.zeros(num_loss_terms)

        def stop():
            max_loss_term_change = (avg_loss_terms - prev_loss_terms).abs().max().item()
            return (epochs is not None and epoch == epochs + 1) or (epoch > 1 and max_loss_term_change < stop_epsilon)

        while not stop():
            epoch_timer = time.perf_counter() if DEBUG_TRAINER else None
            prev_loss_terms = avg_loss_terms.clone()
            total_loss_terms = torch.zeros(num_loss_terms)
            self.model.train()
            for batch_idx, batch in enumerate(loader, start=1):
                batch_timer = time.perf_counter() if DEBUG_TRAINER else None
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                target_graph_attrs = group_node_attributes(batch.node_attributes, batch.batch)
                teacher_attr_targets = build_teacher_attr_targets(
                    target_graph_attrs, None, self.model.shared_attr_vocab
                )

                # --- 1) forward pass ---
                if DEBUG_TRAINER:
                    logger.info(
                        "Trainer epoch %d batch %d/%d forward start | graphs=%d",
                        epoch,
                        batch_idx,
                        total_batches,
                        batch.num_graphs,
                    )
                (
                    decoded_graphs,
                    fitness_pred,
                    graph_latent,
                    task_latent,
                    mu_g,
                    lv_g,
                    mu_t,
                    lv_t,
                    decoder_aux,
                ) = self.model(
                    batch.node_types,
                    batch.edge_index,
                    batch.node_attributes,
                    batch.batch,
                    batch.task_type,
                    batch.task_features,
                    teacher_attr_targets=teacher_attr_targets,
                    fitness_dims=batch.fitness_dim,
                    num_graphs=batch.num_graphs,
                )
                target_y = batch.y
                mean_var = task_latent.var(dim=0).mean().item()
                if mean_var < 1e-3:
                    warn(
                        f"Task latent collapsed (mean variance={mean_var:.2e}): Consider adding task reconstruction to cost"
                    )

                # --- 2) reconstruction losses ---
                dense_adj = to_dense_adj(batch.edge_index, batch.batch, max_num_nodes=None)
                num_target_graphs = dense_adj.size(0)
                graphs_to_compare = min(len(decoded_graphs), num_target_graphs, len(target_graph_attrs))

                loss_adj = torch.tensor(0.0, device=self.device)
                loss_feat = torch.tensor(0.0, device=self.device)
                for i in range(graphs_to_compare):
                    dg = decoded_graphs[i]
                    pred_edges = dg["edge_index"]
                    pred_attrs = dg["node_attributes"]
                    target_attrs = target_graph_attrs[i] if i < len(target_graph_attrs) else []
                    num_target_nodes = len(target_attrs)
                    num_pred_nodes = len(pred_attrs)
                    num_nodes = min(num_pred_nodes, num_target_nodes)
                    if num_nodes == 0:
                        continue
                    adj_true = dense_adj[i, :num_target_nodes, :num_target_nodes]
                    adj_true = adj_true[:num_nodes, :num_nodes].to(self.device)

                    # build binary adjacency preds/targets on 0..num_nodes-1
                    no_pred_edges = True
                    adj_pred = torch.zeros((num_nodes, num_nodes), device=self.device)
                    for s, d in pred_edges.t().tolist():
                        if s < num_nodes and d < num_nodes:
                            adj_pred[s, d] = 1.0
                            no_pred_edges = False

                    mask = torch.triu(torch.ones_like(adj_pred), diagonal=1).bool()
                    if adj_pred[mask].size(0) > 0 and adj_true[mask].size(0) > 0:
                        loss_adj += F.binary_cross_entropy_with_logits(adj_pred[mask], adj_true[mask])
                    elif adj_pred[mask].size(0) > 0:
                        loss_adj += adj_pred[mask].sum()
                    else:
                        loss_adj += adj_true[mask].sum()

                    # node feature loss
                    def convert_string(value):
                        if isinstance(value, str):
                            value = self.model.attr_encoder.get_value_tensor(value)
                        return value

                    def to_tensor(value):
                        value = convert_string(value)
                        if not isinstance(value, torch.Tensor):
                            value = torch.as_tensor([value], dtype=torch.float)
                        return value

                    def attribute_value_loss(value):
                        value = convert_string(value)
                        if isinstance(value, int):
                            value = float(value)
                        if isinstance(value, torch.Tensor):
                            value = value.abs().sum()
                        return value

                    for node_idx in range(num_nodes):
                        all_attr_names = set(pred_attrs[node_idx].keys()) | set(target_attrs[node_idx].keys())
                        for attr_name in all_attr_names:
                            pred_value = pred_attrs[node_idx].get(attr_name)
                            target_value = target_attrs[node_idx].get(attr_name)
                            if (pred_value is not None) and (target_value is not None):
                                loss_feat += F.mse_loss(to_tensor(pred_value), to_tensor(target_value))
                            elif target_value is not None:
                                loss_feat += attribute_value_loss(target_value)
                            else:
                                loss_feat += attribute_value_loss(pred_value)

                if decoder_aux is not None and decoder_aux.get("tokens", 0) > 0:
                    ce_loss = decoder_aux["loss"] / float(decoder_aux["tokens"])
                    loss_feat = loss_feat + ce_loss.to(loss_feat.device)

                loss_adj /= batch.num_graphs
                loss_feat /= batch.num_graphs

                # --- 3) ARD-KL losses ---
                def calc_kl_div_per_dim(log_alpha, logvar, mu):
                    # ARD-KL divergence per sample and per dim
                    precision = torch.exp(log_alpha)
                    var = torch.exp(logvar)
                    # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                    return 0.5 * (-log_alpha - logvar + precision * (var + mu.pow(2)) - 1)

                graph_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_g, lv_g, mu_g).sum(dim=1).mean()
                task_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_t, lv_t, mu_t).sum(dim=1).mean()

                # --- 4) fitness loss ---
                mask = batch.fitness_mask.to(self.device)
                target = target_y.to(self.device)
                pred = self._align_pred_to_mask(fitness_pred, mask)
                diff = (pred - target) * mask
                denom = mask.sum().clamp_min(1.0)
                loss_fitness = diff.pow(2).sum() / denom

                # --- 5) total & backward ---
                loss_terms = [
                    loss_adj,
                    loss_feat,
                    kl_weight * graph_kl_loss,
                    kl_weight * task_kl_loss,
                    fitness_weight * loss_fitness,
                ]
                sum(loss_terms).backward()
                self.optimizer.step()
                total_loss_terms += torch.tensor(loss_terms)
                if DEBUG_TRAINER and batch_timer is not None:
                    logger.info(
                        "Trainer epoch %d batch %d/%d finished | duration=%.3fs",
                        epoch,
                        batch_idx,
                        total_batches,
                        time.perf_counter() - batch_timer,
                    )
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms.cpu().numpy())

            if verbose:
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}")
                else:
                    print(
                        f"Epoch {epoch}/{epochs}, Loss terms per batch: {avg_loss_terms} = {avg_loss_terms.sum():.4f}"
                    )
            if DEBUG_TRAINER and epoch_timer is not None:
                logger.info(
                    "Trainer epoch %d complete | duration=%.3fs loss=%.4f",
                    epoch,
                    time.perf_counter() - epoch_timer,
                    avg_loss_terms.sum().item(),
                )

            # warm-up baseline
            if epoch == warmup_epochs:
                self.initial_loss = avg_loss_terms.sum().item()

            # pruning condition
            if (
                self.initial_loss is not None
                and avg_loss_terms.sum().item() <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.graph_latent_mask.sum().item())
                if active_count > min_active_dims:
                    self.model.prune_latent_dims(num_prune=prune_amount)
                    self.last_prune_epoch = epoch
                    self.initial_loss = avg_loss_terms.sum()  # reset baseline

            epoch += 1
        if DEBUG_TRAINER:
            logger.info(
                "OnlineTrainer.train finished | epochs_run=%d duration=%.3fs",
                epoch - 1,
                time.perf_counter() - train_start,
            )
        return self.loss_history

    def resize_bottleneck(self):
        """Rebuild all modules to permanently prune inactive dims."""
        self.model.resize_bottleneck()
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")

    def _ensure_dataset_target_width(self, new_width: int):
        if new_width <= self.max_fitness_dim:
            return
        pad = new_width - self.max_fitness_dim
        for data in self.dataset:
            data.y = torch.cat([data.y, torch.zeros(pad)], dim=-1)
            data.fitness_mask = torch.cat([data.fitness_mask, torch.zeros(pad)], dim=-1)
        self.max_fitness_dim = new_width

    def _remember_task_signature(self, task_type_id: int, flat_features: np.ndarray, fitness_dim: int):
        self.task_feature_bank[task_type_id] = flat_features
        self.task_metric_dims[task_type_id] = fitness_dim

    def remember_task_signature(self, task_type: str, task_features, fitness_dim: int):
        task_type_id = TASK_TYPE_TO_INDEX[task_type]
        expected_len = TASK_FEATURE_DIMS[task_type]
        flat = flatten_task_features(task_features, expected_len)
        self._remember_task_signature(task_type_id, flat, fitness_dim)

    def get_task_features(self, task_type_id: int) -> Optional[np.ndarray]:
        return self.task_feature_bank.get(task_type_id)

    def get_task_metric_dim(self, task_type_id: int) -> Optional[int]:
        return self.task_metric_dims.get(task_type_id)

    def _normalize_task_feature_shapes(self):
        for data in self.dataset:
            tf = getattr(data, "task_features", None)
            if isinstance(tf, torch.Tensor) and tf.dim() == 1:
                data.task_features = tf.unsqueeze(0)

    @staticmethod
    def _align_pred_to_mask(pred: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        if pred.dim() == 1:
            pred = pred.unsqueeze(1)
        if mask.dim() == 1:
            mask = mask.unsqueeze(0)
        if pred.size(1) < mask.size(1):
            pad = mask.size(1) - pred.size(1)
            pred = torch.cat([pred, pred.new_zeros(pred.size(0), pad)], dim=1)
        elif pred.size(1) > mask.size(1):
            pred = pred[:, : mask.size(1)]
        return pred


if __name__ == "__main__":
    from attributes import *

    num_node_types = 3
    max_nodes = 10
    graph_latent_dim = 32
    fitness_dim = 2
    task_latent_dim = graph_latent_dim - 2
    attr_name_vocab_size = 50
    attr_name_vocab = [generate_random_string(5) for _ in range(attr_name_vocab_size)]
    shared_attr_vocab = SharedAttributeVocab(attr_name_vocab, 5)
    attr_encoder = NodeAttributeDeepSetEncoder(shared_attr_vocab, encoder_hdim=10, aggregator_hdim=20, out_dim=50)

    graph_encoder = GraphEncoder(num_node_types, attr_encoder, graph_latent_dim, hidden_dims=[32, 32])
    task_encoder = TasksEncoder(hidden_dim=16, latent_dim=task_latent_dim, type_embedding_dim=8)
    decoder = GraphDecoder(num_node_types, graph_latent_dim, shared_attr_vocab)
    predictor = TaskConditionedFitnessPredictor(latent_dim=graph_latent_dim + task_latent_dim, hidden_dim=64)
    model = SelfCompressingFitnessRegularizedDAGVAE(graph_encoder, task_encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random

    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = (
            torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)
        )
        dyn_attrs = []
        for _ in range(num_nodes):
            attributes = {}
            for _ in range(random.randint(0, 5)):
                if random.random() <= 0.5:
                    attributes[IntAttribute(random.choice(attr_name_vocab))] = random.randint(0, 10)
                if random.random() <= 0.5:
                    attributes[FloatAttribute(random.choice(attr_name_vocab))] = random.random() * 5.0
                if random.random() <= 0.5:
                    attributes[StringAttribute(random.choice(attr_name_vocab))] = random.choice(attr_name_vocab)
            dyn_attrs.append(attributes)
        return Data(node_types=types, edge_index=edge_index, node_attributes=dyn_attrs)

    # Create synthetic dataset
    class Metric:
        def __init__(self, name, objective):
            self.name = name
            self.objective = objective

    print("Generating random training data")

    def generate_data(num_samples):
        graphs, fitnesses = [], []
        task_type = random.choice(list(TASK_FEATURE_DIMS.keys()))
        task_features = torch.randn((TASK_FEATURE_DIMS[task_type],))
        for _ in range(num_samples):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            graphs.append(graph)
            fitnesses.append(
                {Metric(str(i), "min"): graph.edge_index.size(1) + 0.1 * random.random() for i in range(fitness_dim)}
            )
        return graphs, fitnesses, task_type, torch.randn((TASK_FEATURE_DIMS[task_type],))

    trainer = OnlineTrainer(model, optimizer)
    trainer.add_data(*generate_data(50))
    print("Training")
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10, stop_epsilon=1)
    trainer.resize_bottleneck()

    # Continue training with new data
    trainer.add_data(*generate_data(10))
    print("Continuing training")
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
