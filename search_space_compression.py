# Code originally generated by ChatGPT based on the following papers:
#   D-VAE: A Variational Autoencoder for Directed Acyclic Graphs (https://arxiv.org/abs/1904.11088)
#   DIRECTED ACYCLIC GRAPH NEURAL NETWORKS (https://jiechenjiechen.github.io/pub/dagnn.pdf)
import hashlib
import json
import logging
import math
import os
import random
import time
from collections import deque
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple
from warnings import warn

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.utils import (
    add_self_loops,
    degree,
    softmax,
    to_dense_adj,
    to_dense_batch,
)

from metrics import canonical_log_distance, sort_metrics_by_name
from tasks import TASK_FEATURE_DIMS, TASK_INDEX_TO_DIM, TASK_TYPE_TO_INDEX
from utility import generate_random_string


def _env_flag(name: str) -> bool:
    value = os.environ.get(name, "")
    return value.strip().lower() in {"1", "true", "yes", "on"}


DEBUG_DECODER = _env_flag("L2L_DEBUG_DECODER")
DEBUG_TRAINER = _env_flag("L2L_DEBUG_TRAINER")

logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("[%(levelname)s] %(name)s: %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


def flatten_task_features(task_features, expected_len: Optional[int] = None) -> np.ndarray:
    """Flatten heterogeneous task feature collections into a single 1D array with optional padding."""
    if isinstance(task_features, torch.Tensor):
        return task_features.detach().cpu().view(-1).numpy()
    if isinstance(task_features, np.ndarray):
        return task_features.reshape(-1)
    flat_parts: List[np.ndarray] = []
    for feat in task_features:
        if isinstance(feat, np.ndarray):
            flat_parts.append(feat.reshape(-1))
        elif isinstance(feat, torch.Tensor):
            flat_parts.append(feat.detach().cpu().view(-1).numpy())
        else:
            flat_parts.append(np.atleast_1d(np.asarray(feat)))
    if not flat_parts:
        flat = np.array([], dtype=float)
    else:
        flat = np.concatenate(flat_parts, axis=0)
    if expected_len is not None:
        if flat.size < expected_len:
            flat = np.pad(flat, (0, expected_len - flat.size))
        elif flat.size > expected_len:
            flat = flat[:expected_len]
    return flat


class DAGAttention(MessagePassing):
    """
    DAG-specific attention aggregator (Chen et al. 2021).
    Each node attends over its predecessors + itself (node context).
    """

    def __init__(self, in_channels, out_channels, negative_slope=0.2):
        super().__init__(aggr="add")
        self.lin = nn.Linear(in_channels, out_channels, bias=False)
        self.att = nn.Parameter(torch.Tensor(1, 2 * out_channels))
        self.leaky_relu = nn.LeakyReLU(negative_slope)
        nn.init.kaiming_uniform_(self.lin.weight, negative_slope, mode="fan_in", nonlinearity="leaky_relu")
        nn.init.kaiming_uniform_(self.att, negative_slope, mode="fan_in", nonlinearity="leaky_relu")

    def forward(self, x, edge_index):
        x = self.lin(x)
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j, index, ptr, size_i):
        cat = torch.cat([x_i, x_j], dim=-1)
        alpha = (cat * self.att).sum(dim=-1)
        alpha = self.leaky_relu(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        return x_j * alpha.view(-1, 1)

    def update(self, aggr_out):
        return F.relu(aggr_out)


class AsyncDAGLayer(nn.Module):
    """Simple asynchronous message passing layer following a topological order."""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.lin(x)
        num_nodes = x.size(0)
        device = x.device
        indeg = torch.zeros(num_nodes, dtype=torch.long, device=device)
        indeg.scatter_add_(0, edge_index[1], torch.ones(edge_index.size(1), dtype=torch.long, device=device))
        preds = [[] for _ in range(num_nodes)]
        for s, d in edge_index.t().tolist():
            preds[d].append(s)
        order = [n for n in range(num_nodes) if indeg[n] == 0]
        i = 0
        while i < len(order):
            n = order[i]
            mask = edge_index[0] == n
            for dest in edge_index[1][mask].tolist():
                indeg[dest] -= 1
                if indeg[dest] == 0:
                    order.append(dest)
            i += 1
        h = torch.zeros_like(x)
        for n in order:
            if preds[n]:
                agg = h[preds[n]].mean(dim=0)
                h[n] = F.relu(x[n] + agg)
            else:
                h[n] = F.relu(x[n])
        return h


class TasksEncoder(nn.Module):
    def __init__(self, hidden_dim: int, latent_dim: int, type_embedding_dim: int):
        super().__init__()
        # Separate module per task type because each task has different number of features
        # and features don't semantically align
        self.latent_dim = latent_dim
        self.embedder = nn.Embedding(len(TASK_FEATURE_DIMS), type_embedding_dim)
        self.encoders = nn.ModuleDict(
            {
                str(TASK_TYPE_TO_INDEX[type]): nn.Sequential(
                    nn.Linear(num_features + type_embedding_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                )
                for type, num_features in TASK_FEATURE_DIMS.items()
            }
        )
        # Shared mu/logvar heads
        self.lin_mu = nn.Linear(hidden_dim, latent_dim)
        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, task_types, feature_vecs):
        mu_list, logvar_list = [], []
        for task_type, feature_vec in zip(task_types, feature_vecs):
            task_type_embedding = self.embedder(task_type)
            feature_vec = torch.as_tensor(feature_vec, dtype=torch.float).flatten()
            h = self.encoders[str(task_type.item())](torch.cat((task_type_embedding, feature_vec), dim=0))
            mu_list.append(self.lin_mu(h))
            logvar_list.append(self.lin_logvar(h))
        return torch.stack(mu_list, dim=0), torch.stack(logvar_list, dim=0)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if self.latent_mask is None else z * self.latent_mask

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer

        self.lin_mu = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class SharedAttributeVocab(nn.Module):
    """
    Holds one name-to-index mapping plus a single Embedding that
    can grow on‐the‐fly as new names are added.
    """

    def __init__(self, initial_names: List[str], embedding_dim: int):
        super().__init__()
        self.name_to_index = {name: i for i, name in enumerate(initial_names)}
        self.index_to_name = {i: name for name, i in self.name_to_index.items()}
        for token in ("<UNK>", "<EOS>", "<SOS>"):
            if token not in self.name_to_index:
                idx = len(self.name_to_index)
                self.name_to_index[token] = idx
                self.index_to_name[idx] = token
        self.embedding = nn.Embedding(len(self.name_to_index), embedding_dim)

    @property
    def unk_index(self) -> int:
        return self.name_to_index["<UNK>"]

    @property
    def eos_index(self) -> int:
        return self.name_to_index["<EOS>"]

    @property
    def sos_index(self) -> int:
        return self.name_to_index["<SOS>"]

    def add_names(self, new_names: List[str]):
        names_to_add = [name for name in new_names if name not in self.name_to_index]

        if not names_to_add:
            return

        starting_index = len(self.name_to_index)

        for offset, name in enumerate(names_to_add):
            new_idx = starting_index + offset
            self.name_to_index[name] = new_idx
            self.index_to_name[new_idx] = name

        # expand the embedding matrix with He‐style initialization for the new rows
        old_weight = self.embedding.weight.data
        fan_in = old_weight.size(1)
        device = old_weight.device
        new_rows = torch.randn(len(names_to_add), fan_in, device=device) * math.sqrt(2 / fan_in)
        new_weight = torch.cat([old_weight, new_rows], dim=0)
        self.embedding = nn.Embedding.from_pretrained(new_weight, freeze=False)

    def ensure_index(self, name: str) -> int:
        if name not in self.name_to_index:
            self.add_names([name])
        idx = self.name_to_index[name]
        if idx >= self.embedding.num_embeddings:
            # expand just enough rows to accommodate idx
            missing = idx - self.embedding.num_embeddings + 1
            old_weight = self.embedding.weight.data
            fan_in = old_weight.size(1)
            device = old_weight.device
            new_rows = torch.randn(missing, fan_in, device=device) * math.sqrt(2 / fan_in)
            new_weight = torch.cat([old_weight, new_rows], dim=0)
            self.embedding = nn.Embedding.from_pretrained(new_weight, freeze=False)
        return idx


def attribute_key_to_name(attr_key: Any) -> str:
    if hasattr(attr_key, "name"):
        return attr_key.name  # neat attribute objects
    if isinstance(attr_key, str):
        return attr_key
    return str(attr_key)


def _serialize_attr_value_for_signature(value: Any) -> Any:
    if torch.is_tensor(value):
        if value.numel() == 0:
            return []
        return value.detach().cpu().view(-1).tolist()
    if isinstance(value, np.ndarray):
        if value.size == 0:
            return []
        return value.reshape(-1).tolist()
    if isinstance(value, (list, tuple)):
        return [_serialize_attr_value_for_signature(v) for v in value]
    if isinstance(value, dict):
        return {
            str(k): _serialize_attr_value_for_signature(v)
            for k, v in sorted(value.items(), key=lambda item: str(item[0]))
        }
    if isinstance(value, (bool, int, float, str)) or value is None:
        return value
    return repr(value)


def _serialize_node_types_for_signature(node_types) -> List[int]:
    if node_types is None:
        return []
    if torch.is_tensor(node_types):
        return node_types.detach().cpu().view(-1).tolist()
    if isinstance(node_types, np.ndarray):
        return node_types.reshape(-1).tolist()
    return list(node_types)


def _serialize_edge_index_for_signature(edge_index) -> List[List[int]]:
    if edge_index is None:
        return []
    if torch.is_tensor(edge_index):
        tensor = edge_index.detach().cpu().long()
        if tensor.dim() == 1:
            tensor = tensor.view(2, -1)
        if tensor.numel() == 0:
            return []
        return tensor.t().tolist()
    if isinstance(edge_index, np.ndarray):
        if edge_index.size == 0:
            return []
        arr = edge_index
        if arr.ndim == 1:
            arr = arr.reshape(2, -1)
        return arr.T.tolist()
    edges: List[List[int]] = []
    for edge in edge_index:
        if isinstance(edge, (list, tuple)):
            if len(edge) >= 2:
                edges.append([int(edge[0]), int(edge[1])])
        elif isinstance(edge, dict):
            src = int(edge.get("src", 0))
            dst = int(edge.get("dst", 0))
            edges.append([src, dst])
    return edges


def _serialize_node_attributes_for_signature(node_attributes) -> List[Any]:
    if not node_attributes:
        return []
    serialized_nodes: List[Any] = []
    for attrs in node_attributes:
        if not isinstance(attrs, dict):
            serialized_nodes.append(_serialize_attr_value_for_signature(attrs))
            continue
        serialized_attrs = []
        for key, value in sorted(attrs.items(), key=lambda item: attribute_key_to_name(item[0])):
            serialized_attrs.append([attribute_key_to_name(key), _serialize_attr_value_for_signature(value)])
        serialized_nodes.append(serialized_attrs)
    return serialized_nodes


def graph_signature_from_components(node_types, edge_index, node_attributes) -> str:
    payload = {
        "node_types": _serialize_node_types_for_signature(node_types),
        "edge_index": _serialize_edge_index_for_signature(edge_index),
        "node_attributes": _serialize_node_attributes_for_signature(node_attributes),
    }
    serialized = json.dumps(payload, sort_keys=True, separators=(",", ":"), ensure_ascii=True)
    return hashlib.sha256(serialized.encode("utf-8")).hexdigest()


def graph_signature_from_data(graph: Data) -> str:
    return graph_signature_from_components(
        getattr(graph, "node_types", None),
        getattr(graph, "edge_index", None),
        getattr(graph, "node_attributes", None),
    )


def graph_signature_from_dict(graph_dict: Dict[str, Any]) -> str:
    if not graph_dict:
        return hashlib.sha256(b"{}").hexdigest()
    return graph_signature_from_components(
        graph_dict.get("node_types"),
        graph_dict.get("edge_index"),
        graph_dict.get("node_attributes"),
    )


def group_node_attributes(node_attrs: Sequence[Any], batch_vec: Optional[torch.Tensor]):
    if not node_attrs:
        return []
    first = node_attrs[0]
    if isinstance(first, dict):
        if batch_vec is None or (isinstance(batch_vec, torch.Tensor) and batch_vec.numel() == 0):
            return [node_attrs]
        per_graph = []
        cursor = 0
        max_graph = int(batch_vec.max().item()) + 1 if batch_vec is not None else 0
        for g in range(max_graph):
            count = int((batch_vec == g).sum().item())
            per_graph.append(node_attrs[cursor : cursor + count])
            cursor += count
        if cursor < len(node_attrs):
            per_graph.append(node_attrs[cursor:])
        return per_graph
    return node_attrs


def build_teacher_attr_targets(
    node_attrs: Sequence[Any], batch_vec: Optional[torch.Tensor], shared_vocab: SharedAttributeVocab
) -> List[List[List[int]]]:
    per_graph_attrs = group_node_attributes(node_attrs, batch_vec)
    sequences: List[List[List[int]]] = []
    for graph_attrs in per_graph_attrs:
        node_sequences: List[List[int]] = []
        for attrs in graph_attrs:
            attr_dict = attrs or {}
            attr_names = sorted(attribute_key_to_name(name) for name in attr_dict.keys())
            tokens = [shared_vocab.ensure_index(name) for name in attr_names]
            tokens.append(shared_vocab.eos_index)
            node_sequences.append(tokens)
        sequences.append(node_sequences)
    return sequences


class NodeAttributeDeepSetEncoder(nn.Module):
    """
    Permutation‐invariant encoder for a dictionary of arbitrary node attributes.
    See "Deep Sets" by Zaheer et al. at https://arxiv.org/abs/1703.06114
    """

    def __init__(self, shared_attr_vocab: SharedAttributeVocab, encoder_hdim: int, aggregator_hdim: int, out_dim: int):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.max_value_dim = shared_attr_vocab.embedding.embedding_dim
        self.attr_encoder = nn.Sequential(  # phi
            nn.Linear(shared_attr_vocab.embedding.embedding_dim + self.max_value_dim, encoder_hdim),
            nn.ReLU(),
            nn.Linear(encoder_hdim, encoder_hdim),
            nn.ReLU(),
        )
        self.aggregator = nn.Sequential(  # rho
            nn.Linear(encoder_hdim, aggregator_hdim),
            nn.ReLU(),
            nn.Linear(aggregator_hdim, out_dim),
        )
        self.out_dim = out_dim

    def get_value_tensor(self, value: Any):
        if isinstance(value, (int, float)):
            value = torch.tensor([value], dtype=torch.float)
        elif isinstance(value, str):
            index = self.shared_attr_vocab.name_to_index.get(value, self.shared_attr_vocab.name_to_index["<UNK>"])
            index = torch.tensor(index, dtype=torch.long, device=self.shared_attr_vocab.embedding.weight.device)
            value = self.shared_attr_vocab.embedding(index)
        elif isinstance(value, torch.Tensor):
            # Flatten user provided tensors while preserving autograd info when possible.
            if not value.is_floating_point():
                value = value.to(torch.float)
        else:
            raise TypeError(f"Unsupported attribute value type: {type(value)}")
        value = value.reshape(-1)
        value = value.to(self.shared_attr_vocab.embedding.weight.device)
        if value.numel() < self.max_value_dim:
            pad_amt = self.max_value_dim - value.numel()
            return F.pad(value, (0, pad_amt), "constant", 0.0)
        else:
            return value[: self.max_value_dim]

    def forward(self, attr_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        if not attr_dict or len(attr_dict) == 0:
            return torch.zeros(self.aggregator[-1].out_features)

        phis = []
        for attr, value in sorted(
            attr_dict.items(), key=lambda item: attribute_key_to_name(item[0])
        ):  # consistent ordering
            attr_name = attribute_key_to_name(attr)
            name_idx = self.shared_attr_vocab.ensure_index(attr_name)
            name_index = torch.tensor(name_idx, dtype=torch.long, device=self.shared_attr_vocab.embedding.weight.device)
            value = self.get_value_tensor(value)
            phis.append(self.attr_encoder(torch.cat([self.shared_attr_vocab.embedding(name_index), value], dim=0)))
        return self.aggregator(torch.stack(phis, dim=0).sum(dim=0))


class GraphEncoder(nn.Module):
    """
    Graph encoder with learnable node type embeddings, deep set encoder for node attributes and DAG attention layers.
    Outputs per-graph mu and logvar for VAE.
    """

    def __init__(
        self, num_node_types: int, attr_encoder: NodeAttributeDeepSetEncoder, latent_dim: int, hidden_dims: List[int]
    ):
        super().__init__()
        self.latent_dim = latent_dim
        self.attr_encoder = attr_encoder
        attr_emb_dim = attr_encoder.out_dim
        self.node_type_embedding = nn.Embedding(num_node_types, attr_emb_dim)
        self.convs = nn.ModuleList()
        prev_dim = attr_emb_dim * 2
        for h in hidden_dims:
            self.convs.append(DAGAttention(prev_dim, h))
            prev_dim = h
        # map to latent parameters
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)

    @property
    def max_value_dim(self):
        return self.attr_encoder.max_value_dim

    @property
    def shared_attr_vocab(self):
        return self.attr_encoder.shared_attr_vocab

    def forward(self, node_types, edge_index, node_attributes, batch, num_graphs: Optional[int] = None):
        type_embedding = self.node_type_embedding(node_types)
        num_nodes = len(node_types)
        attr_embedding = torch.zeros((num_nodes, self.attr_encoder.out_dim), device=type_embedding.device)

        per_graph_attrs = group_node_attributes(node_attributes, batch)

        cursor = 0
        for graph_attrs in per_graph_attrs:
            for attrs in graph_attrs:
                if cursor >= num_nodes:
                    warn(
                        f"Attribute encoder produced more vectors than nodes ({cursor + 1} > {num_nodes}); extras discarded"
                    )
                    break
                attr_embedding[cursor] = self.attr_encoder(attrs).to(type_embedding.device)
                cursor += 1
            if cursor >= num_nodes:
                break
        if cursor < num_nodes:
            if cursor == 0 and num_nodes > 0:
                warn(f"No attribute vectors generated; leaving {num_nodes} zero rows")
            elif num_nodes > 0:
                warn(
                    f"Attribute encoder produced {cursor} vectors but expected {num_nodes}; remaining filled with zeros"
                )
        x = torch.cat([type_embedding, attr_embedding], dim=-1)
        for conv in self.convs:
            x = conv(x, edge_index)

        if num_nodes == 0:
            pooled = x.new_zeros((0, x.size(-1)))
        else:
            pooled = global_mean_pool(x, batch)

        if num_graphs is not None:
            expected = int(num_graphs)
            current = pooled.size(0)
            if current < expected:
                pad = pooled.new_zeros((expected - current, pooled.size(1)))
                pooled = torch.cat([pooled, pad], dim=0)
            elif current > expected:
                pooled = pooled[:expected]
        x = pooled
        mu = self.lin_mu(x)
        logvar = self.lin_logvar(x)
        return mu, logvar

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Keep only the latent dims in `kept_idx` for lin_mu and lin_logvar.
        """

        def prune_lin(old):
            layer = nn.Linear(old.in_features, kept_idx.numel()).to(old.weight.device)
            layer.weight.data = old.weight.data[kept_idx]
            layer.bias.data = old.bias.data[kept_idx]
            return layer

        self.lin_mu = prune_lin(self.lin_mu)
        self.lin_logvar = prune_lin(self.lin_logvar)
        self.latent_dim = len(kept_idx)


class AsyncGraphEncoder(GraphEncoder):
    """Graph encoder using AsyncDAGLayer instead of attention."""

    def __init__(
        self, num_node_types: int, attr_encoder: NodeAttributeDeepSetEncoder, latent_dim: int, hidden_dims: List[int]
    ):
        super().__init__(num_node_types, attr_encoder, latent_dim, hidden_dims=[])
        self.convs = nn.ModuleList()
        prev_dim = attr_encoder.out_dim * 2
        for h in hidden_dims:
            self.convs.append(AsyncDAGLayer(prev_dim, h))
            prev_dim = h
        self.lin_mu = nn.Linear(prev_dim, latent_dim)
        self.lin_logvar = nn.Linear(prev_dim, latent_dim)


class GraphDeconvNet(MessagePassing):

    """
    A Graph Deconvolutional Network (GDN) layer that acts as the
    transpose/inverse of a GCNConv.  It takes an input signal X of
    size [N, in_channels] on a graph with edge_index, and produces
    an output signal of size [N, out_channels], without any fixed
    max_nodes or feature‐size assumptions.
    """

    def __init__(self, in_channels: int, out_channels: int, aggr: str = "add"):
        super().__init__(aggr=aggr)
        # weight for the "transpose" convolution
        self.lin = nn.Linear(in_channels, out_channels, bias=True)
        # optional bias after aggregation
        self.bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, edge_index: torch.LongTensor, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            edge_index: LongTensor of shape [2, E] with COO edges.
            x:          FloatTensor of shape [N, in_channels] node signals.
        Returns:
            FloatTensor of shape [N, out_channels].
        """
        x = self.lin(x)  # (acts like W^T in a transposed convolution)
        # if there are no edges, skip the propagate/unpack step
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return F.relu(x + self.bias)  # just apply bias + activation
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        out = self.propagate(
            edge_index, x=x, norm=deg.pow(0.5)[col] * deg.pow(0.5)[row]
        )  # each node collects from neighbors
        return out + self.bias

    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:
        # x_j: neighbor features [E, out_channels]
        # norm: normalization per edge [E]
        return x_j * norm.view(-1, 1)

    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:
        # optional nonlinearity
        return F.relu(aggr_out)


class GraphDecoder(nn.Module):
    """
    Recurrent generation + GDN refinement.
    1) Recurrently generate node embeddings & edges (GraphRNN style).
    2) Refine embeddings via Graph Deconvolutional Nets (GDNs).
    """

    def __init__(
        self,
        num_node_types: int,
        latent_dim: int,
        shared_attr_vocab: SharedAttributeVocab,
        hidden_dim: int = 128,
        gdn_layers: int = 2,
        edge_threshold: float = 0.5,
    ):
        super().__init__()
        self.shared_attr_vocab = shared_attr_vocab
        self.latent_dim = latent_dim
        # — map latent → initial Node‐RNN state
        self.node_hidden_state_init = nn.Linear(latent_dim, hidden_dim)

        # — Node‐RNN (no input, only hidden evolves)
        self.node_rnn = nn.GRUCell(input_size=0, hidden_size=hidden_dim)
        self.stop_head = nn.Linear(hidden_dim, 1)
        self.node_head = nn.Linear(hidden_dim, hidden_dim)
        self.type_head = nn.Linear(hidden_dim, num_node_types)
        self.edge_rnn = nn.GRUCell(input_size=1, hidden_size=hidden_dim)
        self.edge_head = nn.Linear(hidden_dim, 1)

        self.attr_type_rnn = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_type_head = nn.Linear(hidden_dim, 1)
        self.attr_name_rnn = nn.GRU(input_size=self.shared_attr_vocab.embedding.embedding_dim, hidden_size=hidden_dim)
        self.attr_name_head = nn.Linear(hidden_dim, shared_attr_vocab.embedding.embedding_dim)
        self.attr_dims_head = nn.Linear(hidden_dim, 1)  # determine num dimensions per attr
        self.attr_val_rnn = nn.GRU(input_size=1, hidden_size=hidden_dim)
        self.attr_val_head = nn.Linear(hidden_dim, 1)  # extract value per attr dimension
        self.attr_sos_index = self.shared_attr_vocab.sos_index
        self.attr_eos_index = self.shared_attr_vocab.eos_index
        self.attr_unk_index = self.shared_attr_vocab.unk_index

        # — GraphDeconvNet stack (learned spectral decoders)
        self.gdns = nn.ModuleList([GraphDeconvNet(hidden_dim, hidden_dim) for _ in range(gdn_layers)])
        self.max_nodes = 1000
        self.max_attributes_per_node = 50
        self.edge_threshold = edge_threshold
        # Encourage attribute decoder to terminate by progressively biasing the EOS logit.
        self.attr_eos_bias_base = 0.0
        self.attr_eos_bias_slope = 0.1

    def forward(self, latent, teacher_attr_targets: Optional[List[List[List[int]]]] = None):
        """
        (num_graphs, latent_dim)
        returns: list of graphs, each {'node_types': LongTensor[1, N],
                                       'node_attributes': List[{name: attribute_value} x N],
                                       'edge_index': LongTensor[2, E]}
        """
        _forward_prof = torch.autograd.profiler.record_function("GraphDecoder.forward")
        _forward_prof.__enter__()
        try:
            device = latent.device
            all_graphs = []
            attr_teacher_loss = None
            attr_teacher_tokens = 0
            if teacher_attr_targets is not None:
                attr_teacher_loss = latent.new_tensor(0.0)
                # ensure decoder caps accommodate teacher supervision
                max_teacher_nodes = 0
                max_teacher_attrs = 0
                for graph_targets in teacher_attr_targets:
                    max_teacher_nodes = max(max_teacher_nodes, len(graph_targets))
                    for node_seq in graph_targets:
                        if node_seq:
                            # sequences include EOS token; actual attribute count is len - 1
                            max_teacher_attrs = max(max_teacher_attrs, max(len(node_seq) - 1, 0))
                if max_teacher_nodes > self.max_nodes:
                    self.max_nodes = max_teacher_nodes
                if max_teacher_attrs > self.max_attributes_per_node:
                    self.max_attributes_per_node = max_teacher_attrs

            def make_name_input(token_idx: int) -> torch.Tensor:
                token_tensor = torch.tensor([token_idx], dtype=torch.long, device=device)
                return self.shared_attr_vocab.embedding(token_tensor).view(1, 1, -1)

            with torch.autograd.profiler.record_function("GraphDecoder.graph_loop"):
                for l in range(latent.size(0)):
                    decode_stats = None
                    if DEBUG_DECODER:
                        decode_stats = {
                            "graph_index": l,
                            "node_start": time.perf_counter(),
                            "node_loop_exit": None,
                            "node_stop_samples": [],
                            "attr_events": [],
                        }
                    graph_teacher_targets = None
                    target_node_count = None
                    if teacher_attr_targets is not None and l < len(teacher_attr_targets):
                        graph_teacher_targets = teacher_attr_targets[l]
                        if graph_teacher_targets is not None:
                            target_node_count = len(graph_teacher_targets)
                try:
                    latent_norm = float(latent[l].detach().norm().item())
                except Exception:
                    latent_norm = float("nan")
                logger.info("Decoder graph %d start | latent_norm=%.4f", l, latent_norm)
                hidden_node = F.relu(self.node_hidden_state_init(latent[l])).unsqueeze(0)  # (hidden_dim,)
                node_embeddings = []
                edges = []
                node_types = []
                t = 0
                node_loop_timer = time.perf_counter()
                node_loop_iters = 0
                with torch.autograd.profiler.record_function("GraphDecoder.node_loop"):
                    while True:
                        node_loop_iters += 1
                        if t > self.max_nodes:
                            warn("max nodes reached")
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "max_nodes_pre"
                            break
                        if target_node_count is not None and t >= target_node_count:
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "teacher_nodes"
                            break
                        hidden_node = self.node_rnn(torch.zeros(hidden_node.shape[0], 0, device=device), hidden_node)

                        # clamp for precision errors
                        p_stop = 1 - torch.sigmoid(self.stop_head(hidden_node))
                        p_stop = torch.nan_to_num(p_stop, nan=1.0).clamp(0.0, 1.0)
                        if DEBUG_DECODER and decode_stats is not None:
                            decode_stats["node_stop_samples"].append(float(p_stop.item()))
                        if torch.bernoulli(p_stop).item() == 0:
                            if t == 0:
                                # ensure at least one node
                                p_stop = torch.ones_like(p_stop)
                            else:
                                break
                        if t > self.max_nodes:
                            warn("max nodes reached")
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "max_nodes_post"
                            break
                        new_node = self.node_head(hidden_node).squeeze(0)
                        node_embeddings.append(new_node)
                        node_types.append(self.type_head(new_node).argmax(dim=-1).cpu().tolist())

                        # edge generation to previous nodes
                        hidden_edge = hidden_node
                        edge_in = torch.zeros(1, 1, device=device)
                        for i in range(t):
                            hidden_edge = self.edge_rnn(edge_in, hidden_edge)
                            p_edge = torch.sigmoid(self.edge_head(hidden_edge)).view(-1)
                            p_edge = torch.nan_to_num(p_edge, nan=0.0).clamp(0.0, 1.0)
                            if bool((p_edge > self.edge_threshold).item()):
                                edges.append([i, t])
                            edge_in = p_edge.unsqueeze(0)
                        t += 1
                        if target_node_count is not None and t >= target_node_count:
                            if DEBUG_DECODER and decode_stats is not None:
                                decode_stats["node_loop_exit"] = decode_stats["node_loop_exit"] or "teacher_nodes"
                            break
                        if DEBUG_DECODER and t % 50 == 0:
                            elapsed = time.perf_counter() - node_loop_timer
                            logger.info(
                                "Decoder graph %d node generation progress: %d nodes (max=%d) elapsed=%.2fs",
                                decode_stats["graph_index"],
                                t,
                                self.max_nodes,
                                elapsed,
                            )
                        if DEBUG_DECODER and node_loop_iters % 200 == 0:
                            logger.info(
                                "Decoder graph %d node loop iterations=%d (nodes=%d)",
                                decode_stats["graph_index"],
                                node_loop_iters,
                                t,
                            )

            if node_embeddings:
                node_embeddings = torch.stack(node_embeddings, dim=0)
                edges = torch.tensor(edges, dtype=torch.long).t().contiguous()
            else:
                node_embeddings = torch.zeros((0, self.node_head.out_features), device=device)
                edges = torch.zeros((2, 0), dtype=torch.long, device=device)

            if DEBUG_DECODER:
                decode_stats["node_time"] = time.perf_counter() - decode_stats["node_start"]
                exit_reason = decode_stats["node_loop_exit"] or "stop"
                stop_samples = decode_stats["node_stop_samples"]
                avg_p_stop = float(sum(stop_samples) / len(stop_samples)) if stop_samples else float("nan")
                last_p_stop = stop_samples[-1] if stop_samples else float("nan")
                logger.info(
                    "Decoder graph %d node loop exit=%s nodes=%d max=%d iters=%d avg_p_stop=%.4f last_p_stop=%.4f",
                    decode_stats["graph_index"],
                    exit_reason,
                    node_embeddings.size(0),
                    self.max_nodes,
                    node_loop_iters,
                    avg_p_stop,
                    last_p_stop,
                )
                decode_stats["attr_start"] = time.perf_counter()
                decode_stats["node_count"] = int(node_embeddings.size(0))
                decode_stats["edge_count"] = int(edges.size(1)) if edges.numel() > 0 else 0
                decode_stats["attr_nodes"] = 0
                decode_stats["attr_iters"] = 0
                decode_stats["attr_value_cells"] = 0

            # refine via graph deconvolution
            for gdn in self.gdns:
                node_embeddings = gdn(edges, node_embeddings)

            node_attributes = []
            for node_idx, embedding in enumerate(node_embeddings):
                if DEBUG_DECODER:
                    decode_stats["attr_nodes"] += 1
                attrs = {}
                name_hidden = embedding.unsqueeze(0).unsqueeze(0)
                val_hidden = None
                t = 0
                attr_loop_iters = 0
                used_name_indices: Set[int] = set()
                node_teacher_targets: Optional[List[int]] = None
                attr_exit_reason = "eos"
                last_attr_logits: Optional[Tuple[float, float, int]] = None
                if graph_teacher_targets is not None and node_idx < len(graph_teacher_targets):
                    node_teacher_targets = graph_teacher_targets[node_idx] or None

                def project_name_logits(name_out: torch.Tensor) -> torch.Tensor:
                    return self.attr_name_head(name_out).reshape(-1)

                def add_attribute(name_index: int, name: str):
                    nonlocal t
                    raw_dim = self.attr_dims_head(embedding).squeeze()
                    raw_dim = torch.nan_to_num(raw_dim, nan=0.0, posinf=20.0, neginf=-20.0)
                    dim_val = F.softplus(raw_dim)
                    dim_val = torch.nan_to_num(dim_val, nan=1.0, posinf=float(self.max_attributes_per_node), neginf=1.0)
                    attr_dims = int(torch.clamp(dim_val, min=1.0, max=float(self.max_attributes_per_node)).item())
                    attr_dims = max(1, min(attr_dims, self.max_attributes_per_node))
                    values = []
                    value_hidden = embedding.unsqueeze(0).unsqueeze(1)
                    value_input = torch.zeros(1, 1, 1, device=device)
                    for _ in range(attr_dims):
                        value_out, value_hidden = self.attr_val_rnn(value_input, value_hidden)
                        v = self.attr_val_head(value_out).view(-1)
                        values.append(v)
                        value_input = v.unsqueeze(0).unsqueeze(-1)
                    if DEBUG_DECODER:
                        decode_stats["attr_value_cells"] += attr_dims
                        if attr_dims >= self.max_attributes_per_node:
                            logger.info(
                                "Decoder graph %d node %d attribute %s clamped to max dims (%d)",
                                decode_stats["graph_index"],
                                node_idx,
                                name,
                                self.max_attributes_per_node,
                            )
                    if name in attrs:
                        warn(name + " is already defined for currently decoding node")
                        return False
                    attrs[name] = torch.stack(values)
                    t += 1
                    used_name_indices.add(name_index)
                    if DEBUG_DECODER and (t % 25 == 0):
                        logger.info(
                            "Decoder graph %d node %d attr_count=%d",
                            decode_stats["graph_index"],
                            node_idx,
                            t,
                        )
                    return True

                with torch.autograd.profiler.record_function("GraphDecoder.attr_loop"):
                    if node_teacher_targets:
                        prev_token_idx = self.attr_sos_index
                        for target_idx in node_teacher_targets:
                            attr_loop_iters += 1
                            if DEBUG_DECODER and attr_loop_iters % 50 == 0:
                                logger.info(
                                    "Decoder graph %d node %d attr_name iterations=%d",
                                    decode_stats["graph_index"],
                                    node_idx,
                                    attr_loop_iters,
                                )
                            if t > self.max_attributes_per_node:
                                warn("max attributes per node reached")
                                attr_exit_reason = "max_attr_teacher"
                                if DEBUG_DECODER:
                                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                                    teacher_len = len(node_teacher_targets) if node_teacher_targets else 0
                                    logger.info(
                                        "Decoder graph %d node %d attr loop hit teacher cap | attrs=%d of %d | last_eos=%.3f last_max=%.3f last_idx=%d",
                                        decode_stats["graph_index"],
                                        node_idx,
                                        t,
                                        teacher_len,
                                        eos_logit,
                                        top_logit,
                                        top_idx,
                                    )
                                break
                            name_input = make_name_input(prev_token_idx)
                            name_out, name_hidden = self.attr_name_rnn(name_input, name_hidden)
                            if DEBUG_DECODER:
                                decode_stats["attr_iters"] += 1
                            similarity_logits = torch.matmul(
                                self.shared_attr_vocab.embedding.weight,
                                project_name_logits(name_out),
                            )
                            if DEBUG_DECODER:
                                top_logit, top_idx = torch.max(similarity_logits, dim=0)
                                last_attr_logits = (
                                    float(similarity_logits[self.attr_eos_index].item()),
                                    float(top_logit.item()),
                                    int(top_idx.item()),
                                )
                            eos_bias = self.attr_eos_bias_base + self.attr_eos_bias_slope * t
                            similarity_logits[self.attr_eos_index] = similarity_logits[self.attr_eos_index] + eos_bias
                            target_tensor = torch.tensor([target_idx], dtype=torch.long, device=device)
                            if attr_teacher_loss is not None:
                                step_loss = F.cross_entropy(
                                    similarity_logits.unsqueeze(0), target_tensor, reduction="mean"
                                )
                                attr_teacher_loss = attr_teacher_loss + step_loss
                                attr_teacher_tokens += 1
                            prev_token_idx = target_idx
                            if target_idx == self.attr_eos_index:
                                break
                            name = self.shared_attr_vocab.index_to_name.get(target_idx)
                            if name is None:
                                name = generate_random_string(8)
                                target_idx = self.shared_attr_vocab.ensure_index(name)
                            add_attribute(target_idx, name)
                    else:
                        prev_token_idx = self.attr_sos_index
                        while True:
                            attr_loop_iters += 1
                            if DEBUG_DECODER and attr_loop_iters % 50 == 0:
                                logger.info(
                                    "Decoder graph %d node %d attr_name iterations=%d",
                                    decode_stats["graph_index"],
                                    node_idx,
                                    attr_loop_iters,
                                )
                            if t > self.max_attributes_per_node:
                                warn("max attributes per node reached")
                                attr_exit_reason = "max_attr_sampling"
                                if DEBUG_DECODER:
                                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                                    logger.info(
                                        "Decoder graph %d node %d attr loop hit sampling cap | attrs=%d | last_eos=%.3f last_max=%.3f last_idx=%d",
                                        decode_stats["graph_index"],
                                        node_idx,
                                        t,
                                        eos_logit,
                                        top_logit,
                                        top_idx,
                                    )
                                break
                            name_input = make_name_input(prev_token_idx)
                            name_out, name_hidden = self.attr_name_rnn(name_input, name_hidden)
                            if DEBUG_DECODER:
                                decode_stats["attr_iters"] += 1
                            similarity_logits = torch.matmul(
                                self.shared_attr_vocab.embedding.weight,
                                project_name_logits(name_out),
                            )
                            if DEBUG_DECODER:
                                top_logit, top_idx = torch.max(similarity_logits, dim=0)
                                last_attr_logits = (
                                    float(similarity_logits[self.attr_eos_index].item()),
                                    float(top_logit.item()),
                                    int(top_idx.item()),
                                )
                            if used_name_indices:
                                used_idx_tensor = torch.tensor(
                                    list(used_name_indices), device=similarity_logits.device, dtype=torch.long
                                )
                                similarity_logits.index_fill_(0, used_idx_tensor, -1e4)
                            eos_bias = self.attr_eos_bias_base + self.attr_eos_bias_slope * t
                            similarity_logits[self.attr_eos_index] = similarity_logits[self.attr_eos_index] + eos_bias
                            name_index = int(similarity_logits.argmax().item())
                            prev_token_idx = name_index
                            if name_index == self.attr_eos_index:
                                break
                            elif name_index == self.attr_unk_index:
                                name = generate_random_string(8)
                                while name in attrs:
                                    name = generate_random_string(8)
                                name_index = self.shared_attr_vocab.ensure_index(name)
                            else:
                                name = self.shared_attr_vocab.index_to_name.get(name_index)
                                if name is None:
                                    name = generate_random_string(8)
                                    name_index = self.shared_attr_vocab.ensure_index(name)
                                else:
                                    name_index = self.shared_attr_vocab.ensure_index(name)
                                if name in attrs:
                                    continue

                            add_attribute(name_index, name)
                node_attributes.append(attrs)
                if DEBUG_DECODER:
                    teacher_len = len(node_teacher_targets) if node_teacher_targets else 0
                    eos_logit = float("nan") if last_attr_logits is None else last_attr_logits[0]
                    top_logit = float("nan") if last_attr_logits is None else last_attr_logits[1]
                    top_idx = -1 if last_attr_logits is None else last_attr_logits[2]
                    decode_stats["attr_events"].append(
                        {
                            "node_idx": node_idx,
                            "exit_reason": attr_exit_reason,
                            "attr_count": t,
                            "teacher_len": teacher_len,
                            "last_eos_logit": eos_logit,
                            "last_top_logit": top_logit,
                            "last_top_idx": top_idx,
                        }
                    )
                    if attr_exit_reason.startswith("max_attr"):
                        logger.info(
                            "Decoder graph %d node %d summary | exit=%s attrs=%d teacher_len=%d last_eos=%.3f last_max=%.3f last_idx=%d",
                            decode_stats["graph_index"],
                            node_idx,
                            attr_exit_reason,
                            t,
                            teacher_len,
                            eos_logit,
                            top_logit,
                            top_idx,
                        )
                all_graphs.append(
                    {"node_types": torch.as_tensor(node_types), "node_attributes": node_attributes, "edge_index": edges}
                )
                if DEBUG_DECODER:
                    attr_time = time.perf_counter() - decode_stats["attr_start"]
                    total_time = decode_stats["node_time"] + attr_time
                    logger.info(
                        "Decoder graph %d: nodes=%d edges=%d attr_nodes=%d attr_names=%d attr_values=%d | node_time=%.3fs attr_time=%.3fs total=%.3fs",
                        decode_stats["graph_index"],
                        decode_stats["node_count"],
                        decode_stats["edge_count"],
                        decode_stats["attr_nodes"],
                        decode_stats["attr_iters"],
                        decode_stats["attr_value_cells"],
                        decode_stats["node_time"],
                        attr_time,
                        total_time,
                    )
                    attr_cap_hits = 0
                    teacher_cap_hits = 0
                    for event in decode_stats["attr_events"]:
                        if event["exit_reason"].startswith("max_attr"):
                            attr_cap_hits += 1
                            if event["exit_reason"].endswith("teacher"):
                                teacher_cap_hits += 1
                    if attr_cap_hits:
                        logger.info(
                            "Decoder graph %d attr cap hits=%d (teacher=%d, sampling=%d)",
                            decode_stats["graph_index"],
                            attr_cap_hits,
                            teacher_cap_hits,
                            attr_cap_hits - teacher_cap_hits,
                        )
            if attr_teacher_loss is not None:
                return all_graphs, {"loss": attr_teacher_loss, "tokens": attr_teacher_tokens}
            return all_graphs
        finally:
            _forward_prof.__exit__(None, None, None)

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        """
        Permanently remove unused latent dimensions from the node_hidden_state_init layer.
        kept_idx: 1D LongTensor of indices to keep from the original latent vector.
        """
        device = self.node_hidden_state_init.weight.device
        new_node_hidden_state_init = nn.Linear(
            kept_idx.numel(), self.node_hidden_state_init.out_features, bias=True
        ).to(device)
        new_node_hidden_state_init.weight.data.copy_(old.weight.data[:, kept_idx])

        new_node_hidden_state_init.bias.data.copy_(old.bias.data)
        self.node_hidden_state_init = new_node_hidden_state_init
        self.latent_dim = kept_idx.numel()


class InputConvexLayer(nn.Module):
    """Single ICNN layer enforcing non-negative weights on the z-term."""

    def __init__(self, input_dim: int, z_dim: int, out_dim: int, activation=F.softplus):
        super().__init__()
        self.activation = activation
        self.x_linear = nn.Linear(input_dim, out_dim)
        if z_dim > 0:
            self.z_weight = nn.Parameter(torch.empty(out_dim, z_dim))
            nn.init.xavier_uniform_(self.z_weight)
        else:
            self.z_weight = None
        self.bias = nn.Parameter(torch.zeros(out_dim))

    def forward(self, x: torch.Tensor, z_prev: Optional[torch.Tensor]):
        z_term = 0.0
        if self.z_weight is not None and z_prev is not None:
            z_term = F.linear(z_prev, F.softplus(self.z_weight), bias=None)
        return self.activation(self.x_linear(x) + z_term + self.bias)

    def prune_input(self, kept_idx: torch.LongTensor):
        kept = kept_idx.to(self.x_linear.weight.device)
        new_linear = nn.Linear(len(kept), self.x_linear.out_features).to(self.x_linear.weight.device)
        new_linear.weight.data.copy_(self.x_linear.weight.data[:, kept])
        new_linear.bias.data.copy_(self.x_linear.bias.data)
        self.x_linear = new_linear


class InputConvexNN(nn.Module):
    """Input Convex Neural Network surrogate f(z)."""

    def __init__(self, input_dim: int, hidden_dims: Sequence[int], output_dim: int, activation=F.softplus):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.layers = nn.ModuleList()
        prev_dim = 0
        for hidden_dim in hidden_dims:
            self.layers.append(InputConvexLayer(input_dim, prev_dim, hidden_dim, activation=activation))
            prev_dim = hidden_dim
        self.final_bias = nn.Parameter(torch.zeros(output_dim))
        if prev_dim > 0:
            self.final_z_weight = nn.Parameter(torch.empty(output_dim, prev_dim))
            nn.init.xavier_uniform_(self.final_z_weight)
        else:
            self.final_z_weight = None
        self.final_linear = nn.Linear(input_dim, output_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = None
        for layer in self.layers:
            z = layer(x, z)
        z_term = 0.0
        if z is not None and self.final_z_weight is not None:
            z_term = F.linear(z, F.softplus(self.final_z_weight), bias=None)
        return z_term + self.final_linear(x) + self.final_bias

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        for layer in self.layers:
            layer.prune_input(kept_idx)
        kept = kept_idx.to(self.final_linear.weight.device)
        new_final = nn.Linear(len(kept), self.final_linear.out_features).to(self.final_linear.weight.device)
        new_final.weight.data.copy_(self.final_linear.weight.data[:, kept])
        new_final.bias.data.copy_(self.final_linear.bias.data)
        self.final_linear = new_final


class FitnessPredictor(nn.Module):
    """
    Heteroscedastic predictor paired with an optional ICNN convex surrogate
    (see "Input Convex Neural Networks" by Brandon Amos, Lei Xu, J. Zico Kolter [2017]).
    """

    def __init__(
        self,
        latent_dim,
        hidden_dim=64,
        fitness_dim=4,
        icnn_hidden_dims: Optional[Sequence[int]] = None,
        icnn_activation=F.softplus,
    ):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, fitness_dim)
        self.output_dim = fitness_dim
        self.latent_dim = latent_dim
        self.log_metric_scale = nn.Parameter(torch.zeros(fitness_dim))
        if icnn_hidden_dims:
            self.icnn = InputConvexNN(latent_dim, icnn_hidden_dims, fitness_dim, activation=icnn_activation)
        else:
            self.icnn = None

    def forward(self, z_graph: torch.Tensor):
        pred = self.fc2(F.relu(self.fc1(z_graph)))
        log_scales = self.log_metric_scale.unsqueeze(0).expand(pred.size(0), -1)
        convex_pred = self.icnn(z_graph) if self.icnn is not None else None
        return pred, log_scales, convex_pred

    def prune_latent_dims(self, kept_idx: torch.LongTensor):
        kept = kept_idx.to(self.fc1.weight.device)
        new_fc1 = nn.Linear(len(kept), self.fc1.out_features).to(self.fc1.weight.device)
        new_fc1.weight.data.copy_(self.fc1.weight.data[:, kept])
        new_fc1.bias.data.copy_(self.fc1.bias.data)
        self.fc1 = new_fc1
        if self.icnn is not None:
            self.icnn.prune_latent_dims(kept_idx)
        self.latent_dim = len(kept_idx)


class SelfCompressingFitnessRegularizedDAGVAE(nn.Module):
    """
    DAG-VAE with ARD prior that is regularized by fitness prediction from latent and has
    latent-mask for dynamic pruning.
    """

    def __init__(
        self,
        graph_encoder: GraphEncoder,
        decoder: GraphDecoder,
        fitness_predictor: FitnessPredictor,
    ):
        super().__init__()
        self.graph_encoder = graph_encoder
        self.decoder = decoder
        self.fitness_predictor = fitness_predictor
        # ARD: learnable log-precision per latent dimension
        self.log_alpha_g = nn.Parameter(torch.zeros(self.graph_encoder.latent_dim))
        # masks for active latent dims (1=active, 0=pruned)
        self.register_buffer("graph_latent_mask", torch.ones(self.graph_encoder.latent_dim))

    @property
    def shared_attr_vocab(self):
        return self.graph_encoder.shared_attr_vocab

    @property
    def attr_encoder(self):
        return self.graph_encoder.attr_encoder

    @property
    def max_value_dim(self):
        return self.graph_encoder.max_value_dim

    def encode(
        self,
        node_types,
        edge_index,
        node_attributes,
        batch,
        num_graphs=None,
    ):
        return self.graph_encoder(node_types, edge_index, node_attributes, batch, num_graphs=num_graphs)

    def reparameterize(self, mu, logvar, latent_mask):
        std = torch.exp(0.5 * logvar)
        z = mu + torch.randn_like(std) * std
        return z if latent_mask is None else z * latent_mask

    def decode(self, z, teacher_attr_targets: Optional[List[List[List[int]]]] = None):
        return self.decoder(z, teacher_attr_targets=teacher_attr_targets)

    def forward(
        self,
        node_types,
        edge_index,
        node_attributes,
        batch,
        teacher_attr_targets: Optional[List[List[List[int]]]] = None,
        num_graphs: Optional[int] = None,
    ):
        mu_g, lv_g = self.encode(
            node_types,
            edge_index,
            node_attributes,
            batch,
            num_graphs=num_graphs,
        )
        graph_latent = self.reparameterize(mu_g, lv_g, self.graph_latent_mask)
        decoded = self.decode(graph_latent, teacher_attr_targets=teacher_attr_targets)
        if isinstance(decoded, tuple):
            decoded_graphs, decoder_aux = decoded
        else:
            decoded_graphs, decoder_aux = decoded, None
        fitness_pred, metric_log_scales, convex_pred = self.fitness_predictor(graph_latent)
        return (
            decoded_graphs,
            fitness_pred,
            metric_log_scales,
            convex_pred,
            graph_latent,
            mu_g,
            lv_g,
            decoder_aux,
        )

    def prune_latent_dims(self, num_prune=1):
        # Identify and mask out dims with highest precision (least variance)
        def prune(latent_mask, log_alpha, desc):
            # precision = exp(log_alpha)
            latent_mask = latent_mask.cpu().numpy().astype(bool)
            precisions = torch.exp(log_alpha).detach().cpu().numpy()
            active_indices = np.where(latent_mask)[0]
            active_indices = active_indices[np.argsort(precisions[active_indices])]
            dims_to_prune = active_indices[-num_prune:]
            dims_to_prune = [d for d in dims_to_prune.tolist() if precisions[d] > 1]
            latent_mask[dims_to_prune] = 0.0
            if len(dims_to_prune) > 0:
                print(f"Pruned {desc} latent dims: {dims_to_prune}")
            return torch.tensor(latent_mask)

        self.graph_latent_mask.copy_(prune(self.graph_latent_mask, self.log_alpha_g, "graph"))

    def resize_bottleneck(self):
        """
        Permanently shrink bottleneck to active dims via the per-module prune methods.
        """
        graph_kept_idx = self.graph_latent_mask.nonzero(as_tuple=True)[0]
        old_graph_dim = self.graph_latent_mask.numel()
        new_graph_dim = graph_kept_idx.numel()
        if new_graph_dim == old_graph_dim:
            print("No graph latent dims to permanently prune.")
            return
        self.graph_encoder.prune_latent_dims(graph_kept_idx)
        self.decoder.prune_latent_dims(graph_kept_idx)
        self.fitness_predictor.prune_latent_dims(graph_kept_idx)
        self.log_alpha_g = nn.Parameter(self.log_alpha_g.data[graph_kept_idx].clone())
        self.register_buffer("graph_latent_mask", torch.ones(graph_kept_idx.numel()))
        print(f"Permanently resizing graph bottleneck: {old_graph_dim} → {new_graph_dim}")


@dataclass
class StagedBetaSchedule:
    """Piecewise-linear β schedule with warmup, ramp, hold, and optional cycles."""

    start_beta: float = 0.0
    target_beta: float = 0.1
    warmup_epochs: int = 0
    ramp_epochs: int = 50
    hold_epochs: int = 0
    cycle_length: Optional[int] = None
    cycle_floor: Optional[float] = None

    def __post_init__(self):
        if self.start_beta < 0 or self.target_beta < 0:
            raise ValueError("β values must be non-negative")
        for field in ("warmup_epochs", "ramp_epochs", "hold_epochs"):
            value = getattr(self, field)
            if value < 0:
                raise ValueError(f"{field} must be >= 0")
        if self.cycle_length is not None and self.cycle_length <= 0:
            raise ValueError("cycle_length must be None or > 0")

    def value(self, epoch_index: int) -> float:
        """Return β for the provided (0-indexed) global epoch count."""

        epoch = max(0, int(epoch_index))
        if epoch < self.warmup_epochs:
            return self.start_beta

        epoch -= self.warmup_epochs
        if self.ramp_epochs > 0 and epoch < self.ramp_epochs:
            ratio = min(1.0, (epoch + 1) / max(1, self.ramp_epochs))
            return self.start_beta + ratio * (self.target_beta - self.start_beta)

        epoch -= max(self.ramp_epochs, 0)
        if self.hold_epochs > 0:
            if epoch < self.hold_epochs:
                return self.target_beta
            epoch -= self.hold_epochs

        if not self.cycle_length:
            return self.target_beta

        floor = self.cycle_floor if self.cycle_floor is not None else self.start_beta
        if self.target_beta <= floor:
            return self.target_beta

        phase = epoch % self.cycle_length
        frac = phase / self.cycle_length
        return floor + frac * (self.target_beta - floor)


class OnlineTrainer:
    """
    Incremental trainer with ARD-KL + loss-thresholded iterative pruning,
    supporting variable node-feature dimensions via list-of-graphs decoding.
    """

    def __init__(
        self,
        model: SelfCompressingFitnessRegularizedDAGVAE,
        optimizer,
        task=None,
        metric_keys: Optional[Sequence] = None,
    ):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model.to(self.device)
        self.optimizer = optimizer
        self.dataset = []
        self.invalid_dataset = []
        self._graph_signatures: Set[str] = set()
        self.loss_history = []
        self.initial_loss = None
        self.last_prune_epoch = 0
        self.kl_scheduler: Optional[StagedBetaSchedule] = None
        self._kl_global_epoch = 0
        if metric_keys is None:
            if task is None:
                raise ValueError("OnlineTrainer requires either `task` or `metric_keys`.")
            metric_keys = task.metrics
        self._update_metric_metadata(metric_keys)
        self.invalid_target_ratio = 0.2
        self.invalid_ratio_warmup = 5
        self.progress_callback = None
        self.prune_callback = None
        self.decoder_empty_penalty = 0.0

    def _update_metric_metadata(self, metric_keys: Sequence):
        self.sorted_metrics = sort_metrics_by_name(metric_keys)
        self.num_metrics = len(self.sorted_metrics)
        self.task_metric_best_values = torch.as_tensor(
            [metric.best_value for metric in self.sorted_metrics], dtype=torch.float
        )
        self.task_metric_guidance_weights = torch.as_tensor(
            [metric.guidance_weight for metric in self.sorted_metrics], dtype=torch.float
        )

    def _compute_reconstruction_losses(
        self,
        batch,
        decoded_graphs,
        decoder_aux,
        target_graph_attrs,
        teacher_force_weight: float = 1.0,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        dense_adj = to_dense_adj(batch.edge_index, batch.batch, max_num_nodes=None)
        num_target_graphs = dense_adj.size(0)
        graphs_to_compare = min(len(decoded_graphs), num_target_graphs, len(target_graph_attrs))

        loss_adj = torch.tensor(0.0, device=self.device)
        loss_feat = torch.tensor(0.0, device=self.device)
        empty_penalty = float(getattr(self, "decoder_empty_penalty", 0.0) or 0.0)
        empty_hits = 0

        for i in range(graphs_to_compare):
            dg = decoded_graphs[i]
            pred_edges = dg["edge_index"]
            pred_attrs = dg["node_attributes"]
            target_attrs = target_graph_attrs[i] if i < len(target_graph_attrs) else []
            num_target_nodes = len(target_attrs)
            num_pred_nodes = len(pred_attrs)
            num_nodes = min(num_pred_nodes, num_target_nodes)
            if num_nodes == 0:
                empty_hits += 1
                continue
            adj_true = dense_adj[i, :num_target_nodes, :num_target_nodes]
            adj_true = adj_true[:num_nodes, :num_nodes].to(self.device)

            # build binary adjacency preds/targets on 0..num_nodes-1
            no_pred_edges = True
            adj_pred = torch.zeros((num_nodes, num_nodes), device=self.device)
            for s, d in pred_edges.t().tolist():
                if s < num_nodes and d < num_nodes:
                    adj_pred[s, d] = 1.0
                    no_pred_edges = False

            upper_mask = torch.triu(torch.ones_like(adj_pred), diagonal=1).bool()
            if adj_pred[upper_mask].size(0) > 0 and adj_true[upper_mask].size(0) > 0:
                loss_adj += F.binary_cross_entropy_with_logits(adj_pred[upper_mask], adj_true[upper_mask])
            elif adj_pred[upper_mask].size(0) > 0:
                loss_adj += adj_pred[upper_mask].sum()
            else:
                loss_adj += adj_true[upper_mask].sum()

            # node feature loss
            def convert_string(value):
                if isinstance(value, str):
                    value = self.model.attr_encoder.get_value_tensor(value)
                return value

            def to_tensor(value):
                value = convert_string(value)
                if isinstance(value, torch.Tensor):
                    tensor = value.detach()
                else:
                    tensor = torch.as_tensor([value], dtype=torch.float)
                return tensor.reshape(-1).float().to(self.device)

            def attribute_value_loss(value):
                tensor = to_tensor(value)
                return tensor.abs().sum()

            def mse_aligned(pred_tensor, target_tensor):
                if pred_tensor.numel() == target_tensor.numel():
                    return F.mse_loss(pred_tensor, target_tensor)
                size = min(pred_tensor.numel(), target_tensor.numel())
                loss = F.mse_loss(pred_tensor[:size], target_tensor[:size])
                if pred_tensor.numel() > size:
                    loss = loss + pred_tensor[size:].abs().sum()
                if target_tensor.numel() > size:
                    loss = loss + target_tensor[size:].abs().sum()
                return loss

            for node_idx in range(num_nodes):
                all_attr_names = set(pred_attrs[node_idx].keys()) | set(target_attrs[node_idx].keys())
                for attr_name in all_attr_names:
                    pred_value = pred_attrs[node_idx].get(attr_name)
                    target_value = target_attrs[node_idx].get(attr_name)
                    if (pred_value is not None) and (target_value is not None):
                        loss_feat += mse_aligned(to_tensor(pred_value), to_tensor(target_value))
                    elif target_value is not None:
                        loss_feat += attribute_value_loss(target_value)
                    else:
                        loss_feat += attribute_value_loss(pred_value)

        if decoder_aux is not None and decoder_aux.get("tokens", 0) > 0:
            ce_loss = decoder_aux["loss"] / float(decoder_aux["tokens"])
            loss_feat = loss_feat + float(teacher_force_weight) * ce_loss.to(loss_feat.device)

        denom = max(1, int(batch.num_graphs))
        if empty_penalty > 0 and empty_hits:
            loss_adj = loss_adj + empty_penalty * empty_hits
        loss_adj = loss_adj / denom
        loss_feat = loss_feat / denom
        if graphs_to_compare == 0 and empty_penalty > 0:
            loss_adj = loss_adj + empty_penalty
        return loss_adj, loss_feat

    def _graph_dict_to_data(self, graph_dict: dict | None) -> Data | None:
        if not graph_dict:
            return None
        node_types = graph_dict.get("node_types")
        edge_index = graph_dict.get("edge_index")
        node_attributes = graph_dict.get("node_attributes")
        if node_types is None or edge_index is None or node_attributes is None:
            return None
        node_types = node_types.clone().detach().long()
        if isinstance(edge_index, torch.Tensor):
            edge_index = edge_index.clone().detach().long()
        else:
            edge_index = torch.as_tensor(edge_index, dtype=torch.long)
        data = Data(node_types=node_types, edge_index=edge_index, node_attributes=node_attributes)
        data.y = torch.zeros(self.num_metrics)
        return data

    def _metric_name_for_index(self, idx: int) -> str:
        if 0 <= idx < self.num_metrics:
            metric = self.sorted_metrics[idx]
            return getattr(metric, "name", str(metric))
        return f"metric_{idx}"

    def set_progress_callback(self, callback):
        """Register a callable invoked after each epoch with loss summaries."""
        self.progress_callback = callback

    def set_prune_callback(self, callback):
        """Register a callable invoked whenever latent dims are pruned."""
        self.prune_callback = callback

    def configure_kl_scheduler(self, scheduler: Optional[StagedBetaSchedule], reset_state: bool = False):
        """Attach a KL scheduler; optionally reset the accumulated epoch counter."""

        self.kl_scheduler = scheduler
        if reset_state:
            self._kl_global_epoch = 0

    def _resolve_kl_weight(self, static_weight: float) -> float:
        if self.kl_scheduler is None:
            return float(static_weight)
        return float(self.kl_scheduler.value(self._kl_global_epoch))

    def add_data(self, graphs, fitnesses, invalid_flags=None):
        if invalid_flags is None:
            invalid_flags = [False] * len(graphs)
        skipped = 0
        for graph, fitness_dict, is_invalid in zip(graphs, fitnesses, invalid_flags):
            signature = graph_signature_from_data(graph)
            if signature in self._graph_signatures:
                skipped += 1
                continue
            self._graph_signatures.add(signature)
            data = graph.clone()
            fitness = [float(fitness_dict[metric]) for metric in self.sorted_metrics]
            data.y = torch.as_tensor(fitness, dtype=torch.float)
            target_list = self.invalid_dataset if is_invalid else self.dataset
            target_list.append(data)
        if skipped and DEBUG_TRAINER:
            logger.info("OnlineTrainer.add_data skipped %d duplicate graphs", skipped)

    def train(
        self,
        epochs=1,
        batch_size=16,
        kl_weight=10.0,
        fitness_weight=1.5,
        convex_weight=0.5,
        warmup_epochs=None,
        loss_threshold=0.9,
        min_prune_break=5,
        prune_amount=1,
        min_active_dims=5,
        stop_epsilon=1e-3,
        verbose=True,
        generation=0,
        baseline_window=5,
    ):
        if warmup_epochs is None:
            warmup_epochs = 0
        warmup_epochs = max(0, int(warmup_epochs))
        baseline_window = max(1, int(baseline_window))
        recent_loss_window = deque(maxlen=baseline_window)

        train_samples = list(self.dataset)
        if self.invalid_dataset:
            if train_samples:
                train_samples.extend(self.invalid_dataset)
            else:
                train_samples = list(self.invalid_dataset)
        loader = DataLoader(train_samples, batch_size=batch_size, shuffle=True)
        dataset_size = len(train_samples)
        total_batches = len(loader)
        train_start = time.perf_counter()
        if DEBUG_TRAINER:
            logger.info(
                "OnlineTrainer.train start | samples=%d batch_size=%d epochs=%s",
                dataset_size,
                batch_size,
                epochs,
            )
        epoch = 1
        loss_term_labels = [
            "adjacency_recon",
            "attribute_recon",
            "graph_kl",
            "fitness",
            "convex_surrogate",
        ]
        num_loss_terms = len(loss_term_labels)
        prev_loss_terms = torch.zeros(num_loss_terms)
        avg_loss_terms = torch.zeros(num_loss_terms)
        last_loss_term_change = float("inf")

        def stop():
            if epochs is not None:
                return epoch > epochs
            return epoch > 1 and last_loss_term_change < stop_epsilon

        while not stop():
            epoch_timer = time.perf_counter() if DEBUG_TRAINER else None
            prev_loss_terms = avg_loss_terms.clone()
            total_loss_terms = torch.zeros(num_loss_terms)
            per_metric_error_sum = None
            per_metric_weight_sum = None
            self.model.train()
            current_kl_weight = self._resolve_kl_weight(kl_weight)
            for batch_idx, batch in enumerate(loader, start=1):
                batch_timer = time.perf_counter() if DEBUG_TRAINER else None
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                target_graph_attrs = group_node_attributes(batch.node_attributes, batch.batch)
                teacher_attr_targets = build_teacher_attr_targets(
                    target_graph_attrs, None, self.model.shared_attr_vocab
                )
                # --- 1) forward pass ---
                if DEBUG_TRAINER:
                    logger.info(
                        "Trainer epoch %d batch %d/%d forward start | graphs=%d",
                        epoch,
                        batch_idx,
                        total_batches,
                        batch.num_graphs,
                    )
                (
                    decoded_graphs,
                    fitness_pred,
                    metric_log_scales,
                    convex_pred,
                    graph_latent,
                    mu_g,
                    lv_g,
                    decoder_aux,
                ) = self.model(
                    batch.node_types,
                    batch.edge_index,
                    batch.node_attributes,
                    batch.batch,
                    teacher_attr_targets=teacher_attr_targets,
                    num_graphs=batch.num_graphs,
                )
                target_y = batch.y

                # --- 2) reconstruction losses ---
                loss_adj, loss_feat = self._compute_reconstruction_losses(
                    batch,
                    decoded_graphs,
                    decoder_aux,
                    target_graph_attrs,
                    teacher_force_weight=1.0,
                )

                # --- 3) ARD-KL losses ---
                def calc_kl_div_per_dim(log_alpha, logvar, mu):
                    # ARD-KL divergence per sample and per dim
                    precision = torch.exp(log_alpha)
                    var = torch.exp(logvar)
                    # KL formula: 0.5*(-log_alpha - logvar + precision*(var + mu^2) - 1)
                    return 0.5 * (-log_alpha - logvar + precision * (var + mu.pow(2)) - 1)

                graph_kl_loss = calc_kl_div_per_dim(self.model.log_alpha_g, lv_g, mu_g).sum(dim=1).mean()

                # --- 4) fitness loss ---
                target = target_y.to(self.device)
                if target.dim() == 1:
                    target = target.view(batch.num_graphs, self.num_metrics)
                best = self.task_metric_best_values.to(self.device)
                weights = self.task_metric_guidance_weights.to(self.device)
                target_canonical = canonical_log_distance(target, best)
                pred_canonical = canonical_log_distance(fitness_pred, best)
                sq_error = (pred_canonical - target_canonical).pow(2)
                hetero_term = sq_error * torch.exp(-metric_log_scales) + metric_log_scales
                weight_expand = weights.unsqueeze(0).expand_as(sq_error)
                weighted_error = hetero_term * weight_expand
                denom = weight_expand.sum().clamp_min(1.0)
                loss_fitness = weighted_error.sum() / denom
                raw_weighted_error = sq_error * weight_expand
                if per_metric_error_sum is None or per_metric_error_sum.numel() != weighted_error.size(-1):
                    per_metric_error_sum = torch.zeros(weighted_error.size(-1))
                    per_metric_weight_sum = torch.zeros(weighted_error.size(-1))
                per_metric_error_sum += raw_weighted_error.detach().sum(dim=0).cpu()
                per_metric_weight_sum += weight_expand.detach().sum(dim=0).cpu()

                loss_convex = torch.tensor(0.0, device=self.device)
                if convex_pred is not None:
                    convex_canonical = canonical_log_distance(convex_pred, best)
                    convex_error = (convex_canonical - target_canonical).pow(2)
                    convex_weighted = convex_error * weight_expand
                    loss_convex = convex_weighted.sum() / denom

                # --- 5) total & backward ---
                loss_terms = [
                    loss_adj,
                    loss_feat,
                    current_kl_weight * graph_kl_loss,
                    fitness_weight * loss_fitness,
                    convex_weight * loss_convex,
                ]
                sum(loss_terms).backward()
                self.optimizer.step()
                total_loss_terms += torch.tensor(loss_terms)
                if DEBUG_TRAINER and batch_timer is not None:
                    logger.info(
                        "Trainer epoch %d batch %d/%d finished | duration=%.3fs",
                        epoch,
                        batch_idx,
                        total_batches,
                        time.perf_counter() - batch_timer,
                    )
            avg_loss_terms = total_loss_terms / len(loader)
            self.loss_history.append(avg_loss_terms.cpu().numpy())
            total_loss = float(avg_loss_terms.sum().item())
            recent_loss_window.append(total_loss)
            smoothed_loss = sum(recent_loss_window) / len(recent_loss_window)

            per_metric_losses = {}
            if per_metric_weight_sum is not None and per_metric_weight_sum.numel() > 0:
                metric_values = per_metric_error_sum / per_metric_weight_sum.clamp_min(1.0)
                metric_values = metric_values.tolist()
                weight_list = per_metric_weight_sum.tolist()
                for idx, value in enumerate(metric_values):
                    if idx >= len(weight_list) or weight_list[idx] <= 0:
                        continue
                    per_metric_losses[self._metric_name_for_index(idx)] = float(value)

            if self.progress_callback is not None:
                loss_metrics = {name: float(value) for name, value in zip(loss_term_labels, avg_loss_terms.tolist())}
                loss_metrics["kl_beta"] = float(current_kl_weight)
                self.progress_callback(
                    generation=generation,
                    epoch=epoch,
                    total_epochs=epochs,
                    total_loss=total_loss,
                    loss_terms=loss_metrics,
                    per_metric_losses=per_metric_losses,
                    kl_beta=float(current_kl_weight),
                )

            if verbose:
                label_str = ", ".join(
                    f"{name}={value:.4g}" for name, value in zip(loss_term_labels, avg_loss_terms.tolist())
                )
                label_str = f"{label_str}, kl_beta={current_kl_weight:.4g}"
                if not epochs:
                    print(f"Epoch {epoch}, Loss terms per batch: [{label_str}] (total={total_loss:.4f})")
                else:
                    print(f"Epoch {epoch}/{epochs}, Loss terms per batch: [{label_str}] (total={total_loss:.4f})")
            if DEBUG_TRAINER and epoch_timer is not None:
                logger.info(
                    "Trainer epoch %d complete | duration=%.3fs loss=%.4f",
                    epoch,
                    time.perf_counter() - epoch_timer,
                    total_loss,
                )

            # Track convergence of reconstruction terms for adaptive pruning.
            last_loss_term_change = (avg_loss_terms - prev_loss_terms).abs().max().item()
            if self.initial_loss is None and epoch >= warmup_epochs:
                self.initial_loss = smoothed_loss

            if (
                self.initial_loss is not None
                and total_loss <= loss_threshold * self.initial_loss
                and (epoch - self.last_prune_epoch) >= min_prune_break
            ):
                active_count = int(self.model.graph_latent_mask.sum().item())
                if active_count > min_active_dims:
                    active_before = active_count
                    self.model.prune_latent_dims(num_prune=prune_amount)
                    active_after = int(self.model.graph_latent_mask.sum().item())
                    pruned_dims = active_before - active_after
                    logger.info(
                        "Latent mask pruning triggered | generation=%s epoch=%s active_before=%s "
                        "active_after=%s pruned=%s",
                        generation,
                        epoch,
                        active_before,
                        active_after,
                        pruned_dims,
                    )
                    if self.prune_callback is not None:
                        try:
                            self.prune_callback(
                                {
                                    "generation": generation,
                                    "epoch": epoch,
                                    "global_epoch": self._kl_global_epoch,
                                    "active_before": active_before,
                                    "active_after": active_after,
                                    "pruned_dims": pruned_dims,
                                }
                            )
                        except Exception:  # pragma: no cover - defensive logging
                            logger.exception("Latent prune callback failed")
                    self.last_prune_epoch = epoch
                    self.initial_loss = smoothed_loss  # reset baseline with smoothed value

            self._kl_global_epoch += 1
            epoch += 1
        if DEBUG_TRAINER:
            logger.info(
                "OnlineTrainer.train finished | epochs_run=%d duration=%.3fs",
                epoch - 1,
                time.perf_counter() - train_start,
            )
        return self.loss_history

    def decoder_teacher_force_pass(
        self,
        epochs: int,
        batch_size: int,
        teacher_force_weight: float = 2.0,
        generation: int = 0,
        verbose: bool = True,
        extra_graphs: Optional[Sequence[dict]] = None,
    ):
        epochs = max(0, int(epochs))
        if epochs == 0:
            return
        batch_size = max(1, int(batch_size))
        refresh_samples = list(self.dataset)
        if extra_graphs:
            for graph_dict in extra_graphs:
                data = self._graph_dict_to_data(graph_dict)
                if data is not None:
                    refresh_samples.append(data)
        if not refresh_samples:
            return
        loader = DataLoader(refresh_samples, batch_size=batch_size, shuffle=True)
        if verbose:
            print(
                f"Decoder teacher-forcing pass: epochs={epochs} batch_size={batch_size} weight={teacher_force_weight}"
            )
        for epoch in range(1, epochs + 1):
            self.model.train()
            epoch_adj = 0.0
            epoch_feat = 0.0
            batches = 0
            for batch in loader:
                batch = batch.to(self.device)
                self.optimizer.zero_grad()
                target_graph_attrs = group_node_attributes(batch.node_attributes, batch.batch)
                teacher_attr_targets = build_teacher_attr_targets(
                    target_graph_attrs, None, self.model.shared_attr_vocab
                )
                (
                    decoded_graphs,
                    _fitness_pred,
                    _metric_log_scales,
                    _convex_pred,
                    _graph_latent,
                    _mu_g,
                    _lv_g,
                    decoder_aux,
                ) = self.model(
                    batch.node_types,
                    batch.edge_index,
                    batch.node_attributes,
                    batch.batch,
                    teacher_attr_targets=teacher_attr_targets,
                    num_graphs=batch.num_graphs,
                )
                loss_adj, loss_feat = self._compute_reconstruction_losses(
                    batch,
                    decoded_graphs,
                    decoder_aux,
                    target_graph_attrs,
                    teacher_force_weight=teacher_force_weight,
                )
                loss = loss_adj + loss_feat
                loss.backward()
                self.optimizer.step()
                epoch_adj += float(loss_adj.detach().item())
                epoch_feat += float(loss_feat.detach().item())
                batches += 1

            avg_adj = epoch_adj / max(1, batches)
            avg_feat = epoch_feat / max(1, batches)
            if verbose:
                print(f"Decoder epoch {epoch}/{epochs}: adj_loss={avg_adj:.4f} attr_loss={avg_feat:.4f}")
            if self.progress_callback is not None:
                self.progress_callback(
                    generation=generation,
                    epoch=epoch,
                    total_epochs=epochs,
                    total_loss=avg_adj + avg_feat,
                    loss_terms={"decoder_adj": avg_adj, "decoder_attr": avg_feat},
                    per_metric_losses={},
                    kl_beta=0.0,
                )

    def resize_bottleneck(self):
        """Rebuild all modules to permanently prune inactive dims."""
        self.model.resize_bottleneck()
        # reinit optimizer so it only holds new params
        lr = self.optimizer.defaults.get("lr", 1e-3)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        print("Optimizer Reinitialized")


if __name__ == "__main__":
    from attributes import *

    num_node_types = 3
    max_nodes = 10
    graph_latent_dim = 32
    fitness_dim = 2
    attr_name_vocab_size = 50
    attr_name_vocab = [generate_random_string(5) for _ in range(attr_name_vocab_size)]
    shared_attr_vocab = SharedAttributeVocab(attr_name_vocab, 50)
    attr_encoder = NodeAttributeDeepSetEncoder(shared_attr_vocab, encoder_hdim=10, aggregator_hdim=20, out_dim=20)

    graph_encoder = GraphEncoder(num_node_types, attr_encoder, graph_latent_dim, hidden_dims=[32, 32])
    decoder = GraphDecoder(num_node_types, graph_latent_dim, shared_attr_vocab)
    predictor = FitnessPredictor(
        latent_dim=graph_latent_dim,
        hidden_dim=64,
        icnn_hidden_dims=(graph_latent_dim, graph_latent_dim // 2 or 1),
    )
    model = SelfCompressingFitnessRegularizedDAGVAE(graph_encoder, decoder, predictor)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Synthetic DAG generator
    import random

    def generate_random_dag(num_nodes, num_node_types, edge_prob=0.3):
        # each node gets a random type in [0, num_node_types)
        types = torch.randint(0, num_node_types, (num_nodes,), dtype=torch.long)
        edges = []
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                if random.random() < edge_prob:
                    edges.append([i, j])
        edge_index = (
            torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)
        )
        dyn_attrs = []
        for _ in range(num_nodes):
            attributes = {}
            for _ in range(random.randint(0, 5)):
                if random.random() <= 0.5:
                    attributes[IntAttribute(random.choice(attr_name_vocab))] = random.randint(0, 10)
                if random.random() <= 0.5:
                    attributes[FloatAttribute(random.choice(attr_name_vocab))] = random.random() * 5.0
                if random.random() <= 0.5:
                    attributes[StringAttribute(random.choice(attr_name_vocab))] = random.choice(attr_name_vocab)
            dyn_attrs.append(attributes)
        return Data(node_types=types, edge_index=edge_index, node_attributes=dyn_attrs)

    # Create synthetic dataset
    class Metric:
        def __init__(self, name, objective):
            self.name = name
            self.objective = objective

    print("Generating random training data")

    def generate_data(num_samples):
        graphs, fitnesses = [], []
        task_type = random.choice(list(TASK_FEATURE_DIMS.keys()))
        task_features = torch.randn((TASK_FEATURE_DIMS[task_type],))
        for _ in range(num_samples):
            graph = generate_random_dag(random.randint(3, max_nodes), num_node_types, edge_prob=0.4)
            graphs.append(graph)
            fitnesses.append(
                {Metric(str(i), "min"): graph.edge_index.size(1) + 0.1 * random.random() for i in range(fitness_dim)}
            )
        return graphs, fitnesses, task_type, torch.randn((TASK_FEATURE_DIMS[task_type],))

    graphs, fitnesses, *_ = generate_data(50)
    metric_keys = sort_metrics_by_name(fitnesses[0].keys()) if fitnesses else []
    trainer = OnlineTrainer(model, optimizer, metric_keys=metric_keys)
    trainer.configure_kl_scheduler(
        StagedBetaSchedule(start_beta=0.0, target_beta=0.1, warmup_epochs=10, ramp_epochs=30, hold_epochs=10),
        reset_state=True,
    )
    trainer.add_data(graphs, fitnesses)
    print("Training")
    trainer.train(epochs=None, batch_size=8, kl_weight=0.1, warmup_epochs=10, stop_epsilon=1)
    trainer.resize_bottleneck()

    # Continue training with new data
    graphs_new, fitnesses_new, *_ = generate_data(10)
    trainer.add_data(graphs_new, fitnesses_new)
    print("Continuing training")
    trainer.train(epochs=3, batch_size=8, kl_weight=0.1)
