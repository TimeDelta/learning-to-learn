"""
Generated by ChatGPT
"""
import torch
from torch._C import Graph, Node
from torch.jit import ScriptModule
from typing import List, Tuple, Dict

from genome import OptimizerGenome

def build_forward_graph(num_nodes:int, edges:List[Tuple[int,int]], input_keys:List[int], output_keys:List[int]) -> Graph:
    """
    Build a Graph with signature:
      (Tensor, Tensor, List[Tuple[str, Tensor]]) -> Dict[str, Tensor]
    """
    graph = Graph()

    # 1) add the three inputs
    loss_in = graph.addInput()
    loss_in.setType(torch._C.TensorType.get())
    prev_loss_in = graph.addInput()
    prev_loss_in.setType(torch._C.TensorType.get())
    named_params = graph.addInput()
    # List[Tuple[str, Tensor]]
    lp_ty = torch._C.ListType.ofPairs(torch._C.StringType.get(), torch._C.TensorType.get())
    named_params.setType(lp_ty)

    # 2) unpack the List[Tuple[str, Tensor]] into separate outputs
    #    so we can treat them as individual tensors in the graph
    unpack = graph.create("prim::ListUnpack", inputs=[named_params], num_outputs=len(input_keys)-2) # all except loss & prev_loss
    graph.appendNode(unpack)
    param_values = list(unpack.outputs())    # these are Tensors

    # 3) Build a single big Tensor of shape [num_nodes, ...] from
    #    [ loss, prev_loss, *param_values ]
    all_inputs = [loss_in, prev_loss_in] + param_values
    lc = graph.create("prim::ListConstruct", inputs=all_inputs, num_outputs=1)
    graph.appendNode(lc)
    zero = graph.insertConstant(0)
    st = graph.create("aten::stack", inputs=[lc.output(), zero], num_outputs=1)
    graph.appendNode(st)
    features = st.output()

    # 4) do one round of message‐passing: out_feats[n] = sum_in_edges w_e * features[src]
    #    To keep the example small, we use an unrolled sequence of element‐wise ops.
    #    A production version would vectorize with scatter_add as shown earlier.
    #    Here we just demonstrate Node usage.
    accum = graph.insertConstant(None) # placeholder; we’ll replace per-node below

    # For each node id in sorted order, build:
    #    tmp = 0
    #    for (src, dst) in edges if dst == nid:
    #        w = self.w_src_dst        # captured later as a submodule parameter
    #        mul = features[src] * w
    #        tmp = tmp + mul
    #    out_feats[nid] = tmp
    node_outputs: Dict[int, Node] = {}
    for nid in range(num_nodes):
        # start `tmp = 0`
        zero_t = graph.create("aten::zeros_like", inputs=[loss_in], num_outputs=1)
        graph.appendNode(zero_t)
        tmp = zero_t.output()

        for src, dst in edges:
            if dst != nid:
                continue
            # load the weight parameter by name: w_{src}_{dst} in the Module
            w_node = graph.create("prim::GetAttr",
                                  inputs=[graph.block().owningModule().this()], # the `self` object
                                  num_outputs=1)
            w_node.s_("name", f"w_{src}_{dst}")
            graph.appendNode(w_node)
            w_out = w_node.output()

            # gather features[src]
            src_idx = graph.insertConstant(src)
            pick = graph.create("aten::select", inputs=[features, zero, src_idx], num_outputs=1)
            graph.appendNode(pick)
            feat_src = pick.output()

            # mul = feat_src * w_out
            mul = graph.create("aten::mul", inputs=[feat_src, w_out], num_outputs=1)
            graph.appendNode(mul)

            # tmp = tmp + mul
            add = graph.create("aten::add", inputs=[tmp, mul.output()], num_outputs=1)
            graph.appendNode(add)
            tmp = add.output()

        node_outputs[nid] = tmp

    # 5) register outputs: map each output_key to its computed tensor,
    #    but we must return a Dict[str, Tensor], so we build a DictConstruct.
    #    First build a ListConstruct of [ key1, val1, key2, val2, ... ]
    dict_list_elems = []
    for ok in output_keys:
        # key string
        ks = graph.insertConstant(str(ok))
        dict_list_elems.append(ks)
        dict_list_elems.append(node_outputs[ok])

    dc = graph.create("prim::ListConstruct", inputs=dict_list_elems, num_outputs=1)
    graph.appendNode(dc)
    # finally pack to Dict
    dict_node = graph.create("prim::DictConstruct", inputs=[dc.output()], num_outputs=1)
    graph.appendNode(dict_node)
    graph.registerOutput(dict_node.output())

    return graph

def rebuild_and_script(graph_dict, config, key) -> ScriptModule:
    """
    1) Rebuild the genome nodes+connections (as before)
    2) Create ScriptModule, attach `w_src_dst` Parameters
    3) Generate the IR with `build_forward_graph` and hook it up
    """
    # --- rebuild genome structure ---
    genome = OptimizerGenome(key)

    # nodes
    node_types   = gen_dict["node_types"].tolist()
    node_attrs   = gen_dict["node_attributes"]
    for nid, _ in enumerate(node_types):
        ng = NodeGene(nid, None)
        ng.node_type = node_attrs[nid].get("node_type")
        ng.dynamic_attributes = dict(node_attrs[nid])
        genome.nodes[nid] = ng
    genome.next_node_id = len(node_types)

    # connections
    for src, dst in gen_dict["edge_index"].t().tolist():
        cg = genome.create_connection(config, src, dst)
        cg.enabled = True
        genome.connections[(src, dst)] = cg

    # --- make a fresh ScriptModule and give it weight params ---
    module = ScriptModule()
    for (src, dst), conn in genome.connections.items():
        if conn.enabled:
            w = getattr(conn, "weight", 1.0)
            module.register_parameter(f"w_{src}_{dst}", torch.nn.Parameter(torch.tensor(w)))

    # --- build the Graph and attach as `forward` ---
    graph = build_forward_graph(
        num_nodes   = len(genome.nodes),
        edges       = list(genome.connections.keys()),
        input_keys  = config.input_keys,
        output_keys = config.output_keys
    )
    module._c._create_method_from_graph("forward", graph)
    return module
